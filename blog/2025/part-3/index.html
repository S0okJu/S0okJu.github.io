<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3><strong>서버 구축기 — Part 3. 쿠버네티스에서 대시보드 접속 그리고 운영의 문제점</strong></h3> <h3>현황</h3> <p>지금까지 Multipass 를 활용하여 쿠버네티스 환경을 만들고, 실제로 적용하는 과정을 거쳤습니다.<br>현재 3개의 노드로 구성되어 있으며, nfs, mysql, mongo와 같이 데이터베이스가 있는 인스턴스는 nfs, 젠킨스 서버는 ops 인스턴스에 저장했습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/524/1*5pn8A61R_Jfh7ASyXzBBFw.png"><figcaption>Multipass 인스턴스 목록</figcaption></figure> <p>최대한 클라우드 NFS 스토리지처럼 구현하고 싶어 별도의 인스턴스에 NFS 서버를 설치해서 사용했습니다.</p> <h3>대시보드 확인하기</h3> <p>일단 쿠버네티스 내에서는 외부와 통신할 수 있는 기능은 크게 두 가지입니다.</p> <ul> <li>Ingress</li> <li>Nodeport</li> </ul> <p>일단 사설 네트워크 내에서만 사용할 예정이므로 Nodeport 방식을 사용하겠습니다.</p> <h4>예제 — Grafana ui 확인하기</h4> <p>예로 프로메테우스를 사용한다고 가정해보자. 우선 grafana 대시보드에 접근해야 합니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/655/1*p0uenUYjTO_6_5o2BunIcw.png"></figure> <p>grafana ui와 관련된 애플리케이션의 service type를 nodeport로 변경해준 후에 Host server(192.168.50.27)에 방화벽을 설정하면 192.168.50.27 주소로 Grafana ui에 접근할 수 있습니다.</p> <p>그러나 서버에 문제가 생기면 재배포하게 되는데, 쿠버네티스 스케줄러에 따라서 다른 노드에 배치될 수 있습니다. 이럴 경우 배치된 노드를 확인해가며 기존의 방화벽을 닫고, 새로운 방화벽을 열어줘야 합니다.</p> <p>쿠버네티스에서는 여러 스케줄링 방식이 있는데¹, 그 중 간단한 nodeSelector 로 node1에 강제 배치하여 정적으로 사용할 수 있도록 구성했습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/719/1*j49EtiFb1JxM4OLNGbD0Mw.png"></figure> <h4>nodeSelector 설정</h4> <p>node1에 label를 type=ui로 설정했습니다.</p> <pre>kubectl label nodes node1 type=ui</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/473/1*R3MGl0m_nrLHaQ0XaCYiJg.png"><figcaption>kubectl get node node1</figcaption></figure> <h4>grafana ui service type 변경</h4> <p>Prometheus, Grafana의 원활한 설치를 위해 <a href="https://github.com/prometheus-operator/kube-prometheus" rel="external nofollow noopener" target="_blank">kube-prometheus helm</a>을 사용했습니다. helm 차트의 특성상 values.yaml를 설정하여 값을 주입하므로 <strong>values.yaml</strong> 를 확인해야 합니다.<br>kube-prometheus helm 차트는 여러 helm 차트로 구성되어 있어 설정하고 싶은 서비스의 <strong>values.yaml</strong>를 설정했습니다.</p> <p>kube-prometheus-stack/charts/grafana/values.yaml 파일에 대략 216번째 줄에 아래와 같은 서비스 타입을 Nodeport로 변경했습니다.</p> <pre>## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service).<br>## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it.<br>## ref: http://kubernetes.io/docs/user-guide/services/<br>##<br>service:<br>  enabled: true<br>    # type: ClusterIP<br>  type: NodePort # Custom <br>  loadBalancerIP: ""<br>  loadBalancerClass: ""<br>  loadBalancerSourceRanges: []<br>  port: 80<br>  targetPort: 3000<br>  nodePort: 30060 # Custom </pre> <h4>helm nodeSelector 설정</h4> <p>kube-prometheus-stack/charts/grafana/values.yaml 의 314번째 줄에 node1에 배치되도록 설정했습니다.</p> <pre>## Node labels for pod assignment<br>## ref: https://kubernetes.io/docs/user-guide/node-selection/<br>#<br>nodeSelector:<br>  type: ui</pre> <p>helm으로 배포한 후 pod 리스트를 보면 <strong>{helm으로 배포한 이름}-grafana-{문자열}</strong> 보이는데 파드 내부에는 grafana와 관련된 컨테이너가 포함되어 있습니다. 즉, 우리가 찾던 ui도 grafana pod 내부에 있다는 것입니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/679/1*pO3fix1FmK71vyE8urz-NQ.png"></figure> <p>grafana 파드의 정보를 보면 nodeSelector가 정상적으로 적용되었음을 알 수 있습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1012/1*LJX3CBrjJgGqc-DzA1pE8A.png"></figure> <h4>방화벽 설정</h4> <p>node1 인스턴스의 nodeport(30060)에 트래픽이 들어올 수 있도록 허용했습니다.</p> <pre>sudo iptables -t nat -I PREROUTING -i enp3s0 -p tcp --dport 30060 -j DNAT --to-destination 10.120.52.23:30060<br>sudo iptables -I FORWARD 1 -p tcp -d 10.120.52.23 --dport 30060 -j ACCEPT</pre> <ol> <li>30060 포트로 enp3s0 인터페이스에 들어오는 트래픽을 10.120.52.23:30060로 리다이렉션한다.</li> <li>10.120.52.23:30060에 접속하는 트래픽을 허용한다.</li> </ol> <p>서버의 IP로 접속하면 그라파나 대시보드를 확인할 수 있습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1008/1*0SQyfRikdCiuTQC9-iSvww.png"><figcaption>Grafana Dashboard</figcaption></figure> <h3>문제점</h3> <p>Multipass로 인스턴스를 만들고, 외부와의 접속을 위해 트래픽을 제어하는 것은 문제점이 있습니다.</p> <ol> <li>외부와의 접속을 할때마다 iptable를 설정해야 한다.<br>서버가 shutdown이 될 경우 이전에 설정했던 iptables rule이 사라진다. 이럴 경우 규칙을 다시 설정하거나 이를 대비해서 별도의 패키지를 설치해서 iptables이 영속성이 있도록 설정²해야 합니다.</li> <li>인스턴스가 많으면 많을수록 관리하기 어렵다.<br>쿠버네티스 이외에도 다양한 인스턴스가 있지만 일리리 명령어로 상태를 확인하고 설정하는 것은 번거롭습니다. 그러므로 별도의 <strong>오케스트레이션 도구가 필요합니다.</strong> </li> </ol> <p>위의 해결책으로 <a href="https://www.openstack.org/software/project-navigator/openstack-components#openstack-services" rel="external nofollow noopener" target="_blank">OpenStack</a> 서버를 구축하고자 합니다. 인스턴스를 public cloud 처럼 자유자재로 생성 및 삭제할 수 있으며, 네트워크 설정이 용이했습니다. 또한 클라우드에 사용되는 기술을 대부분 활용할 수 있어 클라우드를 알아가는데 좋은 도구라고 생각했습니다.</p> <p>다음 포스팅은 기존의 서버를 포맷시키고, openstack 서버 구축기로 찾아오겠습니다.</p> <ol> <li> <a href="https://kubernetes.io/ko/docs/tasks/configure-pod-container/assign-pods-nodes/" rel="external nofollow noopener" target="_blank">https://kubernetes.io/ko/docs/tasks/configure-pod-container/assign-pods-nodes/</a>↩︎</li> <li> <a href="https://unix.stackexchange.com/questions/52376/why-do-iptables-rules-disappear-when-restarting-my-debian-system" rel="external nofollow noopener" target="_blank">https://unix.stackexchange.com/questions/52376/why-do-iptables-rules-disappear-when-restarting-my-debian-system</a>↩︎</li> </ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=481343ac0c3f" width="1" height="1" alt="">&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/s0okju-tech/%EC%84%9C%EB%B2%84-%EA%B5%AC%EC%B6%95%EA%B8%B0-part-3-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%97%90%EC%84%9C-%EB%8C%80%EC%8B%9C%EB%B3%B4%EB%93%9C-%EC%A0%91%EC%86%8D-%EA%B7%B8%EB%A6%AC%EA%B3%A0-%EC%9A%B4%EC%98%81%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90-481343ac0c3f" rel="external nofollow noopener" target="_blank">서버 구축기 — Part 3. 쿠버네티스에서 대시보드 접속 그리고 운영의 문제점</a> was originally published in <a href="https://medium.com/s0okju-tech" rel="external nofollow noopener" target="_blank">S0okJu Technology Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p> </body></html>