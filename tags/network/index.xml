<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Network on S0okJu.dev</title><link>https://s0okju.github.io/tags/network/</link><description>Recent content in Network on S0okJu.dev</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 16 Mar 2025 00:00:00 +0900</lastBuildDate><atom:link href="https://s0okju.github.io/tags/network/index.xml" rel="self" type="application/rss+xml"/><item><title>HexaCTF 12. 쿠버네티스 멀티 노드 선정 이유와 웹 애플리케이션 배포 회고</title><link>https://s0okju.github.io/p/hexactf-12/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0900</pubDate><guid>https://s0okju.github.io/p/hexactf-12/</guid><description>&lt;p>이제부터 워커 노드를 2개 선정한 이유와 쿠버네티스에 웹 애플리케이션을 배포하면서 느낀점을 작성하겠습니다.&lt;/p>
&lt;h2 id="쿠버네티스-워커-노드-구성">쿠버네티스 워커 노드 구성
&lt;/h2>&lt;p>초기에 저는 사용자가 배포한 Challenge만의 독립적인 환경이 필요했습니다. 그래서 워커 노드를 2개 설치한 후 &lt;code>nodeSelector&lt;/code>를 활용하여 애플리케이션 역할에 따라 각 노드에 배치될 수 있도록 구성했습니다.&lt;br>
하나의 워커 노드에 웹 애플리케이션과 기타 모니터링 도구를 배치하고 다른 하나에는 Challenge 서버만 배치하도록 설계했습니다.&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/hexactf-12/image.png"
width="601"
height="281"
srcset="https://s0okju.github.io/p/hexactf-12/image_hu_138392bc5d04347.png 480w, https://s0okju.github.io/p/hexactf-12/image_hu_a8338b3dd0369a20.png 1024w"
loading="lazy"
alt="워커 노드 애플리케이션 배치도"
class="gallery-image"
data-flex-grow="213"
data-flex-basis="513px"
>&lt;/p>
&lt;h3 id="질문-1-왜-워커-노드가-3개가-아닌가요">질문 1. 왜 워커 노드가 3개가 아닌가요?
&lt;/h3>&lt;p>아래의 질문을 할 수 있을 것 같습니다.&lt;/p>
&lt;blockquote>
&lt;p>&amp;ldquo;프로그램 역할에 따라 노드 별로 배치한다고 하셨잖아요. 그렇다면 노드 3개 배포해야 하는 것 아닌가요? &amp;quot;&lt;/p>&lt;/blockquote>
&lt;p>위의 질문은 옳은 말이지만 &lt;strong>서버 리소스가 부족&lt;/strong>하여 노드 3개를 배포하지 못했습니다.
쿠버네티스는 배포할때 Kubespray 플랫폼을 활용합니다. Kubespray에 워커 노드를 배포할때는 모두 동일한 이미지와 리소스 타입이 적용됩니다. 워커 노드에 이미지를 ubuntu:20.04에 리소스 타입으로 2vCPU, 4GB로 선택했다면 워커 노드 모두 동일하게 적용되는 것이죠.&lt;/p>
&lt;h4 id="쿠버네티스-인스턴스에-배치된-리소스-양">쿠버네티스 인스턴스에 배치된 리소스 양
&lt;/h4>&lt;blockquote>
&lt;p>워커 노드는 메모리 선정 기준이 확실히 있었으나 나머지는&lt;a class="link" href="https://docs.kublr.com/installation/hardware-recommendation/" target="_blank" rel="noopener"
> kublr에서 제시한 기준표&lt;/a>의 2배를 산정해서 생성했습니다. 넉넉하면 좋지 않을까 하는 마음으로 리소스를 늘렸던 것 같습니다.&lt;/p>&lt;/blockquote>
&lt;ul>
&lt;li>마스터 노드 : 4vCPU, 8GB&lt;/li>
&lt;li>워커 노드: 8vCPU, 16GB&lt;/li>
&lt;/ul>
&lt;p>그러나 실제 하드웨어는 64GB의 메모리를 가지고 있으며 Jenkins, NFS 서버, OpenStack 서버의 메모리 양을 고려했을때 워커 노드를 3개를 배치할 수 없었습니다.&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/hexactf-12/image-1.png"
width="811"
height="170"
srcset="https://s0okju.github.io/p/hexactf-12/image-1_hu_6ff0a98be6b391d8.png 480w, https://s0okju.github.io/p/hexactf-12/image-1_hu_cca59a81745b2c66.png 1024w"
loading="lazy"
alt="오픈 스택 컴퓨팅 리소스 잔여량"
class="gallery-image"
data-flex-grow="477"
data-flex-basis="1144px"
>&lt;/p>
&lt;h3 id="질문-2-워커-노드의-메모리를-16gb로-선택한-이유">질문 2. 워커 노드의 메모리를 16GB로 선택한 이유
&lt;/h3>&lt;blockquote>
&lt;p>CPU는 연산할때 주로 많이 쓰이는 리소스입니다. 그러나 CTF 대회 문제 중에서 연산 집중적인 문제가 큰 비중을 차지하지 않습니다. 전반적으로 CPU 사용량이 무난하다고 판단해 추가적인 CPU 사용 기준을 설정하지 않았습니다.&lt;/p>&lt;/blockquote>
&lt;p>Challenge의 최대 사용량을 계산했을때 16GB 메모리가 적당하다고 생각하고 있습니다.&lt;/p>
&lt;p>출제할 문제의 베이스 이미지는 쿠버네티스 클러스터를 배포할 때 알 수 없습니다. 미리 예측해서 배포해야 합니다. 그러므로 최대 사용 가능한 컴퓨팅 자원량을 계산하여 자원 부족으로 인해 문제가 발생하지 않도록 설계해야 했습니다.&lt;/p>
&lt;p>&lt;strong>Challenge 문제에 쿠버네티스 limit quota를 활용해 리소스의 최대 사용량을 설정할 것입니다. 이 정도를 토대로 총 예상 사용량을 계산하기로 결정했습니다.&lt;/strong>&lt;/p>
&lt;h4 id="challenge-서버의-특징">Challenge 서버의 특징
&lt;/h4>&lt;p>Challenge 서버는 &lt;strong>특정 기능 하나만 구현되어 있어 단순하다는 특징&lt;/strong>을 가지고 있습니다. 또한 한명만 서버에 접속할 수 있죠.
CPU, 메모리 활용 관점에서 예측하면 다음과 같습니다.&lt;/p>
&lt;ol>
&lt;li>CPU - 빌드 초기에 많이 쓰인다. 그 이외에는 무난할 것 같다.&lt;/li>
&lt;li>메모리 - 메모리 상한선이 정해져 있을 것 같다.&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;a class="link" href="https://s0okju.github.io/p/hexactf2025-review/" target="_blank" rel="noopener"
>실제 대회를 운영하면서&lt;/a> 예상한 패턴이 어느정도 맞긴 했습니다.&lt;/p>&lt;/blockquote>
&lt;h4 id="메모리-사용량만-고려한-이유">메모리 사용량만 고려한 이유
&lt;/h4>&lt;p>우선 CPU는 선정 기준을 아직까지도 감을 찾지 못했습니다. 🥹 이건 실무에서 한번 배우고 싶네요.&lt;/p>
&lt;p>메모리는 CPU과 다르게 초기에 데이터를 적재한 후에 실행되는 특징을 가지고 있습니다. CPU가 사용 후 바로 반납하는 느낌이라면 메모리는 누적의 느낌이 강합니다.(개인적인 생각입니다.)
메모리는 관리를 제대로 하지 않으면 빨리 소모되는 자원인 셈이죠. 그러므로 limit, request quota로 사용 가능한 최대 메모리 용량를 지정하여 불필요한 사용량을 줄이는 것이 중요하다고 생각했습니다.&lt;/p>
&lt;h4 id="예상-사용-메모리-용량-계산">예상 사용 메모리 용량 계산
&lt;/h4>&lt;p>Challenge를 활용하는 CTF 문제는 주로 Web, System입니다. System은 가벼운 리눅스 이미지를 활용하며 자원 사용량의 편차가 크지 않습니다. 반면 Web의 경우에는 어떤 이미지를 활용했냐에 따라 편차가 매우 큽니다. 그러므로 상한선을 선택할 때는 스프링부트 플랫폼의 평균 메모리 사용량 256MB를 기준으로 계산했습니다.&lt;/p>
&lt;blockquote>
&lt;p>256MB(스프링부트 기준 메모리 양) x 3(사용자 당 동시 실행 컨테이너 수) x 20(사용자) = 15GB&lt;/p>&lt;/blockquote>
&lt;p>15GB는 제가 생각한 최대 메모리 사용량이라고 생각합니다. 사용자 수가 2배, 3배로 크게 증가하지 않는 한 예상 범위 내로 메모리를 활용할 것으로 보입니다.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>여담&lt;/strong>
System 문제와 Web 문제의 메모리 사용량은 2배 ~ 4배 차이로 상당히 큽니다. 대회마다 문제 양과 유형이 크게 변화하지 않기 때문에 메모리 사용량을 비율로 계산할까 고민했습니다.
실제 대회에 참가한 친구들에게 물어보니 Web 문제에 가장 많이 도전하고 시간 투자를 많이 한다고 합니다. 그래서 Web 문제의 최대 메모리 사용량을 기준으로 선택했습니다.&lt;/p>&lt;/blockquote>
&lt;h4 id="challenge-request-limit-quota">Challenge request, limit quota
&lt;/h4>&lt;p>저는 모든 Challenge의 Deployment에 아래와 같은 resource quota를 설정했습니다.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">limits&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;500m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;512Mi&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;250m&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s2">&amp;#34;256Mi&amp;#34;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>왜 최대 사용량을 256MB으로 지정했으면서 실제로는 512MB로 선정했는지 궁금하셨을 것 같습니다.&lt;/p>
&lt;blockquote>
&lt;p>왜 DevOps, SRE가 개발 경험이 있어야 하는지 뼈저리게 느꼈습니다. 경험이 없으니 전반적으로 감을 잡을 수 없었습니다.&lt;/p>&lt;/blockquote>
&lt;p>Challenge 서버에서 기능은 하나 뿐이고 사용자도 실질적으로 한명이기 때문에 리소스 사용량이 크게 변하지 않을 것이라고 생각했습니다. 그런데 눈으로 직접 보지 않는 이상 모르겠다는 생각을 했습니다.&lt;br>
마침 참여가 확정된 사용자는 예상보다 적어서 사용 패턴을 확인할 겸 limit를 512MB로 선정하게 되었습니다.&lt;/p>
&lt;h2 id="쿠버네티스-hexactf-서비스-아키텍처-설계-및-회고">쿠버네티스 HexaCTF 서비스 아키텍처 설계 및 회고
&lt;/h2>&lt;p>웹 애플리케이션은 hexactf라는 네임 스페이스에 배포했습니다.
hexactf 내에 있는 애플리케이션은 모두 하나의 Helm으로 배포했습니다.&lt;/p>
&lt;blockquote>
&lt;p>Operator 배포 방식에 대한 자세한 내용은 &lt;a class="link" href="https://s0okju.github.io/p/hexactf-9/" target="_blank" rel="noopener"
>HexaCTF9&lt;/a>를 참고하시길 바랍니다.&lt;/p>&lt;/blockquote>
&lt;blockquote>
&lt;p>눈썰미 좋으신 분들은 알겠지만 Queue 또한 메세지를 저장해야 하므로 Statefulset으로 배포해야 합니다.&lt;/p>&lt;/blockquote>
&lt;p>&lt;img src="https://s0okju.github.io/p/hexactf-12/image-2.png"
width="624"
height="241"
srcset="https://s0okju.github.io/p/hexactf-12/image-2_hu_c3a21299daae4a6d.png 480w, https://s0okju.github.io/p/hexactf-12/image-2_hu_f07ed3800517c945.png 1024w"
loading="lazy"
alt="쿠버네티스 HexaCTF 애플리케이션 구성도"
class="gallery-image"
data-flex-grow="258"
data-flex-basis="621px"
>&lt;/p>
&lt;h3 id="아쉬운-점">아쉬운 점
&lt;/h3>&lt;p>가장 아쉬운 점은 모든 애플리케이션을 하나의 Helm으로 배포한 것입니다.&lt;/p>
&lt;ol>
&lt;li>데이터베이스&lt;/li>
&lt;/ol>
&lt;p>운영하면서 DB에 진짜 오류가 많았습니다. &lt;code>kubectl logs&lt;/code> 로 매일 문제점을 파악했는데요. 로그만으로는 명확한 원인은 파악할 수 없었지만 &lt;a class="link" href="https://hiwony.tistory.com/85" target="_blank" rel="noopener"
>InterfaceError가 빈번하게 나타나는 것을 보아 Connection leak 문제라고 생각했습니다. &lt;/a>&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">pymysql.err.InterfaceError: (0, &amp;#39;&amp;#39;)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>그래서 문제의 근거를 찾고자 Prometheus의 ServiceMonitor 연결을 시도했으나 실패했습니다. 시간이 없어서 끝까지 해보지는 못했습니다.&lt;br>
대회가 끝나고 다시 생각해보니 Operator로 MariaDB를 따로 배포하면 좋지 않았을까 하는 아쉬움이 있습니다. 내부적으로 모니터링 기능을 쉽게 설정할 수 있으며 상황에 따라서 확장과 관리가 용이하기 때문입니다.&lt;/p>
&lt;ol start="2">
&lt;li>큐&lt;/li>
&lt;/ol>
&lt;p>사진에는 Queue라고 적었지만 정확하게 Kafka입니다.&lt;/p>
&lt;blockquote>
&lt;p>Kafka를 선정한 이유는 한번이라도 써보고 싶었기 때문입니다. ^-^! 쓰고 나서 후회했습니다.&lt;/p>&lt;/blockquote>
&lt;p>눈썰미가 좋으신 분들은 알고 계셨겠지만 Queue 또한 StatefulSet으로 배포해야 합니다. 큐에는 메세지가 저장되어 있기 때문에 상태를 저장해야 하기 때문입니다.&lt;br>
프로젝트에서는 운이 좋게도 큐에 문제가 생기지 않아 데이터 유실이 되지 않았습니다. 그러다 대회를 마무리하고 나서 보니 배포 리소스를 잘못 설정했다는 것을 깨닫게 되었습니다.&lt;/p>
&lt;p>Queue도 프로젝트의 Helm으로 관리하는 것이 아닌 분리해서 Operator로 관리하는 것이 좋다고 생각했습니다. 데이터베이스와 동일한 이유로 확장과 관리가 용이하기 때문입니다.&lt;/p>
&lt;h2 id="배포-프로세스-회고">배포 프로세스 회고
&lt;/h2>&lt;h3 id="작은-일도-수작업은-힘들다">작은 일도 수작업은 힘들다.
&lt;/h3>&lt;p>프로젝트 당시 &lt;strong>자동화는 하나도 하지 않고 수작업으로 변경사항을 일리리 확인하고 빌드하고 배포&lt;/strong>했습니다.&lt;/p>
&lt;p>젠킨스 서버는 존재합니다. 그런데 주요 목적은 &lt;strong>수동&lt;/strong>으로 HexaCTF 웹 애플리케이션, Challenge 이미지를 빌드하여 이미지 레지스트리에 전송하는 용도였습니다.&lt;/p>
&lt;p>초기에는 프로젝트가 소규모이므로 CI/CD는 필요없다는 의견이 대다수였습니다. 저도 어느 정도 동의했으며 한 사람(본인)이 모든 배포 프로세스를 수용할 수 있을 것이라고 생각했습니다.&lt;/p>
&lt;p>대회 일주일 전에 git pull, helm upgrade 명령어만 무한 반복으로 치니 다양한 생각이 들었습니다.&lt;/p>
&lt;ul>
&lt;li>Helm으로 동시에 배포했는데 애플리케이션 하나는 이전 버전으로 롤백하고 다른 애플리케이션은 업그레이드 버전을 활용해야 하는 경우 -&amp;gt; 하나로 배포하는게 아니라 팀 단위로 쪼개서 CI/CD 쓸걸&lt;/li>
&lt;li>Helm upgrade 했는데 빌드 과정을 생략하여 변경 사항이 반영되지 않은 경우 -&amp;gt; 젠킨스로 이미지를 자동으로 빌드할걸&lt;/li>
&lt;li>branch를 잘못 설정했으면서 수동으로 빌드하기 눌러 구버전으로 이미지 빌드하기 -&amp;gt; 젠킨스로 이미지를 자동으로 빌드했으면 내가 브랜치를 잘못 썼다는 걸 알았을텐데&lt;/li>
&lt;li>helm 파일 수정본이 반영되지 않는 경우 -&amp;gt; ArgoCD 쓸걸&lt;/li>
&lt;li>문제 발생 시 팀원에게 직접 알리기 -&amp;gt; CI/CD에 있는 알림 기능을 활용할걸&lt;/li>
&lt;li>기타 등등 &amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>휴먼 이슈로 인해 예상보다 문제는 많았고 자동화를 안한 것을 뼈저리게 후회했습니다.&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/hexactf-12/image-3.png"
width="400"
height="300"
srcset="https://s0okju.github.io/p/hexactf-12/image-3_hu_76fcfbc179472712.png 480w, https://s0okju.github.io/p/hexactf-12/image-3_hu_49815617fdd2ab21.png 1024w"
loading="lazy"
alt="Challenge Operator, API 버그 수정하고 또 … 살려줘"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="320px"
>&lt;/p>
&lt;h2 id="마치며">마치며
&lt;/h2>&lt;p>웹 애플리케이션 배포에 대해 기술적으로 작성할만한 것은 없었습니다. 자동화라는 좋은 시스템을 버리고 선사 시대의 방식으로 수행했으니까요. 그만큼 몸소 느낀점이 많으며 제목도 배포가 아닌 &amp;ldquo;회고&amp;quot;로 지은 것 같습니다.&lt;/p>
&lt;p>이제 기술적인 내용은 끝입니다. 5개월 끝에 드디어 글을 모두 작성할 수 있었네요.&lt;br>
다음에 &amp;ldquo;HexaCTF 시리즈 어워드&amp;quot;를 마지막으로 시리즈를 마치겠습니다.&lt;/p></description></item><item><title>HexaCTF 11. 오픈 스택 서버 네트워크 개선기 - 서버에 NIC 2개 삽입한 이유</title><link>https://s0okju.github.io/p/hexactf-11/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0900</pubDate><guid>https://s0okju.github.io/p/hexactf-11/</guid><description>&lt;ul>
&lt;li>2025-03-15 - 내용 수정&lt;/li>
&lt;/ul>
&lt;h2 id="들어가며">들어가며
&lt;/h2>&lt;p>간략한 인프라 구성도입니다.&lt;/p>
&lt;p>여기서 눈에 띄는 점이 2가지가 있습니다.&lt;/p>
&lt;ol>
&lt;li>한 서버(OpenStack)에 NIC가 2개&lt;/li>
&lt;li>일부 사용자는 VPN 사용하여 접속한다&lt;/li>
&lt;/ol>
&lt;p>이제부터 왜 오픈스택 서버에 2개의 NIC를 삽입했는지 설명하겠습니다.&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/hexactf-11/image.png"
width="662"
height="362"
srcset="https://s0okju.github.io/p/hexactf-11/image_hu_21f73670fe72963c.png 480w, https://s0okju.github.io/p/hexactf-11/image_hu_3118b1722d6421d4.png 1024w"
loading="lazy"
alt="인프라 구성도"
class="gallery-image"
data-flex-grow="182"
data-flex-basis="438px"
>&lt;/p>
&lt;h2 id="nic-1개-삽입-시-문제점">NIC 1개 삽입 시 문제점
&lt;/h2>&lt;p>OpenStack은 기본적으로 2개의 네트워크 인터페이스가 요구됩니다. 그래서 설치가 비교적 간단한 DevStack이든 kolla-ansible이든 &lt;strong>하나의 물리적 NIC로만 구성한다면 운영체제에서 하나의 논리적 네트워크 인터페이스를 만들어서 설정해야 합니다.&lt;/strong>&lt;br>
하나의 NIC로만 구성하면 172.24.4.0/24(상황에 따라 다름)와 같은 네트워크 인터페이스를 생성하여 Provider Network를 구성하게 됩니다. 그러나 &lt;strong>Provider Network로부터 IP를 사설 IP(172.24.4.x)로 할당받으므로 외부 컴퓨터(Manager)에서 직접 접근할 수 없습니다.&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/hexactf-11/image-1.png"
width="735"
height="658"
srcset="https://s0okju.github.io/p/hexactf-11/image-1_hu_25510a72ccace2c7.png 480w, https://s0okju.github.io/p/hexactf-11/image-1_hu_d278d82f08a022b.png 1024w"
loading="lazy"
alt="NIC 1개 구성 시 문제점 - Provider Network에 직접 접근할 수 없음"
class="gallery-image"
data-flex-grow="111"
data-flex-basis="268px"
>&lt;/p>
&lt;p>결국 Floating IP가 할당된 가상머신에 접근하기 위해서는 서버 내에 NAT 설정을 해야 합니다.&lt;/p>
&lt;p>실제 프로젝트에서는 Provider Network를 물리 네트워크 인터페이스에 연결시키고 Management Network는 논리 네트워크 인터페이스로 연결했습니다. 즉 Floating IP를 &lt;code>192.168.50.x&lt;/code>로 할당을 받을 것이며 아무런 라우팅 설정 없이 직접 접근할 수 있도록 구성했습니다. 왜냐하면 Management Network에서는 하나의 IP만 활용되지만, 많은 가상 머신을 만들면서 Provider Network를 통해 2개 이상의 IP가 필요하기 때문입니다.&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/hexactf-11/image-2.png"
width="575"
height="620"
srcset="https://s0okju.github.io/p/hexactf-11/image-2_hu_fb9fbc23e1f2b3cc.png 480w, https://s0okju.github.io/p/hexactf-11/image-2_hu_804e497bef8f64f4.png 1024w"
loading="lazy"
alt="NIC 1개 구성 - 라우팅 설정이 필요함"
class="gallery-image"
data-flex-grow="92"
data-flex-basis="222px"
>&lt;/p>
&lt;p>대신 서버의 IP로 구성된 Resource API에 접근하기 위해서는 별도의 라우팅 설정이 요구됩니다. 라우팅 설정하는 방법은 크게 두 가지입니다.&lt;/p>
&lt;ol>
&lt;li>라우터에 라우팅 규칙 설정&lt;/li>
&lt;li>클라이언트(Manager), 서버의 라우팅 테이블 수정&lt;/li>
&lt;/ol>
&lt;h3 id="1-라우터에-라우팅-규칙-설정의-문제점">1. 라우터에 라우팅 규칙 설정의 문제점
&lt;/h3>&lt;blockquote>
&lt;p>그곳에는 많은 서버들이 있다.&lt;/p>&lt;/blockquote>
&lt;p>서버가 있는 장소가 제 것만 있으면 1번은 당연한 선택지일 것입니다. 그러나 제 서버 이외에도 다른 사람들의 서버가 있으며 그것들 또한 가상화 솔루션 서버로 이뤄져 있습니다.
공용으로 사용되는 라우터에 라우팅 테이블을 설정하면 문제가 하나 발생하게 됩니다. &lt;strong>다른 서버 이용자가 의도치 않게 OpenStack의 사설 네트워크와 동일한 IP에 접속을 요청하면 OpenStack 서버로 패킷이 전송되는 문제가 발생할 수 있습니다.&lt;/strong>&lt;/p>
&lt;p>실제 경험해보지 않았지만 제가 예상한 문제 시나리오는 크게 두 가지 입니다.&lt;/p>
&lt;ol>
&lt;li>다른 사용자(User2) 가 자신의 서버에 접근을 요청할 경우&lt;/li>
&lt;li>다른 사용자가 User 2의 서버에 접속하여 가상 머신에 접근을 요청할 경우&lt;/li>
&lt;/ol>
&lt;p>시나리오의 모든 전제 조건은 라우팅 테이블의 최고 우선순위가 default(0.0.0.0) -&amp;gt; 라우터 게이트웨이인 경우 입니다. &lt;strong>다시 말해 라우팅 테이블의 우선순위를 수정하면 아래의 문제는 어느 정도 해소할 수 있습니다.&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/hexactf-11/image-3.png"
width="570"
height="762"
srcset="https://s0okju.github.io/p/hexactf-11/image-3_hu_6d087047adf6057f.png 480w, https://s0okju.github.io/p/hexactf-11/image-3_hu_3f38e9d663e1b2a7.png 1024w"
loading="lazy"
alt="IP 주소 충돌 시나리오 2가지"
class="gallery-image"
data-flex-grow="74"
data-flex-basis="179px"
>&lt;/p>
&lt;p>그러나 4~5대의 각 서버 사용자에게 연락을 드려 일리리 조사를 하는 것도 여간 번거로운 일이 아닙니다. 그래서 다른 방법을 생각해야 합니다.&lt;/p>
&lt;h3 id="2-클라이언트-서버에-네트워크-규칙-추가의-문제점">2. 클라이언트, 서버에 네트워크 규칙 추가의 문제점
&lt;/h3>&lt;blockquote>
&lt;p>네트워크 규칙 정보는 상태를 영구적으로 저장하지 않는다.&lt;/p>&lt;/blockquote>
&lt;p>다른 방법으로는 클라이언트와 서버에 라우팅을 설정하면 됩니다.&lt;/p>
&lt;ol>
&lt;li>클라이언트&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>192.168.50.27(서버 IP)를 통해 10.0.10.0/24에 해당되는 패킷을 전송하는 라우팅 룰 추가합니다.&lt;/li>
&lt;/ul>
&lt;ol start="2">
&lt;li>서버&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>물리 인터페이스에서 패킷을 송신하여 10.0.10.0/24 수신을 허락합니다.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>자세한 사항은 &lt;a class="link" href="https://s0okju.github.io/p/server-setup-4/" target="_blank" rel="noopener"
>서버 구축기 - 4. Kolla-ansible 설치 시 마주한 네트워크 문제&lt;/a> 을 참고하시길 바랍니다.&lt;/p>&lt;/blockquote>
&lt;p>문제는 iptable, route 룰 모두 영구적으로 상태를 저장하지 않습니다. &lt;strong>컴퓨터, 서버를 한번 끄면 route, iptable 규칙이 사라집니다.&lt;/strong>
물론 컴퓨터 부팅 후 규칙을 저장하는 init script를 제작하여 영구적으로 만들 수 있으며, iptable의 경우 도구를 활용하여 영구적으로 상태를 저장할 수 있습니다.&lt;/p>
&lt;p>저는 노트북을 통해 서버에 자주 접속합니다. 제 노트북에서도 내부적으로 가상 머신을 활용하곤 합니다.
이런 상황에서 2번처럼 영구적으로 저장하게 된다면 어떻게 될까요? 서버에 설정한 라우팅 규칙을 잊어 동일한 네트워크 대역의 가상머신에 접근을 요청하게 된다면? 결국 &lt;strong>유연성이 떨어진다는 문제&lt;/strong>가 발생하게 됩니다.&lt;/p>
&lt;h2 id="nic-2개-삽입한-이유">NIC 2개 삽입한 이유
&lt;/h2>&lt;p>위의 문제를 종합하면 &lt;strong>또다른 서버를 공유하고 있는 환경에서 가상 네트워크 인터페이스에 직접 접근하기 위한 적절한 라우팅 설정이 어렵다는 것입니다.&lt;/strong>
결국 위의 문제를 해결하기 위해서는 OpenStack 관련 모든 네트워크 대역을 물리 네트워크 인터페이스로만 구성하면 됩니다. 그렇다면 추가적인 라우팅 규칙을 설정할 필요가 없습니다.&lt;/p>
&lt;h3 id="nic-스위치들은-모두-어디에-연결될까">NIC 스위치들은 모두 어디에 연결될까?
&lt;/h3>&lt;p>동일한 네트워크 대역을 가지기 위해 NIC는 &lt;strong>모두 동일한 스위치에 연결&lt;/strong>됩니다. 즉 192.168.50.1/24 대역이라면 서버는 192.168.50.27로 할당받고 Floating IP로도 192.168.50.122 이런 식으로 할당받는 것이죠. &lt;strong>Provider Network는 서버와 동일한 대역의 IP를 가지게 됩니다.&lt;/strong>&lt;/p>
&lt;h3 id="이유-1-resource-api-floating-ip-직접-접근할-수-있다">이유 1. Resource API, Floating IP 직접 접근할 수 있다.
&lt;/h3>&lt;p>대부분의 인프라 리소스는 Terraform을 활용해 배포합니다. Kubernetes의 경우 Kubespray를 활용해서 배포하기 때문에 별다른 선택지는 없었고, 그 이외에는 관리가 편리하여 Terraform을 활용하게 되었습니다.&lt;/p>
&lt;p>만약에 NIC가 2개라면 위에서 설명한 라우팅 설정을 할 필요가 없어집니다. 왜냐하면 Resource API는 서버의 IP(&lt;code>192.168.50.27&lt;/code>)이고, Provider Network는 동일한 대역인 &lt;code>192.168.50.x&lt;/code>으로 접근할 수 있으니까요.&lt;/p>
&lt;h3 id="이유-2-포트-충돌을-해소할-수-있다">이유 2. 포트 충돌을 해소할 수 있다.
&lt;/h3>&lt;p>&lt;strong>이건 NIC 2개를 선택한 이전에 오픈 스택을 선택한 이유이기도 합니다.&lt;/strong>&lt;/p>
&lt;p>HexaCTF 플랫폼 자체는 2개의 워커 노드를 가진 쿠버네티스 내에서 작동됩니다.
문제를 해결하는 사람(이용자)가 접근해야 하는 사이트는 크게 두 가지 이며 배포된 장소가 다릅니다.&lt;/p>
&lt;ol>
&lt;li>HexaCTF 웹사이트 - Worker Node 1&lt;/li>
&lt;li>Challenge 서버 - Worker Node 2&lt;/li>
&lt;/ol>
&lt;h4 id="오픈-스택이-아닌-경우--단일-ip로만-구성된-경우">오픈 스택이 아닌 경우 = 단일 IP로만 구성된 경우
&lt;/h4>&lt;p>HexaCTF 관련 서비스들은 모두 NodePort를 통해 외부와 통신합니다. 웹사이트는 고정 NodePort 포트 번호를 지정해도 Challenge 서버는 NodePort 범위(30000~32767)내에 랜덤으로 포트 번호가 정해지기 때문에 &lt;strong>포트 번호 충돌 가능성이 존재&lt;/strong>합니다. 예로 웹 사이트를 30000으로 열어도 Challenge 서버가 30000 포트 번호로 생성되면 충돌이 발생하게 되는 것이죠.&lt;/p>
&lt;blockquote>
&lt;p>Challenge는 사용자가 동적으로 생성되는 쿠버네티스 커스텀 리소스입니다. 사용자는 Challenge 요청하여 NodePort의 포트 번호를 얻으면 http 혹은 nc을 통해 시스템에 접속하게 됩니다.&lt;/p>&lt;/blockquote>
&lt;p>그럼 누군가는 이렇게 생각할 수 있을 것 같습니다.&lt;/p>
&lt;blockquote>
&lt;p>Q. Worker Node 1에 배포된 애플리케이션은 NodePort가 고정되어 있으니 다른 포트 번호로 포트 포워딩 시키면 되지 않을까요? 예로 쿠버네티스 워커 노드에서는 애플리케이션을 30010로 열지만 실제 서버에는 5010번으로 iptable 규칙을 수정하면 되잖아요?&lt;/p>
&lt;p>A. 그것도 좋은 방법이라고 생각합니다. 실제로 서버는 정전이 많이 발생되는 환경이기에 iptable 규칙이 영속적으로 유지될 수 있도록 설정해야 합니다. iptable 규칙이 포함된 별도의 스크립트를 작성하면 포트 충돌 문제는 해소할 수 있습니다.&lt;/p>&lt;/blockquote>
&lt;p>스크립트를 작성하여 부팅 시 적용시키는 방법이 있습니다. 그러나 이것저것 추가하고 삭제하면서 스크립트를 업데이트 하지 않아 포트 충돌 문제가 발생할 수 있습니다.&lt;br>
관리를 잘하면 별다른 문제가 발생하지 않습니다. 그러나 &lt;strong>유동적으로 추가되고 삭제되는 것들을 일리리 관리하는 것은 실수가 많이 발생한다고 생각하여 스크립트 작성은 좋은 선택지로 보고 있지 않습니다.&lt;/strong>&lt;/p>
&lt;h4 id="오픈-스택을-활용할-경우--개별-ip-소유">오픈 스택을 활용할 경우 = 개별 IP 소유
&lt;/h4>&lt;p>개별적으로 IP를 가지게 되면 포트 충돌 문제를 완전히 해소할 수 있습니다.
Worker Node 1, 2는 서로 다른 IP를 가지고 있기 때문에 포트 번호가 겹치더라도 서로 다른 서비스로 인식되니까요.&lt;/p>
&lt;h3 id="이유-3-역할-별-접근-제어가-가능하다">이유 3. 역할 별 접근 제어가 가능하다.
&lt;/h3>&lt;p>HexaCTF는 크게 3가지 사용자가 있습니다.&lt;/p>
&lt;ol>
&lt;li>시스템 운영자( or 문제 등록자)&lt;/li>
&lt;li>개발자&lt;/li>
&lt;li>사용자&lt;/li>
&lt;/ol>
&lt;h4 id="오픈-스택이-아닌-경우--단일-ip로만-구성된-경우-1">오픈 스택이 아닌 경우 = 단일 IP로만 구성된 경우
&lt;/h4>&lt;p>단일 IP인 경우를 생각해 봅시다.
모든 애플리케이션을 NodePort로 열었습니다. 그리고 나서 라우터에 포트 포워딩을 수행합니다.
문제는 저희가 사용하고 있는 라우터는 포트 범위 기반으로 포트포워딩은 못하고 DMZ로 열어야 합니다. 모든 포트를 열어야 하는 상황인거죠.&lt;/p>
&lt;p>DMZ란 내부 네트워크에 존재하지만 외부에 접근할 수 있는 특수한 영역을 의미합니다. 주로 외부에 많이 노출되는 FTP, Web Server에 DMZ를 구성하며 불필요한 서비스 노출을 최소화하기 위해 주로 활용됩니다.&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/hexactf-11/image-4.png"
width="1920"
height="1080"
srcset="https://s0okju.github.io/p/hexactf-11/image-4_hu_79a63deaced69902.png 480w, https://s0okju.github.io/p/hexactf-11/image-4_hu_87e943dcf443f1ab.png 1024w"
loading="lazy"
alt="출처 - https://www.zenarmor.com/docs/network-security-tutorials/what-is-dmz"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="426px"
>&lt;/p>
&lt;p>만약에 단일 IP로 할경우 포트 포워딩을 할때 단일 IP에 전체의 포트를 외부에 오픈하게 됩니다. 이러한 환경에서 악의적인 사용자가 오픈한 포트를 스캔한 후 관리자 서비스에 접근할 수 있습니다. 이는&lt;strong>역할을 명확하게 분리하지 않아 발생한 불필요한 노출 문제이며, 명백하게 보안 문제로 이어지게 됩니다.&lt;/strong>&lt;/p>
&lt;blockquote>
&lt;p>Q. 방화벽을 설치하면 되잖아요!&lt;br>
A. 이럴 경우에는 하드웨어 방화벽이 필요합니다.. 돈이 없어요.. 🥹&lt;/p>&lt;/blockquote>
&lt;h4 id="오픈-스택을-활용할-경우--개별-ip-소유-1">오픈 스택을 활용할 경우 = 개별 IP 소유
&lt;/h4>&lt;p>오픈스택을 활용하여 모든 리소스에 IP를 할당하면 위와 같은 문제를 해결할 수 있습니다. 우선 인터넷을 연결할 수 있는 서비스와 그렇지 않는 서비스를 IP를 통해 명확하게 나눌 수 있게 됩니다. 즉 서비스를 분류하는 기준이 포트 번호가 아닌 IP로 확장된 것이죠.&lt;/p>
&lt;h5 id="사용자">사용자
&lt;/h5>&lt;p>Worker Node1, 2에 각각 사설 IP를 할당하여 단일 포트만 오픈해야 하는 서비스는 포트 포워딩을 통해 인터넷과 연결해주고, 범위로 지정해야 할 경우에는 DMZ로 인터넷과 연결해주면 됩니다.&lt;/p>
&lt;p>이런 식으로 구성하면 사용자는 Worker Node 1 내에 배포된 애플리케이션과 Worker Node 2에 배포된 Challenge 서비스만 접근할 수 있게 되어 불필요한 노출을 최소화하게 됩니다.&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/hexactf-11/image-5.png"
width="602"
height="498"
srcset="https://s0okju.github.io/p/hexactf-11/image-5_hu_475fc17c32d26117.png 480w, https://s0okju.github.io/p/hexactf-11/image-5_hu_57905927e19bcdec.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="290px"
>&lt;/p>
&lt;h5 id="운영자-혹은-개발자">운영자 혹은 개발자
&lt;/h5>&lt;p>하지만 운영자와 개발자도 쿠버네티스 내에 배포된 각종 도구 혹은 다른 서비스에 접근해야 합니다.&lt;br>
그렇다면 어떻게 접근할까요? 바로 &lt;strong>VPN&lt;/strong>을 활용하는 것입니다. 즉 별도의 인터넷 연결 없이 사설 IP에 접근할 수 있습니다.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>가상사설망&lt;/strong>(假想私設網) 또는 &lt;strong>VPN&lt;/strong>(&lt;a class="link" href="https://ko.wikipedia.org/wiki/%EC%98%81%EC%96%B4" title="영어"
target="_blank" rel="noopener"
>영어&lt;/a>: virtual private network)은 공중 &lt;a class="link" href="https://ko.wikipedia.org/wiki/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC" target="_blank" rel="noopener"
>네트워크&lt;/a>를 통해 한 회사나 몇몇 단체가 내용을 바깥 사람에게 드러내지 않고 통신할 목적으로 쓰이는 사설 통신망이다.&lt;/p>
&lt;ul>
&lt;li>출처: &lt;a class="link" href="https://ko.wikipedia.org/wiki/%EA%B0%80%EC%83%81%EC%82%AC%EC%84%A4%EB%A7%9D" target="_blank" rel="noopener"
>위키백과&lt;/a>&lt;/li>
&lt;/ul>&lt;/blockquote>
&lt;p>이런식으로 구성하면 개발자와 관리자는 안전하게 관련 서비스에 접근할 수 있으며, 그 이외의 사용자는 해당 서비스에 접근할 수 없게 됩니다.&lt;/p>
&lt;h2 id="정리">정리
&lt;/h2>&lt;p>NIC를 2개로 삽입한 이유, 오픈 스택을 활용한 이유 등 선택의 이유는 복합적입니다. 그래서 순차적으로 원인과 결과를 명확하게 말하기 어려운 것 같습니다.&lt;/p>
&lt;p>핵심은 아래와 같습니다.&lt;/p>
&lt;blockquote>
&lt;p>외부와 통신해야 하는 모든 시스템은 동일한 네트워크 대역에서 직접 접근할 수 있는 사설 IP를 가져야 하며, 필요에 따라 인터넷에 노출시킬 수 있다.&lt;/p>&lt;/blockquote>
&lt;h2 id="마치며">마치며
&lt;/h2>&lt;p>이번 글은 여러가지 의미로 작성하기 어려웠습니다. 사설 IP인데 운영체제에서 만든 네트워크 인터페이스를 통해 할당받은 IP는 똑같이 사설 IP라고 설명해야 하나? 여러가지 고민이 많았던 것 같습니다.&lt;/p>
&lt;p>다음 글에서는 쿠버네티스로 애플리케이션을 어떻게 구성하고 배포했는지 설명하겠습니다.&lt;/p>
&lt;h2 id="참고">참고
&lt;/h2>&lt;ul>
&lt;li>&lt;a class="link" href="https://tech.osci.kr/ovs_network_flow/" target="_blank" rel="noopener"
>OpenStack 환경에서의 OVS 네트워크 흐름 심층 분석 - 오픈소스컨설팅 테크블로그&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://aws-hyoh.tistory.com/161" target="_blank" rel="noopener"
>VPN 쉽게 이해하기&lt;/a>&lt;/li>
&lt;li>&lt;a class="link" href="https://www.zenarmor.com/docs/network-security-tutorials/what-is-dmz" target="_blank" rel="noopener"
>What is a DMZ (Demilitarized Zone) Network? - zenarmor.com&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>클라우드 네트워크 - 클라우드와 VLAN</title><link>https://s0okju.github.io/p/cloud-network-1/</link><pubDate>Sun, 27 Oct 2024 00:00:00 +0900</pubDate><guid>https://s0okju.github.io/p/cloud-network-1/</guid><description>&lt;h2 id="lan이란">LAN이란
&lt;/h2>&lt;p>LAN(Local Address Network)은 위키백과에 의하면 네트워크 매체를 이용하여 가까운 지역을 한데 묶은 컴퓨터 네트워크&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>입니다.
그런데 이 말 자체는 상당히 추상적이라고 생각합니다. 여러 글을 찾던 중 &lt;a class="link" href="https://aws-hyoh.tistory.com/" target="_blank" rel="noopener"
>네트워크 엔지니어 환영의 기술 블로그&lt;/a>에서 &lt;a class="link" href="https://aws-hyoh.tistory.com/85" target="_blank" rel="noopener"
>LAN에 대해 잘 설명한 글&lt;/a>을 알게 되었습니다.&lt;/p>
&lt;blockquote>
&lt;p>집과 같은 소규모 네트워크에서부터 사무실, 회사와 같은 중규모 이상의 네트워크에 이르기까지 &lt;strong>동일한 IP 대역과 동일한 Subnet Mask를 사용&lt;/strong>한다면 크기와 상관 없이 LAN이라고 부를 수 있다.&lt;/p>
&lt;p>출처 - &lt;a class="link" href="https://aws-hyoh.tistory.com/85" target="_blank" rel="noopener"
>네트워크 엔지니어 환영의 기술블로그 : LAN 쉽게 이해하기 &lt;/a>&lt;/p>&lt;/blockquote>
&lt;p>예시를 들어보겠습니다. 저는 현재 홈 서버를 운영하고 있습니다. 기기의 IP주소를 확인하게 되면 192.168.50.x라는 동일한 IP 대역을 가지고 있습니다. 다시말해 저는 LAN을 쓰고 있다고 보면 됩니다.&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/cloud-network-1/image.png"
width="441"
height="291"
srcset="https://s0okju.github.io/p/cloud-network-1/image_hu_c98354d26bb085c.png 480w, https://s0okju.github.io/p/cloud-network-1/image_hu_2dc8ce16dd3e0a80.png 1024w"
loading="lazy"
alt="홈 서버 구성도"
class="gallery-image"
data-flex-grow="151"
data-flex-basis="363px"
>&lt;/p>
&lt;p>여기 문제가 발생하게 됩니다. 만약에 다양한 IP 대역을 쓰고 싶다면 어떻게 해야할까요? 물리적인 장치를 추가하는 방법이 있습니다만 상황에 따라서는 불가능할 수 있습니다. 또 다른 해결 방안으로 VLAN이 있습니다.&lt;/p>
&lt;h2 id="vlan">VLAN
&lt;/h2>&lt;p>하나의 단일 네트워크는 하나의 브로드캐스트 영역이며 이러한 구간을 LAN이라고 합니다. 이때 VLAN은 LAN 논리적으로 브로드캐스트 도메인을 나누는 영역이라고 표현할 수 있습니다. 즉 하나의 스위치 만으로도 &lt;strong>여러 개의 네트워크 대역을 사용&lt;/strong>할 수 있게 되는 것입니다.&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/cloud-network-1/image-1.png"
width="645"
height="363"
srcset="https://s0okju.github.io/p/cloud-network-1/image-1_hu_ee5e790b4a245ea4.png 480w, https://s0okju.github.io/p/cloud-network-1/image-1_hu_7711525fca5f5511.png 1024w"
loading="lazy"
alt="출처 - https://yoursmaddy.medium.com/vlans-segmenting-networks-for-better-performance-and-security-306d5d6b47a"
class="gallery-image"
data-flex-grow="177"
data-flex-basis="426px"
>&lt;/p>
&lt;p>&lt;strong>VLAN은 네트워크를 분할만하지 서로 통신하는 기능을 제공하지 않습니다.&lt;/strong> 위의 그림처럼 VLAN2의 패킷을 VLAN3에 보낼 수 없게 됩니다. 그러므로 L3 스위치 또는 라우터를 사용하여 VLAN 간의 라우팅을 수행해야 합니다.&lt;/p>
&lt;blockquote>
&lt;p>L3 스위치와 라우터는 초기에는 기술적인 차이가 있었으나 기술의 발전으로 차이가 거의 없다고 합니다.&lt;br>
L3 스위치가 L2 스위치 + 라우터의 특징을 가지고 있으며, VLAN을 사용하기가 더 효율적이라고 합니다. &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>&lt;/p>&lt;/blockquote>
&lt;p>VLAN은 이와 같은 설명이 전부는 아닙니다. 스위치 간 어떻게 통신할지 등등 관련된 이야기가 많습니다. 그러나 이해를 위해 네트워크를 분할한다는 점만 알아주세요.&lt;/p>
&lt;h2 id="vlan과-cloud-network">VLAN과 Cloud Network
&lt;/h2>&lt;p>퍼블릭 클라우드에서는 VPC를 정의한 후 Private, Public 서브넷을 정의하게 됩니다. 당연한 말이지만 물리적인 장치와 1:1 매칭하다보면 &amp;ldquo;어떻게?&amp;rdquo; 라는 물음만 남는 것 같습니다. 자료를 찾아보면서 내린 결론은 클라우드 네트워킹의 핵심은 &lt;strong>가상화&lt;/strong>라는 것이었습니다.&lt;/p>
&lt;p>Cloud에서는 가상 라우터를 통해 Network(VPC)를 구성하고, VLAN을 활용하여 Subnet을 구성하게 됩니다. 그리고 클라우드 리소스는 Subent에서 ip를 할당받아 네트워크 범위 내에서 통신하게 됩니다.&lt;/p>
&lt;p>&lt;img src="https://s0okju.github.io/p/cloud-network-1/image-2.png"
width="1108"
height="516"
srcset="https://s0okju.github.io/p/cloud-network-1/image-2_hu_30c21a0346101b54.png 480w, https://s0okju.github.io/p/cloud-network-1/image-2_hu_ea70babc91ac5d84.png 1024w"
loading="lazy"
alt="출처 - https://www.shapeblue.com/a-beginners-guide-to-cloudstack-networking/"
class="gallery-image"
data-flex-grow="214"
data-flex-basis="515px"
>&lt;/p>
&lt;h2 id="다음-이야기---openstack를-통해-알아보는-클라우드-네트워크">다음 이야기 - Openstack를 통해 알아보는 클라우드 네트워크
&lt;/h2>&lt;p>Openstack은 Cloud OS로 오픈소스입니다. AWS와 같은 퍼블릭 클라우드 내부구조는 공개되지 않았지만 Openstack를 통해 큰 틀의 클라우드 구조를 파악할 수 있습니다.&lt;br>
Openstack은 네트워크 구성 시 OVS를 사용합니다. Openstack에서도 다른 퍼블릭 클라우드처럼 Subnet 개념이 존재하며 VLAN 기술을 사용하게 됩니다.&lt;/p>
&lt;blockquote>
&lt;p>OVS는 다중 계층 스위치의 오픈소스 구현체로 여러 프로토콜 및 표준을 지원하면서 하드웨어 가상화 환경을 위한 스위칭 스택을 제공합니다.&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/p>&lt;/blockquote>
&lt;p>&lt;img src="https://s0okju.github.io/p/cloud-network-1/image-3.png"
width="1194"
height="534"
srcset="https://s0okju.github.io/p/cloud-network-1/image-3_hu_4ef0c5b138d2708.png 480w, https://s0okju.github.io/p/cloud-network-1/image-3_hu_a7ac7913109366c.png 1024w"
loading="lazy"
alt="출처 - https://platform9.com/blog/getting-started-openstack-neutron/"
class="gallery-image"
data-flex-grow="223"
data-flex-basis="536px"
>&lt;/p>
&lt;p>현재 홈서버로 Openstack를 활용하고 있습니다. 다음 글에서 홈서버를 통해 어떻게 네트워크가 구성되었는지 자세히 말씀 드리겠습니다.&lt;/p>
&lt;h1 id="reference">Reference
&lt;/h1>&lt;ul>
&lt;li>&lt;a class="link" href="https://aws-hyoh.tistory.com/75" target="_blank" rel="noopener"
>https://aws-hyoh.tistory.com/75&lt;/a>&lt;/li>
&lt;/ul>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a class="link" href="https://ko.wikipedia.org/wiki/%EA%B7%BC%EA%B1%B0%EB%A6%AC_%ED%86%B5%EC%8B%A0%EB%A7%9D" target="_blank" rel="noopener"
>https://ko.wikipedia.org/wiki/%EA%B7%BC%EA%B1%B0%EB%A6%AC_%ED%86%B5%EC%8B%A0%EB%A7%9D&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>그림으로 배우는 네트워크 원리, Gene 저자, 영진닷컴&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a class="link" href="https://en.wikipedia.org/wiki/Open_vSwitch" target="_blank" rel="noopener"
>https://en.wikipedia.org/wiki/Open_vSwitch&lt;/a>&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item></channel></rss>