<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://s0okju.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://s0okju.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-17T08:46:11+00:00</updated><id>https://s0okju.github.io/feed.xml</id><title type="html">DaGyeong Kim</title><subtitle></subtitle><entry><title type="html">서버 구축기 — Part 5. OpenStack을 설치하면서 마주한 오류들</title><link href="https://s0okju.github.io/blog/2025/part-5-openstack/" rel="alternate" type="text/html" title="서버 구축기 — Part 5. OpenStack을 설치하면서 마주한 오류들"/><published>2025-06-05T10:57:58+00:00</published><updated>2025-06-05T10:57:58+00:00</updated><id>https://s0okju.github.io/blog/2025/-part-5-openstack---</id><content type="html" xml:base="https://s0okju.github.io/blog/2025/part-5-openstack/"><![CDATA[<h3>서버 구축기 — Part 5. OpenStack을 설치하면서 마주한 오류들</h3> <h3>br-ex로 옮겼더니 인터넷 연결이 안되요!!</h3> <p>외부 인터페이스를 br-ex 인터페이스에 옮길때<br/>netplan(/etc/netplan/00- * )을 확인하여 옮긴 인터페이스가 {} 를 가지는지 확인하자!</p> <pre>network:<br /> ethernets:<br />  ...<br />  enp3s0: {}<br />  br-ex :<br />   ...</pre> <h3>Swift — 503 Service Unavailable</h3> <p>만약에 Swift에 Service Unavailable이 나온다면 Swift가 제대로 구성되었는지 확인해보자.</p> <pre>raise UnexpectedResponse(msg, resp)#012<br />swift.common.internal_client.UnexpectedResponse: <br />Unexpected response: <br />503 Service Unavailable\u0000&quot;,&quot;<br />log_level&quot;:&quot;err&quot;,&quot;Hostname&quot;:&quot;server&quot;</pre> <p>디스크에 파티션을 나누고 xfs 파일 시스템으로 포맷팅 할때 globals.yml에서 <strong>지정해준 이름을 무조건 설정</strong>해야 합니다. 그래야 swift를 설치할때 자동으로 폴더가 마운팅됩니다.</p> <p>만약에 이름이 지정되지 않는다면 아래와 같이 sda2,3,4에서 mount가 되지 않습니다.</p> <blockquote>수동으로 fstab를 수정해도 swift를 제대로 사용할 수 없습니다.</blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/701/1*7_XcGqv05RfzABrQOqHEfQ.png"/></figure> <h3>glance image upload error</h3> <p>kolla ansible로 cloud image upload를 시도하면 즉각적으로 저장되지 않고 queued 상태가 됩니다. 이는 이미지 저장 시 문제가 생겼다는 의미입니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/618/1*1uTVW3_r54RPXfPx1CRhgg.png"/></figure> <p>Internal Server Error가 발생했다는 것은 glance-api에 문제가 있다고 해석할 수 있습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SHteXkcNNByYpZ2szU0QYQ.png"/></figure> <p>swift를 찾아보니 glance 관련된 컨테이너가 제대로 생성되지 않았습니다. 하지만 그 외에 swift 컨테이너가 생성된 것을 보아 내부적으로 문제가 생긴 것으로 판단했습니다.<br/>그래서 이미지를 저장하는 공간을 swift에서 file(서버 디렉토리)로 변경하여 이미지가 제대로 설치될 수 있게 변경했습니다.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6c4d26743df0" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/s0okju-tech/%EC%84%9C%EB%B2%84-%EA%B5%AC%EC%B6%95%EA%B8%B0-part-5-openstack%EC%9D%84-%EC%84%A4%EC%B9%98%ED%95%98%EB%A9%B4%EC%84%9C-%EB%A7%88%EC%A3%BC%ED%95%9C-%EC%98%A4%EB%A5%98%EB%93%A4-6c4d26743df0">서버 구축기 — Part 5. OpenStack을 설치하면서 마주한 오류들</a> was originally published in <a href="https://medium.com/s0okju-tech">S0okJu Technology Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">서버 구축기 — Part 4. Kolla-ansible 설치 시 마주한 네트워크 문제</title><link href="https://s0okju.github.io/blog/2025/part-4-kolla-ansible/" rel="alternate" type="text/html" title="서버 구축기 — Part 4. Kolla-ansible 설치 시 마주한 네트워크 문제"/><published>2025-06-05T10:47:12+00:00</published><updated>2025-06-05T10:47:12+00:00</updated><id>https://s0okju.github.io/blog/2025/-part-4-kolla-ansible-----</id><content type="html" xml:base="https://s0okju.github.io/blog/2025/part-4-kolla-ansible/"><![CDATA[<h3><strong>서버 구축기 — Part 4. Kolla-ansible 설치 시 마주한 네트워크 문제</strong></h3> <h3>DevStack의 한계</h3> <p>DevStack으로 Openstack을 배포하면서 큰 문제점을 느끼게 되었습니다.</p> <ul><li>Nova Instance에 직접 접근하는 것이 어렵다.</li><li>리소스를 추가 및 삭제하는 것이 어렵다.</li></ul> <p>위의 문제점은 <strong>네트워크로 인해 발생</strong>한 것입니다.<br/>예로 제가 Nova Instance를 생성해 Floating ip를 할당한다고 가정해 봅시다. External Network의 IP 범위가 외부와 연결이 불가능해 직접적으로 접근할 수 없게 됩니다. 그럼 ssh로 서버에 직접 접속해서 관리하는 방안도 있습니다. 그럼 Openstack을 왜 쓴거지? 키는 어떻게 관리하지? 등 다양한 꼬리 질문이 따라오게 됩니다.</p> <h3>Kolla Ansible</h3> <p>방안을 모색하는 중 Openstack은 DevStack 이외에도 다양한 배포 방안이 있다는 것을 알게 되었습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*s51SwlDRdLgySf0ln0HK_w.png"/><figcaption>출처 — <a href="https://docs.openstack.org/contributors/ko_KR/common/introduction.html">https://docs.openstack.org/contributors/ko_KR/common/introduction.html</a></figcaption></figure> <blockquote>Kolla Ansible이란 Ansible 플레이북을 통해 OpenStack의 모든 서비스를 <strong>컨테이너로 배포</strong>하고 유지 보수하며 운영할 수 있도록 도와주는 프로젝트이다. 이를 통해 복잡한 설치 과정과 수동 설정을 최소화하고, 재현 가능한 환경을 제공하여 효율적이며 안정적인 클라우드 인프라 운영을 가능하게 한다.¹</blockquote> <p>즉 우리가 설정한 시스템 서비스를 컨테이너 형태로 구축되는 것입니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/983/1*OjkuIdEx4rOD5D57Ni3bFw.png"/><figcaption>출처 — <a href="https://velog.io/@larshavin/Kolla-Ansible%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-OpenStack-%EC%84%A4%EC%B9%98-1">https://velog.io/@larshavin/Kolla-Ansible%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-OpenStack-%EC%84%A4%EC%B9%98-1</a></figcaption></figure> <h3>설치</h3> <blockquote>설치는 <a href="https://parandol.tistory.com/72">파란돌님의 블로그 — Kolla-ansible로 Openstack All-in-one 설치하기(Installing Openstack All-in-one with Kolla-ansible)</a>를 적극 참고했습니다.</blockquote> <h4>설치 목록</h4> <p>Kolla-ansible의 장점은 원하는 리소스 선택과 설정이 쉽다는 것입니다. 그래서 저는 기본적인 리소스를 선택하여 설치했습니다.</p> <p><strong>설치할 리소스</strong></p> <ul><li>Keystone</li><li>Glance</li><li>Nova</li><li>Neutron</li><li>Cinder</li><li>Swift</li><li>Horizon</li></ul> <h4><strong>all-in-one or node?</strong></h4> <p>오픈스택은 크게 3가지 노드가 있습니다.</p> <ul><li>Controller node : 전체 오픈스택 서비스를 관리하기 위해 사용됩니다. 컨트롤러 관리 및 노드 간 연결을 의해서는 최소한 2개 이상의 인터넷 인터페이스가 필요합니다.</li><li>Compute node : Nova 기반의 인스턴스를 작동하기 위해 사용되는 하이퍼바이저를 실행하는 노드입니다.</li><li>Network node : 다양한 네트워크 서비스 에이전트를 실행하며, 이를 가상 네트워크에 인스턴스를 연결합니다.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/626/1*83AA2ZM3v7u4gNKYm4O-8Q.png"/><figcaption>출처 — <a href="https://docs.oracle.com/cd/E36784_01/html/E54155/archover.html">https://docs.oracle.com/cd/E36784_01/html/E54155/archover.html</a></figcaption></figure> <p>여러 가이드를 보면 노드들은 물리적으로 분리되어 있습니다. 서버 내 가상머신을 구축하여 구현하는 방법이 있습니다. 그러나 복잡한 관계로 <strong>서비스 기능을 하나의 호스트에 설치할 수 있는 all-in-one²</strong>을 선택하게 되었습니다.</p> <h3>Openstack 네트워크</h3> <p><strong>가장 어렵고 앞으로 계속 공부해야 하는 분야</strong>인 것 같습니다. 완전히 이해한 것은 아니지만 알고 있는 그대로 작성하겠습니다.</p> <p>오픈 스택에서 네트워크는 Management, Tunnel, External 네트워크가 있습니다.</p> <ul><li>Management Network : 관리용 네트워크로 각 컴포넌트와 관련된 API를 호출하는데 사용됩니다.</li><li>Tunnel Network : vm instance 간 네트워크를 구축하는데 사용됩니다.</li><li>External Network : vm instance가 인터넷과 통신하기 위한 네트워크입니다.</li></ul> <p>네트워크 서비스는 크게 두 가지 옵션이 있습니다.</p> <ul><li>Provider Network : 가상 네트워크를 물리적 네트워크로 연결합니다. 즉 물리적인 네트워크가 vm 인스턴스가 활용하는 네트워크가 됩니다.</li><li>Self-Service Network : 오픈스택을 사용하는 사용자가 직접 자신만의 네트워크를 구축할 수 있는 네트워크 입니다.</li></ul> <blockquote>Provider 네트워크는 부하분산 서비스 혹은 방화벽 서비스 등 고급 기능을 지원하지 않습니다.</blockquote> <p>제가 사용하는 옵션은 비교적 간단한 Provider Network 입니다.</p> <h4>테스트용 Openstack 서버의 네트워크 접근 문제</h4> <p>kolla-ansible에서 제공해주는 globals.yml 를 확인해보겠습니다. 환경 파일을 보면 Internal, External Network로 구분되어 있습니다.</p> <p>network_interface는 Internal network에 해당되는 인터페이스를 지정하는 것으로 저는 외부 인터넷과 연결되지 않는 enp2s0 인터페이스를 사용했습니다. 반면 neutron_external_interface는 provider로 제공할 인터페이스로 인터넷과 연결할 수 있는 enp3s0 인터페이스를 사용했습니다.</p> <pre>##############################<br /># Neutron - Networking Options<br />##############################<br /># ... <br /># followed for other types of interfaces.<br />network_interface: &quot;enp2s0&quot;<br />...<br /><br /># ...<br />neutron_external_interface: &quot;enp3s0&quot;</pre> <p>그림을 그려보면 아래와 같습니다.<br/>(아래의 그림은 172.17.0.250/24이 아니라 172.16.0.250/24 입니다. )</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/504/1*P3BnvplmSjgwmCcsDpk3CQ.png"/><figcaption>openstack network architecture</figcaption></figure> <p>하지만 이번 오픈스택 서버는 어디까지나 개인으로만 사용되는 공간입니다. 즉 <strong>클라우드 서비스를 쓰는 것도 저 혼자이고, 관리하는 것도 저 혼자인 것입니다</strong>. 관리를 위한 네트워크 접속(Internal Network)도, 오픈스택 리소스 접근할 수 있는 네트워크 접속(External Network)도 가능해야 합니다.</p> <p>그러나 저는 클라우드 리소스의 API를 접근할 수 없습니다. 왜냐하면 서로 다른 네트워크 대역을 가지고 있기 때문입니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/601/1*RaRRa1pn8LQ6L8fTwmpK_A.png"/></figure> <p>이러한 문제를 해결하기 위해서는 라우팅 테이블을 설정해야 할 것입니다. 라우터에 직접 설정하는 방법과 운영체제에서 처리하는 방법이 있는데, 물리적으로 라우터가 하나만 존재하기 때문에 운영체제에서 처리해야 합니다.</p> <p>저는 ip forward를 사용했습니다.</p> <blockquote>IP-Forward란 커널 기반으로 라우팅 포워딩으로 하나의 인터페이스로 들어온 패킷을 다른 서브넷을 가진 네트워크 인터페이스로 패킷을 포워딩시키는 것이다.</blockquote> <pre># Kernel parameter update <br />$ sudo sysctl -w net.ipv4.conf.all.forwarding=1 <br />$ sudo sysctl net.ipv4.conf.all.forwarding net.ipv4.conf.all.forwarding = 1</pre> <p>br-ex 송신되어 enp2s0 인터페이스에 수신되는 것을 허락한다는 의미입니다.</p> <blockquote>br-ex은 OpenVSwitch(OVS) 적용시 물리 네트워크 인터페이스와 직접 연결되는 가상 스위치입니다. 외부 네트워크에서 가상 머신을 접근할때 사용됩니다.</blockquote> <pre>sudo iptables -I FORWARD -i br-ex -o enp2s0 -j ACCEPT<br />sudo iptables -nL FORWARD</pre> <p>클라이언트도 설정이 필요합니다.<br/>192.168.50.27를 통해 172.16.0.0/24(enp2s0)에 접근한다는 routing rule를 추가합니다.</p> <pre>sudo route -n add 172.16.0.0/24 192.168.50.27</pre> <p>여기서 왜 192.168.50.27를 통해 172.16.0.0/24 네트워크에 접근한다고 설정한 것일까요? 오픈 스택 서버에서 ip_forward를 시켰습니다. 즉 서버가 라우터의 역할을 하는 것입니다. 그래서 클라이언트에서 라우팅 룰을 설정할 때에는 172.16.0.250/24에 포워딩을 시킨 서버의 ip 주소로 설정해야 하는 것입니다.</p> <h3>정리</h3> <p>테스트를 수행하기 위해 Openstack 서버를 구축하게 되었습니다. 구축 시 요구사항은 인스턴스에 접근할 수 있으면서 리소스 API에 접근 가능해야 한다는 것이었습니다.<br/>서로 다른 네트워크 대역을 가져 리소스 API에 접근하지 못했으나 서버에 ip forward를 수행하여 문제를 해결했습니다.</p> <p>문제를 해결해 보니 Neutron에 대해 모르고 있다는 사실을 알게 되었습니다. 다음 포스팅은 Neutron 톱아보기로 돌아오겠습니다.</p> <h3>Reference</h3> <ul><li><a href="https://velog.io/@lijahong/0%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-Linux-%EA%B3%B5%EB%B6%80-%EB%B0%A9%ED%99%94%EB%B2%BD-%EC%BB%B4%ED%93%A8%ED%84%B0">https://velog.io/@lijahong/0%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-Linux-%EA%B3%B5%EB%B6%80-%EB%B0%A9%ED%99%94%EB%B2%BD-%EC%BB%B4%ED%93%A8%ED%84%B0</a></li><li><a href="https://velog.io/@larshavin/Kolla-Ansible%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-OpenStack-%EC%84%A4%EC%B9%98-1">https://velog.io/@larshavin/Kolla-Ansible%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-OpenStack-%EC%84%A4%EC%B9%98-1</a></li><li><a href="https://blog.naver.com/love_tolty/220237750951">https://blog.naver.com/love_tolty/220237750951</a></li></ul> <ol><li><a href="https://tech.osci.kr/openstack_cinder/">https://tech.osci.kr/openstack_cinder/</a>↩︎</li><li><a href="https://blog.naver.com/love_tolty/220237750951">https://blog.naver.com/love_tolty/220237750951</a> ↩︎</li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=484efa8a1c38" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/s0okju-tech/%EC%84%9C%EB%B2%84-%EA%B5%AC%EC%B6%95%EA%B8%B0-part-4-kolla-ansible-%EC%84%A4%EC%B9%98-%EC%8B%9C-%EB%A7%88%EC%A3%BC%ED%95%9C-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EB%AC%B8%EC%A0%9C-484efa8a1c38">서버 구축기 — Part 4. Kolla-ansible 설치 시 마주한 네트워크 문제</a> was originally published in <a href="https://medium.com/s0okju-tech">S0okJu Technology Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">서버 구축기 — Part 3. 쿠버네티스에서 대시보드 접속 그리고 운영의 문제점</title><link href="https://s0okju.github.io/blog/2025/part-3/" rel="alternate" type="text/html" title="서버 구축기 — Part 3. 쿠버네티스에서 대시보드 접속 그리고 운영의 문제점"/><published>2025-05-27T15:33:54+00:00</published><updated>2025-05-27T15:33:54+00:00</updated><id>https://s0okju.github.io/blog/2025/-part-3------</id><content type="html" xml:base="https://s0okju.github.io/blog/2025/part-3/"><![CDATA[<h3><strong>서버 구축기 — Part 3. 쿠버네티스에서 대시보드 접속 그리고 운영의 문제점</strong></h3> <h3>현황</h3> <p>지금까지 Multipass 를 활용하여 쿠버네티스 환경을 만들고, 실제로 적용하는 과정을 거쳤습니다.<br/>현재 3개의 노드로 구성되어 있으며, nfs, mysql, mongo와 같이 데이터베이스가 있는 인스턴스는 nfs, 젠킨스 서버는 ops 인스턴스에 저장했습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/524/1*5pn8A61R_Jfh7ASyXzBBFw.png"/><figcaption>Multipass 인스턴스 목록</figcaption></figure> <p>최대한 클라우드 NFS 스토리지처럼 구현하고 싶어 별도의 인스턴스에 NFS 서버를 설치해서 사용했습니다.</p> <h3>대시보드 확인하기</h3> <p>일단 쿠버네티스 내에서는 외부와 통신할 수 있는 기능은 크게 두 가지입니다.</p> <ul><li>Ingress</li><li>Nodeport</li></ul> <p>일단 사설 네트워크 내에서만 사용할 예정이므로 Nodeport 방식을 사용하겠습니다.</p> <h4>예제 — Grafana ui 확인하기</h4> <p>예로 프로메테우스를 사용한다고 가정해보자. 우선 grafana 대시보드에 접근해야 합니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/655/1*p0uenUYjTO_6_5o2BunIcw.png"/></figure> <p>grafana ui와 관련된 애플리케이션의 service type를 nodeport로 변경해준 후에 Host server(192.168.50.27)에 방화벽을 설정하면 192.168.50.27 주소로 Grafana ui에 접근할 수 있습니다.</p> <p>그러나 서버에 문제가 생기면 재배포하게 되는데, 쿠버네티스 스케줄러에 따라서 다른 노드에 배치될 수 있습니다. 이럴 경우 배치된 노드를 확인해가며 기존의 방화벽을 닫고, 새로운 방화벽을 열어줘야 합니다.</p> <p>쿠버네티스에서는 여러 스케줄링 방식이 있는데¹, 그 중 간단한 nodeSelector 로 node1에 강제 배치하여 정적으로 사용할 수 있도록 구성했습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/719/1*j49EtiFb1JxM4OLNGbD0Mw.png"/></figure> <h4>nodeSelector 설정</h4> <p>node1에 label를 type=ui로 설정했습니다.</p> <pre>kubectl label nodes node1 type=ui</pre> <figure><img alt="" src="https://cdn-images-1.medium.com/max/473/1*R3MGl0m_nrLHaQ0XaCYiJg.png"/><figcaption>kubectl get node node1</figcaption></figure> <h4>grafana ui service type 변경</h4> <p>Prometheus, Grafana의 원활한 설치를 위해 <a href="https://github.com/prometheus-operator/kube-prometheus">kube-prometheus helm</a>을 사용했습니다. helm 차트의 특성상 values.yaml를 설정하여 값을 주입하므로 <strong>values.yaml</strong> 를 확인해야 합니다.<br/>kube-prometheus helm 차트는 여러 helm 차트로 구성되어 있어 설정하고 싶은 서비스의 <strong>values.yaml</strong>를 설정했습니다.</p> <p>kube-prometheus-stack/charts/grafana/values.yaml 파일에 대략 216번째 줄에 아래와 같은 서비스 타입을 Nodeport로 변경했습니다.</p> <pre>## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service).<br />## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it.<br />## ref: http://kubernetes.io/docs/user-guide/services/<br />##<br />service:<br />  enabled: true<br />    # type: ClusterIP<br />  type: NodePort # Custom <br />  loadBalancerIP: &quot;&quot;<br />  loadBalancerClass: &quot;&quot;<br />  loadBalancerSourceRanges: []<br />  port: 80<br />  targetPort: 3000<br />  nodePort: 30060 # Custom </pre> <h4>helm nodeSelector 설정</h4> <p>kube-prometheus-stack/charts/grafana/values.yaml 의 314번째 줄에 node1에 배치되도록 설정했습니다.</p> <pre>## Node labels for pod assignment<br />## ref: https://kubernetes.io/docs/user-guide/node-selection/<br />#<br />nodeSelector:<br />  type: ui</pre> <p>helm으로 배포한 후 pod 리스트를 보면 <strong>{helm으로 배포한 이름}-grafana-{문자열}</strong> 보이는데 파드 내부에는 grafana와 관련된 컨테이너가 포함되어 있습니다. 즉, 우리가 찾던 ui도 grafana pod 내부에 있다는 것입니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/679/1*pO3fix1FmK71vyE8urz-NQ.png"/></figure> <p>grafana 파드의 정보를 보면 nodeSelector가 정상적으로 적용되었음을 알 수 있습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1012/1*LJX3CBrjJgGqc-DzA1pE8A.png"/></figure> <h4>방화벽 설정</h4> <p>node1 인스턴스의 nodeport(30060)에 트래픽이 들어올 수 있도록 허용했습니다.</p> <pre>sudo iptables -t nat -I PREROUTING -i enp3s0 -p tcp --dport 30060 -j DNAT --to-destination 10.120.52.23:30060<br />sudo iptables -I FORWARD 1 -p tcp -d 10.120.52.23 --dport 30060 -j ACCEPT</pre> <ol><li>30060 포트로 enp3s0 인터페이스에 들어오는 트래픽을 10.120.52.23:30060로 리다이렉션한다.</li><li>10.120.52.23:30060에 접속하는 트래픽을 허용한다.</li></ol> <p>서버의 IP로 접속하면 그라파나 대시보드를 확인할 수 있습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1008/1*0SQyfRikdCiuTQC9-iSvww.png"/><figcaption>Grafana Dashboard</figcaption></figure> <h3>문제점</h3> <p>Multipass로 인스턴스를 만들고, 외부와의 접속을 위해 트래픽을 제어하는 것은 문제점이 있습니다.</p> <ol><li>외부와의 접속을 할때마다 iptable를 설정해야 한다.<br/>서버가 shutdown이 될 경우 이전에 설정했던 iptables rule이 사라진다. 이럴 경우 규칙을 다시 설정하거나 이를 대비해서 별도의 패키지를 설치해서 iptables이 영속성이 있도록 설정²해야 합니다.</li><li>인스턴스가 많으면 많을수록 관리하기 어렵다.<br/>쿠버네티스 이외에도 다양한 인스턴스가 있지만 일리리 명령어로 상태를 확인하고 설정하는 것은 번거롭습니다. 그러므로 별도의 <strong>오케스트레이션 도구가 필요합니다.</strong></li></ol> <p>위의 해결책으로 <a href="https://www.openstack.org/software/project-navigator/openstack-components#openstack-services">OpenStack</a> 서버를 구축하고자 합니다. 인스턴스를 public cloud 처럼 자유자재로 생성 및 삭제할 수 있으며, 네트워크 설정이 용이했습니다. 또한 클라우드에 사용되는 기술을 대부분 활용할 수 있어 클라우드를 알아가는데 좋은 도구라고 생각했습니다.</p> <p>다음 포스팅은 기존의 서버를 포맷시키고, openstack 서버 구축기로 찾아오겠습니다.</p> <ol><li><a href="https://kubernetes.io/ko/docs/tasks/configure-pod-container/assign-pods-nodes/">https://kubernetes.io/ko/docs/tasks/configure-pod-container/assign-pods-nodes/</a>↩︎</li><li><a href="https://unix.stackexchange.com/questions/52376/why-do-iptables-rules-disappear-when-restarting-my-debian-system">https://unix.stackexchange.com/questions/52376/why-do-iptables-rules-disappear-when-restarting-my-debian-system</a>↩︎</li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=481343ac0c3f" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/s0okju-tech/%EC%84%9C%EB%B2%84-%EA%B5%AC%EC%B6%95%EA%B8%B0-part-3-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4%EC%97%90%EC%84%9C-%EB%8C%80%EC%8B%9C%EB%B3%B4%EB%93%9C-%EC%A0%91%EC%86%8D-%EA%B7%B8%EB%A6%AC%EA%B3%A0-%EC%9A%B4%EC%98%81%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90-481343ac0c3f">서버 구축기 — Part 3. 쿠버네티스에서 대시보드 접속 그리고 운영의 문제점</a> was originally published in <a href="https://medium.com/s0okju-tech">S0okJu Technology Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">GitOps 전환기 — Part 5. ArgoCD를 활용한 CD 구성하기</title><link href="https://s0okju.github.io/blog/2025/gitops-part-5-argocd-cd/" rel="alternate" type="text/html" title="GitOps 전환기 — Part 5. ArgoCD를 활용한 CD 구성하기"/><published>2025-05-26T08:40:28+00:00</published><updated>2025-05-26T08:40:28+00:00</updated><id>https://s0okju.github.io/blog/2025/gitops-part-5-argocd--cd-</id><content type="html" xml:base="https://s0okju.github.io/blog/2025/gitops-part-5-argocd-cd/"><![CDATA[<h3>GitOps 전환기 — Part 5. ArgoCD를 활용한 CD 구성하기</h3> <h3>ArgoCD란?</h3> <p>ArgoCD는 GitOps를 구현하기 위한 도구 중 하나로 쿠버네티스 애플리케이션의 자동 배포를 위한 오픈소스 도구입니다. CI/CD 과정에서 CD 부분을 담당하며, <strong>Git 저장소의 변경 사항을 감지</strong>하여 자동으로 쿠버네티스 애플리케이션에 반영시킬 수 있습니다.</p> <h4>GitOps란?</h4> <p>GitOps란 DevOps의 적용하는 실천 방법 중 하나로 클라우드 네이티브 애플리케이션을 대상으로 한 지속적인 배포에 초점을 맞추고 있습니다. 이름에서 나타나듯이 배포와 운영에 관련된 모든 요소를 Git으로 관리하는 것에 초점이 맞춰져 있습니다.</p> <p>GitOps는 배포와 관련된 모든 것을 선언형 기술 형태로 작성하여 이러한 파일들을 레포지토리 형태로 관리합니다. 그리고 선언형 기술서와 운영 환경 간 상태 차이가 없도록 유지시켜주는 자동화 시스템을 구성합니다.</p> <h4>어떻게 변경사항을 감지할까?</h4> <p>ArgoCD가 Git 저장소의 변경사항을 확인하는 방법은 크게 두 가지가 있습니다.</p> <ul><li>Polling: 3초마다 저장소와 ArgoCD의 현재 상태와 비교하여 sync 수행</li><li>Webhook: Git 저장소의 변경 사항에 따라 즉시 sync 수행</li></ul> <p>Polling 상태의 경우 최악의 상황에 따라 3분 후에 변경사항을 반영될 수 있습니다. 그러므로 실시간 동기화를 중요하다고 생각한다면 Webhook Event를 사용하면 됩니다.</p> <h3>프로젝트에 적용하기</h3> <blockquote>해당 게시글은 프로젝트에 ArgoCD를 어떻게 적용했는지 작성되어 있습니다. 설치 방법은 <a href="https://argo-cd.readthedocs.io/en/stable/getting_started/">공식 문서</a> 를 참고하시길 바랍니다.</blockquote> <h4>프로젝트 구성</h4> <p>현재 프로젝트에서는 ctf-app, challenge-api의 이미지 해시값을 기반으로 Config Repository manifest를 업데이트하는 방식으로 구성했습니다.</p> <p>ArgoCD sync 결과를 프로젝트마다 할당된 슬랙 채널에 전송할 수 있도록 설정했습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/831/1*J7owwscpwWS-IWITaZNxCw.png"/><figcaption>CI/CD 구성도</figcaption></figure> <h4>Polling vs Webhook Event</h4> <p>프로젝트에서는 Polling 방식으로 수행했습니다. <br/>지속적인 배포를 수행하기 위해서는 Webhook 방식으로 수행하는 것이 적절합니다. 그러나 <strong>관리해야 하는 프로젝트 수가 적고 ArgoCD 자체를 처음 사용하기도 해서 polling 방식을 선택</strong>하게 되었습니다.</p> <p>위에서 Polling 상태의 경우 최악의 상황에 따라 3분 후에 변경사항을 반영될 수 있다고 말씀 드렸습니다. 3분이 수치 상으로 짧다고 생각할지라도 체감상으로 엄청 길게 느껴집니다. 프로젝트 수가 적고 배포 일정이 2–3시간으로 짧아 수동적으로 sync를 수행하며 비효율적인 대기 시간을 줄였습니다.</p> <p>다음 프로젝트에 ArgoCD를 활용하게 된다면 Webhook 방식을 사용할 것입니다.</p> <h4>ArgoCD가 다른 Namespace에 배포하는 법</h4> <p>배포 구성도를 보면 ArgoCD는 argocd namespace에 존재하고 배포해야 할 애플리케이션은 hexactf namespace에 존재합니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jqp9J4ld52yDBG7coWAgNw.png"/><figcaption>namespace 별 쿠버네티스 애플리케이션 구성도</figcaption></figure> <p>default가 아닌 다른 namespace에 애플리케이션을 배포하려면 project 단위로 namespace를 허용해야 합니다. 그렇지 않으면 permission denied 오류를 만날 수 있습니다.</p> <p>default project가 아닌 hexactf라는 프로젝트를 생성하여 hexactf namespace 배포를 허용했습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jpZGAiEYPJu45VH-z0riEg.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*O47ruaiQPeILVPWDV4MhnQ.png"/><figcaption>project: hexactf namespace 배포 허용</figcaption></figure> <p>namespace 뿐만 아니라 쿠버네티스 리소스도 허용/차단을 설정할 수 있습니다. <br/>프로젝트 중에서는 ClusterRole, ClusterRoleBinding이 필요하므로 Cluster Resource allow list에 추가했습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*I1VUeucoHfDG-heecWq8Vg.png"/><figcaption>project: resource allow list에 ClusterRole, ClusterRoleBinding 추가</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZLZZKM0_jruTS4uCjLkzkA.png"/><figcaption>challenge-api 쿠버네티스 배포 결과</figcaption></figure> <h4>Config Repository 구성</h4> <p>Config Repository로 관리해야 하는 프로젝트는 두 개가 있습니다. 각각의 프로젝트는 서로 다른 개발자가 저장소를 사용하고 있습니다. 저는 배포를 담당하고 있으며 Kustomize 패키징 도구를 활용하여 배포 코드를 관리하고 있습니다.</p> <p><strong>하나의 저장소에 다수의 manifest 관리의 문제점</strong></p> <p>저는 초기에 하나의 저장소에 2개의 프로젝트를 폴더로 분리하여 관리했습니다. 그 이유는 배포를 저 혼자 담당하며 분리해서 관리하는 것이 비효율적이라고 판단했기 때문입니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/301/1*a80fDpL7bw0F-B6YuZF-GA.png"/><figcaption>Config Repository: Kustomize 구조</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/952/1*Zn2U5v2xvcOuBxScc6DL4w.png"/><figcaption>ArgoCD에 등록한 application 목록</figcaption></figure> <p>그러나 slack notification을 설정하면서 이러한 구성이 잘못되었음을 알게 되었습니다.</p> <blockquote>slack notification 설정은 <a href="https://argo-cd.readthedocs.io/en/stable/operator-manual/notifications/services/slack/">공식 문서</a>를 참고하시길 바랍니다.</blockquote> <p>비록 <strong>하나의 애플리케이션의 manifest만 변경될지라도 모든 애플리케이션이 변경되었다고 간주되어 모든 슬랙 채널에 알림을 보내는 것</strong>이었습니다. 이로 인해 다수의 불필요한 알림을 보내게 되어 팀원에게 불필요한 잡음을 준다고 생각했습니다.</p> <p>문제를 해결하기 위해 slack 메세지를 보니 Revision의 해시가 같다는 공통점을 찾았습니다. 해당 저장소의 커밋 해시가 바뀌면서 하위의 모든 애플리케이션이 OutOfSync로 인식하고 sync를 수행하고 있었습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/551/1*dzlQh1y24T6nav8f4QVgPg.png"/><figcaption>challenge-api ArgoCD Slack message</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/551/1*DN6Pg3l9zkOfujYUirdS_w.png"/><figcaption>ctf-app ArgoCD Slack message</figcaption></figure> <p>ArgoCD Issue를 보면 auto-sync를 설정할때 위와 같은 작업은 자연스러운 것이라고 말합니다. 여기서 auto-sync(자동 동기화)는 ArgoCD가 Git 저장소와 Kubernetes 클러스터 리소스의 상태를 자동으로 비교하며 변경사항 발생 시 적용해주는 기능입니다.</p> <p>내용을 종합했을때 제가 설정한 Config Repository 구조는 잘못되었음을 알 수 있습니다.</p> <blockquote>Argo CD uses the SHA to determine whether an auto-sync needs to occur. This would be a pretty fundamental change.<br/>출처 — <a href="https://github.com/argoproj/argo-cd/issues/1646">https://github.com/argoproj/argo-cd/issues/1646</a></blockquote> <p>하나의 저장소로만 관리할 시 프로젝트 별 변경 추적의 어려워지고 불필요한 롤백이 발생하게 됩니다. 왜냐하면 저장소 하나가 하나의 대상으로 간주되기 때문입니다. <br/>이러한 문제점으로 인해 GitOps의 장점을 활용하지 못하게 됩니다.</p> <p><strong>Best Practice: 프로젝트 manifest 마다 저장소를 분리하자</strong></p> <p>공식 문서에 의하면 커밋 해시 단위로 manifest가 불변이여야 한다는 best practice가 있으며, 이는 곧 <strong>각각의 프로젝트는 서로 다른 저장소에 관리되어야 한다는 의미로 해석</strong>할 수 있습니다.</p> <blockquote>Ensuring Manifests At Git Revisions Are Truly Immutable<br/>출처 — <a href="https://argo-cd.readthedocs.io/en/stable/user-guide/best_practices/#ensuring-manifests-at-git-revisions-are-truly-immutable">https://argo-cd.readthedocs.io/en/stable/user-guide/best_practices/#ensuring-manifests-at-git-revisions-are-truly-immutable</a></blockquote> <p>저장소의 분리는 관리해야 하는 저장소가 늘어나는 대신에 정확한 변경 이력 추적과 적합한 범위의 롤백이 가능하여 GitOps의 장점을 활용할 수 있게 됩니다.</p> <p><strong>추후 계획: 프로젝트 manifest 마다 저장소 분리하기</strong><br/>문제를 뒤늦게 인지해서 이번 프로젝트에서는 하나의 저장소에 다수의 manifest를 관리했습니다.</p> <p>그러나 다음 기회에는 프로젝트마다 분리하여 불필요한 알림을 줄이고, 정확하게 변경 이력을 추적할 수 있도록 설정할 것입니다.</p> <h3>시리즈를 마치며</h3> <p>처음 CI/CD를 사용하지 않고 배포를 수행하다가 비효율성을 느껴 CI/CD를 도입하게 되었습니다. 막상 적용하고 나니 도구의 장점을 제대로 살리지 못한 것 같습니다.</p> <ul><li>Tekton(CI): 재활용 가능한 코드</li><li>ArgoCD(CD): 프로젝트 별 manifest 관리,Webhook을 활용</li></ul> <p>다음에는 부족하다고 생각한 부분을 모두 수정하여 보완해야 할 것 같습니다.</p> <h3>관련 시리즈</h3> <ul><li><a href="https://medium.com/s0okju-tech/gitops-%EC%A0%84%ED%99%98%EA%B8%B0-1-%EC%82%AC%EC%A0%84-%EA%B3%84%ED%9A%8D-825a780b096a">GitOps 전환기 — Part 1. 사전 계획</a></li><li><a href="https://medium.com/s0okju-tech/gitops-%EC%A0%84%ED%99%98%EA%B8%B0-2-mariadb-operator%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-mariadb-%EA%B5%AC%EC%84%B1-ee3d61a7b32d">GitOps 전환기 — Part 2. MariaDB Operator를 활용한 MariaDB 구성</a></li><li><a href="https://medium.com/s0okju-tech/gitops-%EC%A0%84%ED%99%98%EA%B8%B0-3-gitops-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EA%B3%84%ED%9A%8D-%EC%84%B8%EC%9A%B0%EA%B8%B0-4af79e4ed80f">GitOps 전환기 — Part 3. GitOps 파이프라인 계획 세우기</a></li><li><a href="https://medium.com/s0okju-tech/gitops-%EC%A0%84%ED%99%98%EA%B8%B0-4-tekton%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-ci-%EA%B5%AC%EC%84%B1%ED%95%98%EA%B8%B0-f4898483329c">GitOps 전환기 — Part 4. Tekton을 활용한 CI 구성하기</a></li></ul> <h3>References</h3> <ul><li><a href="https://dobby-isfree.tistory.com/216">https://dobby-isfree.tistory.com/216</a></li><li><a href="https://wlsdn3004.tistory.com/37">https://wlsdn3004.tistory.com/37</a></li><li><a href="https://github.com/argoproj/argo-cd/issues/1646">https://github.com/argoproj/argo-cd/issues/1646</a></li><li><a href="https://argo-cd.readthedocs.io/en/stable/">https://argo-cd.readthedocs.io/en/stable/</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=251936a8a31f" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/s0okju-tech/gitops-%EC%A0%84%ED%99%98%EA%B8%B0-part-5-argocd%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-cd-%EA%B5%AC%EC%84%B1%ED%95%98%EA%B8%B0-251936a8a31f">GitOps 전환기 — Part 5. ArgoCD를 활용한 CD 구성하기</a> was originally published in <a href="https://medium.com/s0okju-tech">S0okJu Technology Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">OSSCA: OpenStack CLI /SDK 체험형 활동 후기</title><link href="https://s0okju.github.io/blog/2025/ossca-openstack-cli-sdk/" rel="alternate" type="text/html" title="OSSCA: OpenStack CLI /SDK 체험형 활동 후기"/><published>2025-05-25T07:53:59+00:00</published><updated>2025-05-25T07:53:59+00:00</updated><id>https://s0okju.github.io/blog/2025/ossca-openstack-cli-sdk---</id><content type="html" xml:base="https://s0okju.github.io/blog/2025/ossca-openstack-cli-sdk/"><![CDATA[<blockquote>해당 게시글은 <a href="https://www.oss.kr/open_up_intro">OpenUp</a>에서 주관하는 <a href="https://www.contribution.ac/">2025 OSSCA 체험형 1차</a> 후기입니다.</blockquote> <h3>OpenStack CLI/SDK 프로젝트를 선택한 이유</h3> <p>OpenStack을 조금씩 활용하면서 작게나마 관심을 가지게 되었습니다. 공식 문서 등 여러 문서를 보면서 공부를하고 있지만 전문적으로 배울 수 있는 기회는 적었습니다.</p> <p>그 중 OSSCA에서 OpenStack Client 프로젝트에 대해 알게 되었고 다른 프로젝트를 보지 않고 바로 OpenStack으로 지원했습니다.(1지망, 2지망 모두 OpenStack으로 지원했습니다.)</p> <h3>활동 방식</h3> <p>팀마다 다르겠지만 주 1회씩 온라인 혹은 오프라인(택1)로 진행됩니다. 저는 지방에 거주하고 있기 때문에 대부분 온라인에서 수행했습니다.</p> <h3>OpenStack의 기여 방식은 독특하다.</h3> <p>대부분 오픈소스 기여라고 한다면 GitHub 플랫폼을 주로 떠올릴 것 같습니다.그러나 OpenStack의 경우에는 Gerrit(Code Review), StoryBoard(Issue), irc(Communication), Opendev(Repository)등 역할에 따라서 분리되어 있었습니다.</p> <p>실제로 프로젝트에 merge하기 위해서는 Gerrit만의 독특한코드 리뷰 시스템이 있습니다. 코어 리뷰어로부터 점수를 얻어야하며, zuul이라는 CI 도구를 통해 Verified 점수를 얻어야 합니다. <br/>플랫폼이 많아서 헷갈릴 뿐이지 리뷰 과정 자체는 정말 체계적인 것 같다는 인상을 받았습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-YmBxMK2gRP5y__fjGh8Lg.png"/><figcaption>실제로 실습한 Gerrit 페이지</figcaption></figure> <h3>Openstack CLI/SDK와 관련된 활동</h3> <p>저는 openstack 명령어를 많이 활용합니다. <br/>제가 자주 사용한 명령어가 openstack-client임을 이번 기회에 처음 알게 된 것 같습니다.</p> <p>이번에는 크게 두 가지 활동을 수행했습니다.</p> <ol><li>코드 수정해보기</li><li>수정한 코드에 대한 테스트 코드 작성해보기</li></ol> <p>구체적인 내용을 알고 싶다면 다음에 직접 참여해보는 것을 추천드립니다.</p> <h3>OpenStack 실무 이야기</h3> <p>마지막 2주차부터 실무에서는 OpenStack을 어떻게 활용하냐에 대한 강의가 있었습니다.</p> <p>그리고 제가 평소에 궁금했지만 알 수 없었던 부분을 전부 알려주셨습니다.</p> <p>ex) 왜 컴포넌트 Controller 부분은 도커로 배포해도 괜찮은 걸까?</p> <p>클라우드 시스템 관련해서는 블로그와 같은 공개된 매체에서 얻기 어려운 것 같습니다. 그래서 더욱 멘토님의 이야기 하나하나 소중하게 들었습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/420/1*iHmgmR0HQlEfuXGAgSPHMg.gif"/><figcaption>나의 뇌세포들</figcaption></figure> <h3>느낀점</h3> <p><strong>Python 공부해야겠다.</strong></p> <p>지금까지 저는 이론적으로 공부를 아주 조금¹ 하면서 코딩에 대해서는 등한시했던 것 같습니다. 특히 <strong>오픈스택이 오픈소스였다는 것을 잠시 잊었던 것 같습니다.</strong></p> <p>소스코드를 보면서 Python만으로 클라우드 시스템을 만들 수 있다는 것에 감탄했습니다. 더 나아가 체계적인 테스트 코드를 보면서 테스트 코드를 잘 짜는 법이 무엇인가에 대해 고민하기 시작했습니다.</p> <p>앞으로 나아가기 위해서는 Python 공부를 하며 OpenStack의 소스코드를 교재 삼아 직접 활용하는 연습을 해야겠습니다.</p> <p><strong>기여 활동에 두려워하지 말자</strong></p> <p>제가 꼭 해보고 싶은 일 중에 <strong>오픈소스 기여</strong>가 있습니다. 그러나 (마음의) 진입장벽이 있어서 선뜻 해보지 못했습니다.</p> <p>프로젝트 활동 중에서 실제로 기여를 해보는 과제가 있었습니다. 비록 최종적으로 기여를 하지 못했지만 직접 issue를 찾아보고 배운 내용을 토대로 문제 사항을 파악한 것만으로도 자신감을 얻은 것 같습니다.</p> <p>올해의 목표로 오픈소스를 1회 기여해보겠습니다.</p> <p>¹: 공부했다고 하기에 민망한 수준.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=dbeb653a7542" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/s0okju-tech/ossca-openstack-cli-sdk-%EC%B2%B4%ED%97%98%ED%98%95-%ED%99%9C%EB%8F%99-%ED%9B%84%EA%B8%B0-dbeb653a7542">OSSCA: OpenStack CLI /SDK 체험형 활동 후기</a> was originally published in <a href="https://medium.com/s0okju-tech">S0okJu Technology Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Gin 예외처리 — Part 2. 커스텀 예외처리 구현하기</title><link href="https://s0okju.github.io/blog/2025/gin-part-2/" rel="alternate" type="text/html" title="Gin 예외처리 — Part 2. 커스텀 예외처리 구현하기"/><published>2025-05-19T12:07:37+00:00</published><updated>2025-05-19T12:07:37+00:00</updated><id>https://s0okju.github.io/blog/2025/gin-part-2---</id><content type="html" xml:base="https://s0okju.github.io/blog/2025/gin-part-2/"><![CDATA[<h3><strong>Gin 예외처리 — Part 2. 커스텀 예외처리 구현하기</strong></h3> <p><a href="https://medium.com/s0okju-tech/gin-%EC%98%88%EC%99%B8%EC%B2%98%EB%A6%AC-part-1-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EC%BD%94%EB%93%9C%EB%A1%9C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EB%8A%94-%EC%98%88%EC%99%B8%EC%B2%98%EB%A6%AC-%EB%AC%B8%EC%A0%9C%EC%A0%90-c2afdf7e7be2">이전 글</a>에서는 Gin의 라이브러리를 활용하지 않고 예외처리를 구현할 시 발생할 수 있는 문제점에 대해 설명했습니다. 이제부터 Gin에서 제공하는 기능을 활용하여 예외처리를 수행하도록 하겠습니다.</p> <h3>Error Wrapping</h3> <h4>Gin에서의 예외 처리</h4> <p><a href="https://pkg.go.dev/github.com/gin-gonic/gin#Context.Error">Gin Error() 함수</a>에 의하면 Gin의 Context에 Error를 담은 후에 Middleware에서 처리하는 것을 권장하고 있습니다.</p> <blockquote><em>Error attaches an error to the current context. The error is pushed to a list of errors. </em><strong><em>It’s a good idea to call Error for each error that occurred during the resolution of a request.</em></strong><em> A middleware can be used to collect all the errors and push them to a database together, print a log, or append it in the HTTP response. Error will panic if err is nil.</em></blockquote> <p>라이브러리 코드를 보면Errors필드가 정의되어 있는데 errorMsg는[]*Error에러 리스트로 타입을 가지고 있습니다. 위의 설명대로 error의 리스트가 Context 내부에 구현되어 있는 것입니다.</p> <pre>// gin/context.go<br />type Context struct {  <br /> // ... <br />   Errors errorMsgs<br /> // .. <br />}<br /><br />// gin/errors.go<br />type errorMsgs []*Error</pre> <p>즉,ctx.Error를 활용하여 입력받은 에러를 Gin의 에러로 감싼 후에 Context의 Errors 리스트에 넣게 됩니다.</p> <pre>// gin/context.go<br />// Error attaches an error to the current context. The error is pushed to a list of errors.// It&#39;s a good idea to call Error for each error that occurred during the resolution of a request.// A middleware can be used to collect all the errors and push them to a database together,  <br />// print a log, or append it in the HTTP response.  <br />// Error will panic if err is nil.<br />func (c *Context) Error(err error) *Error {  <br />    if err == nil {  <br />       panic(&quot;err is nil&quot;)  <br />    }  <br />  <br />    var parsedError *Error  <br />    ok := errors.As(err, &amp;parsedError)  <br />    if !ok {  <br />       parsedError = &amp;Error{  <br />          Err:  err,  <br />          Type: ErrorTypePrivate,  <br />       }  <br />    }  <br />  <br />    c.Errors = append(c.Errors, parsedError)  <br />    return parsedError  <br />}</pre> <p>Context 내부에 있는 Error 리스트는 Middleware에서 처리하게 됩니다.</p> <p>Gin에서는 <a href="https://pkg.go.dev/github.com/gin-gonic/gin#HandlerFunc">HandlerFunc</a>를 slice로 구현된 <a href="https://pkg.go.dev/github.com/gin-gonic/gin#HandlersChain">HandlerChain</a>이 있는데 이는 Gin이 각가지의 Handler를 Chain내에 넣고 처리됩니다.</p> <pre>type HandlersChain []HandlerFunc</pre> <p>그럼 오류를 어떻게 발생시키면 될까? HandlerChain 내에 있는 대기 중인(Pending) Handler를 호출하지 않도록 하면됩니다.</p> <p>즉, Context를 <a href="https://pkg.go.dev/github.com/gin-gonic/gin#Context.Abort">Abort</a>를 활용하여 이후 처리할 Handler가 없어지면서 종료시키는 것입니다.</p> <blockquote><em>Abort prevents pending handlers from being called. Note that this will not stop the current handler. Let’s say you have an authorization middleware that validates that the current request is authorized. If the authorization fails (ex: the password does not match), </em><strong><em>call Abort to ensure the remaining handlers for this request are not called.</em></strong></blockquote> <h3>프로젝트에 적용하기</h3> <h4>절차</h4> <p>위의 내용을 가지고 실제로 적용해보겠습니다.<br/>프로젝트 구성은 3계층으로 구성되어 있으며 커스텀 에러 감싸기는 서비스 로직에 수행하도록 구현했습니다.</p> <ol><li>Service 계층에 Error Wrapping을 한다.</li><li>Controller에 context 내에 있는 에러 리스트에 예외를 넣는다.</li><li>Middleware에 Error Wrapping한 것을 Unwrapping 하면서 예외 타입을 학인한다.</li><li>커스텀 에러라면 WebCode에 따른 응답값을 반환한다.</li></ol> <p>그림으로 표현하자면 아래와 같습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*xL_7nVFHJjSLONeBMbqYng.png"/><figcaption>Custom error 구조도</figcaption></figure> <h4>코드</h4> <p>코드는 아래의 두 사이트를 참고했습니다.</p> <ul><li><a href="https://d2.naver.com/helloworld/2690202">Naver D2, Golang, 그대들은 어떻게 할 것인가 — 3. error 래핑</a></li><li><a href="https://d2.naver.com/helloworld/6507662">Naver D2, Golang, 그대들은 어떻게 할 것인가 — 4. error 핸들링</a></li></ul> <p><strong>Service Layer</strong></p> <p>Persistence Layer에서 얻은 에러 값을 직접 받은 후에 Service 계층에서 적절하게 NetError로 감쌌습니다.</p> <pre>// service<br />func (s *MemberService) CreateMember(ctx *gin.Context, req data.RegisterReq) (*ent.Member, *errorutils.NetError) {<br /> // Check member Exist<br /> existedMem, err := s.Store.GetMemberByEmail(ctx, req.Email)<br /> if err != nil {<br />  return nil, &amp;errorutils.NetError{Code: codes.MemberInternalServerError, Err: err}<br /> }<br /> if ent.IsNotFound(err) {<br />  mem, err2 := s.Store.Create(ctx, req)<br />  if err2 != nil {<br />   return nil, &amp;errorutils.NetError{Code: codes.MemberCreationError, Err: err2}<br />  }<br />  return mem, nil<br /> }<br /> return existedMem, nil<br />}</pre> <p><strong>Controller Layer</strong></p> <p>Controller는 Service 계층에서 감싼 커스텀 에러를 받은 후에 Context의 에러 리스트에 넣었습니다.</p> <pre>func (controller *MemberController) RegisterMember(ctx *gin.Context) {<br /> req := data.RegisterReq{}<br /> err := ctx.ShouldBindJSON(&amp;req)<br /> if err != nil {<br />  _ = ctx.Error(errorutils.NewNetError(codes.MemberInvalidJson, err))<br />  return<br /> }<br /><br /> // Create member<br /> mem, err2 := controller.service.CreateMember(ctx, req)<br /> if err2 != nil {<br />  // Service 계층에서 받은 에러를 Context 내 에러 리스트에 넣는다.<br />  _ = ctx.Error(err2)<br />  return<br /> }<br /><br /> mid := data.MemberId{MemberId: mem.ID}<br /> response.SuccessWith(ctx, codes.MemberCreationSuccess, mid)<br />}</pre> <p><strong>Middleware</strong></p> <p>에러 응답값을 반환할 HandlerFunc를 구현했습니다.</p> <ol><li><a href="https://pkg.go.dev/github.com/gin-gonic/gin#Context.Next">Next</a>를 활용하여 대기 중인 핸들러를 실행시킨다.</li><li><a href="https://pkg.go.dev/errors#example-As">errors.As</a>를 활용하여 Context 에러 리스트에 있는 에러가 커스텀 에러인지 확인한다. 정확히 말하자면 에러를 unwrapping하면서 커스텀 에러인지 확인한 후에 있다면 netError에 넣게 된다.</li><li>WebCode를 활용하여 응답값을 얻은 후 <a href="https://pkg.go.dev/github.com/gin-gonic/gin#Context.AbortWithStatusJSON">AbortWithStatusJson</a> 를 활용하여 Response json을 전송한다.</li></ol> <pre>func ErrorHandler() gin.HandlerFunc {  <br />    return func(ctx *gin.Context) {  <br />  // Pending 중인 핸들러 실행 <br />       ctx.Next()  <br />       // JSON이 두번 쓰이는 것을 대비해서 Body 확인  <br />       isBodyWritten := ctx.Writer.Written()  <br />       err := ctx.Errors.Last()  <br />  <br />       if err != nil {  <br />        // 커스텀 에러인지 확인 <br />          var netErr *errorutils.NetError  <br />          if errors.As(err, &amp;netErr) {  <br />             code := netErr.GetCode()  <br />             statusCode := codes.GetStatus(code)  <br />             res := response.NewErrorResponse(code)  <br />  <br />             // Abort with the appropriate status code and response  <br />             if !isBodyWritten {  <br />                ctx.AbortWithStatusJSON(int(statusCode), res)  <br />             }  <br />          } else {  <br />             res := response.NewErrorResponse(codes.GlobalInternalServerError)  <br />             if !isBodyWritten {  <br />                ctx.AbortWithStatusJSON(http.StatusInternalServerError, res)  <br />             }  <br />          }  <br />  <br />       }  <br />    }  <br />}</pre> <h3>마무리</h3> <p>Go의 예외 매커니즘은 다른 언어와 달라서 틀을 잡는데 많은 시간을 사용했습니다. 공식 문서나 다른 사람들의 예제 코드를 분석하면서, Go스러움이 무엇인지 조금 배운 것 같습니다.</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=eadba0b377ee" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/s0okju-tech/gin-%EC%98%88%EC%99%B8%EC%B2%98%EB%A6%AC-part-2-%EC%BB%A4%EC%8A%A4%ED%85%80-%EC%98%88%EC%99%B8%EC%B2%98%EB%A6%AC-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0-eadba0b377ee">Gin 예외처리 — Part 2. 커스텀 예외처리 구현하기</a> was originally published in <a href="https://medium.com/s0okju-tech">S0okJu Technology Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Gin 예외처리 — Part 1. 프로젝트 코드로 살펴보는 예외처리 문제점</title><link href="https://s0okju.github.io/blog/2025/gin-part-1/" rel="alternate" type="text/html" title="Gin 예외처리 — Part 1. 프로젝트 코드로 살펴보는 예외처리 문제점"/><published>2025-05-14T15:12:37+00:00</published><updated>2025-05-14T15:12:37+00:00</updated><id>https://s0okju.github.io/blog/2025/gin-part-1-----</id><content type="html" xml:base="https://s0okju.github.io/blog/2025/gin-part-1/"><![CDATA[<h3>Gin 예외처리 — Part 1. 프로젝트 코드로 살펴보는 예외처리 문제점</h3> <h3>Go와 예외처리</h3> <h4>일반적인 Go의 예외처리 방법</h4> <p>Go에서는 함수에서 반환된 <a href="https://go.dev/blog/error-handling-and-go">에러 객체(error)</a>로 처리합니다. 다행히도 multi-return이 가능하기에 에러 반환을 더욱 수월하게 해줄 수 있습니다.</p> <pre>f, err := Sqrt(-1)<br />if err != nil {<br />    fmt.Println(err)<br />}</pre> <h4>try ~ catch가 없는 이유</h4> <p><a href="https://go.dev/doc/faq#exceptions">공식 문서</a>에 의하면 try ~ catch는 난해한 코드를 생성하며, 개발자에게 너무많은 일반적인 예외를 처리하도록 장려합니다.</p> <blockquote><em>We believe that coupling exceptions to a control structure, as in the </em><em>try-catch-finally idiom, results in convoluted code. It also tends to encourage programmers to label too many ordinary errors, such as failing to open a file, as exceptional.</em></blockquote> <p>다른 언어처럼 try ~ catch 를 어렴풋이 구현할 수 있습니다. 바로 중간에 실행의 흐름을 끊는panic함수를 사용하는 것입니다.<br/>반대로 생각하자면 <strong>모든 에러들을 panic으로 처리해야 할까?</strong> 라고 생각하면 좋은 선택지는 아니라고 생각합니다.<br/>이런 이유로 Go는 시의적절하기 예외처리할 수 있도록 error를 반환하는 방식으로 처리합니다.</p> <h4>Error Wrapping</h4> <p>Error Wrapping이란 쉽게 말하자면 error 객체를 감싸는 또다른 구조체를 만드는 것입니다.<br/>Go에서는 에러처리할 때 Error 객체를 넘겨줍니다. 물론 일반 에러 객체를 넘겨줄 수 있지만 개발자가 직접 만든 에러를 만들어서 넘겨줄 수 있습니다.</p> <p>gin에서의 Error를 확인하겠습니다. gin의 Error 내에 필드로 error가 존재합니다. 아래 코드와 같은 과정을 Error Wrapping이라고 보면 됩니다.</p> <pre>// Error represents a error&#39;s specification.<br />type Error struct {  <br />    Err  error  <br />    Type ErrorType  <br />    Meta any  <br />}</pre> <p>ctx.Error를 실행했는데 의도치 않게 errors.As가 적절하게 실행되지 않는다고 가정하겠습니다.error.As는 Error Type을 확인하는 함수인데, 만약에 타입이 적절하지 않는다면, 입력한 error을 감싼 Error를 반환하게 됩니다.</p> <pre>func (c *Context) Error(err error) *Error {  <br />    if err == nil {  <br />       panic(&quot;err is nil&quot;)  <br />    }  <br />  <br />    var parsedError *Error  <br />    ok := errors.As(err, &amp;parsedError)  <br />    if !ok {  <br />       parsedError = &amp;Error{  <br />          Err:  err,  <br />          Type: ErrorTypePrivate,  <br />       }  <br />    }  <br />  <br />    c.Errors = append(c.Errors, parsedError)  <br />    return parsedError  <br />}</pre> <p>원본 에러(error)는 Unwrap()함수를 통해 얻을 수 있습니다.</p> <pre>// Unwrap returns the wrapped error, to allow interoperability with errors.Is(), errors.As() and errors.Unwrap()  <br />func (msg *Error) Unwrap() error {  <br />    return msg.Err  <br />}</pre> <p>Error의 구조를 정리하면 아래와 같습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/551/1*1EEloxJEdNcRKxt5NiZW0A.png"/><figcaption>Error 구조</figcaption></figure> <h3>프로젝트 코드의 문제점</h3> <h4>Gin Context의 잘못된 활용</h4> <p><a href="https://pkg.go.dev/context#pkg-overview">공식 문서</a>에서 말하는 Context는 데드라인, 취소 시그널, API에 대한 경계값을 가지는 값으로 정의합니다. 초반에는 조건에 따라 실행이 중단될 수 있다는 것으로 이해했습니다.</p> <p>gin은 자체적인 Context를 가지고 있으며, context를 중단시킬 수 있는 여러 함수들이 존재합니다. gin에서 제공하는 context를 활용하여 Service Layer에서 커스텀 에러 타입으로 반환하도록 구현해보겠습니다.</p> <pre>func (controller *MemberController) RegisterMember(ctx *gin.Context, req request.RegisterReq) {<br /> req = request.RegisterReq{}<br /> err := ctx.ShouldBindJSON(req)<br /> // ... <br /> // Create member<br /> err2 := controller.service.CreateMember(ctx, req)<br /> if err2 != nil {<br />  errorutils.ErrorFunc(ctx, err2)<br />  return<br /> }<br /> webutils.Success(ctx)<br />}</pre> <p>커스텀 에러 타입을 자세히 보면, 자체적으로 제작한 에러 코드와 error을 담을 Err 필드가 존재합니다.</p> <pre>type Error struct {<br /> // Code is a custom error codes<br /> ErrorType ErrorType<br /> // Err is a error string<br /> Err error<br /> // Description is a human-friendly message.<br /> Description string<br />}</pre> <p>애플리케이션에 오류 발생시 현재 실행을 멈추고, 응답값을 보내는 ErrorFunc 함수도 만들었습니다.</p> <pre>func ErrorFunc(ctx *gin.Context, err *Error) {<br /> res := getCode(err.ErrorType)<br /><br /> ctx.AbortWithStatusJSON(res.Code, res)<br /> return<br />}</pre> <p><a href="https://pkg.go.dev/github.com/gin-gonic/gin#Context.Abort">공식 문서</a> 에 의하면AbortWithStatusJSON에는 내부적으로 Context를 중단시킬 수 있는Abort 함수를 사용합니다. 구체적으로 <a href="https://pkg.go.dev/github.com/gin-gonic/gin#Context.Abort">Abort 함수</a>는 현재의 handler는 그대로 남지만, 그 이후의 handler를 처리하지 않겠다는 것이다. 즉, <strong>Abort()를 실행한 이후에도 남은 코드가 실행</strong>되는 것입니다.</p> <h4>Gin Error 미사용</h4> <p><a href="https://pkg.go.dev/github.com/gin-gonic/gin#Context.Error">공식 문서</a>에 의하면 Gin은 자신들의 Error를 사용하는 것을 권장하며, middleware가 이를 처리하여 오류 response를 처리하라고 명시되어 있습니다.</p> <blockquote>Error attaches an error to the current context. The error is pushed to a list of errors. It’s a good idea to call Error for each error that occurred during the resolution of a request. A middleware can be used to collect all the errors and push them to a database together, print a log, or append it in the HTTP response. Error will panic if err is nil.</blockquote> <p>즉, 오류가 발생할때마다 gin의 Context에서 제공해주는 Error로 감싸며, Middleware에 있는 Handler가 이를 순차적으로 처리해야 한다는 것입니다.</p> <h3>2부에서는</h3> <p>지금까지는 내가 만들었던 예외처리에는 어떠한 문제점이 있는지 확인해봤습니다.</p> <p>2부에서는 위에서 설명한 잘못된 에러처리를 공식문서에서 제시한 올바른 에러처리를 구현해보겠습니다.</p> <ul><li>Middleware에 Handler 구현</li><li>gin.Error를 활용하여 Error를 wrapping하고, Middleware에서 처리하기</li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c2afdf7e7be2" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/s0okju-tech/gin-%EC%98%88%EC%99%B8%EC%B2%98%EB%A6%AC-part-1-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EC%BD%94%EB%93%9C%EB%A1%9C-%EC%82%B4%ED%8E%B4%EB%B3%B4%EB%8A%94-%EC%98%88%EC%99%B8%EC%B2%98%EB%A6%AC-%EB%AC%B8%EC%A0%9C%EC%A0%90-c2afdf7e7be2">Gin 예외처리 — Part 1. 프로젝트 코드로 살펴보는 예외처리 문제점</a> was originally published in <a href="https://medium.com/s0okju-tech">S0okJu Technology Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Testing: Pytest를 활용한 단위 테스트 구현</title><link href="https://s0okju.github.io/blog/2025/testing-pytest/" rel="alternate" type="text/html" title="Testing: Pytest를 활용한 단위 테스트 구현"/><published>2025-05-12T13:08:53+00:00</published><updated>2025-05-12T13:08:53+00:00</updated><id>https://s0okju.github.io/blog/2025/testing-pytest----</id><content type="html" xml:base="https://s0okju.github.io/blog/2025/testing-pytest/"><![CDATA[<h3>Test Code 란?</h3> <p>테스트 코드는 소프트웨어의 기능과 동작을 테스트하는데 사용되는 코드입니다. 테스트 코드는 개발자가 작성한 코드를 실행하여 예상되는 결과가 나오는지 확인하며 소프트웨어 결함을 찾습니다.</p> <h3>종류</h3> <p>테스트 코드는 V 모델에 따라 크게 4가지로 나뉩니다. 각 테스트의 범위에 따라 예상 기댓값이 적절하게 맞는지 확인하여 테스트를 수행하게 됩니다.</p> <blockquote><strong>V-모델</strong><br/>소프트웨어 개발의 주요 단계들(비즈니스 요구사항 명세화에서 배포까지)과 상응하는 테스트 레벨(인수 테스팅에서 유닛 테스팅까지) 사이의 일대일 대응 관계를 설명하는 순차적 개발 수명주기 모델입니다.</blockquote> <figure><img alt="" src="https://cdn-images-1.medium.com/max/459/0*KAT3cjxOUcKMBLDG.png"/><figcaption>출처 — <a href="https://www.testbytes.net/wp-content/uploads/2019/05/v-model-in-software-testing.png">https://www.testbytes.net/wp-content/uploads/2019/05/v-model-in-software-testing.png</a></figcaption></figure> <p>이 중 개발자는 단위 테스트와 통합 테스트를 주로 다루게 됩니다.</p> <ul><li><strong>단위 테스트</strong></li><li><strong>통합 테스트</strong></li><li>시스템 테스트</li><li>사용자 인수 테스트</li></ul> <h3>Test Double</h3> <p>테스트를 목적으로 실제 객체, 연관된 객체를 직접 사용하기 어려울때 대신 사용하는 가짜 객체를 의미합니다.</p> <h4>사용 이유</h4> <ol><li>실제 객체가 아직 개발되지 않았거나 접근이 불가능한 경우에 대한 테스트가 가능하다.</li><li>복잡한 환경이나 시나리오를 간단하게 모의할 수 있다.</li><li>테스트 실행 속도를 크게 향상시킬 수 있다.</li><li>테스트의 독립성을 보장한다.</li></ol> <p>테스트 더블을 활용하여 객체의 결과값을 예측할 수 있으며, 이를 활용하여 높은 품질의 소프트웨어를 더 빠르고 효과적으로 개발할 수 있게 됩니다.</p> <h4>종류</h4> <p>Test Double은 여러 종류가 있지만 그 중 단위 테스트에서 많이 활용되는 Fake, Stub, Mock에 대해서만 설명하겠습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*hKtNkKUIOVGIjRKc.jpg"/><figcaption>출처 — <a href="https://dev-ellachoi.tistory.com/124">https://dev-ellachoi.tistory.com/124</a></figcaption></figure> <p><strong>Fake</strong><br/>객체의 행동을 모방하여 제작합니다.</p> <p><strong>Stub</strong><br/>특정 메서드가 정해진 응답값을 반환하는 객체입니다. Fake 객체와의 차이점은 <strong>응답값이 고정</strong>되어 있으며 <strong>상태</strong>를 검증하는데 사용됩니다.</p> <p><strong>Mock </strong><br/>Mock는 <strong>반환값이 없는 함수나 특정 객체에서 특정 함수가 호출되었는지 테스트할때 사용</strong>됩니다. 즉, 내가 바라는대로 호출에 대한 기댓값을 명시하고, 명시한 내용에 따라서 잘 작동되는지 확인합니다. <br/>Stub과 유사하지만 호출된 메서드에 대해 <strong>행위</strong>를 검증할 때 사용됩니다.</p> <h3>단위 테스트</h3> <h4>개념</h4> <p>단위 테스트는 소프트웨어 개발에서 일반적으로 사용되는 테스트 중 하나로, 개별적인 코드 단위가 의도한대로 작동하는지 확인합니다.</p> <h4>적용하기</h4> <p>실제 프로젝트에 적용해보겠습니다. <br/>레포지토리에는 Challenge id로 조회하여 Challenge 객체를 반환하는 프라이빗 함수가 있습니다.</p> <pre>class ChallengeRepository:<br />    def __init__(self, session):<br />        self.session = session<br />    <br /> # Challenge 객체를 가져오는 함수 <br />    def _get_challenge(self, challenge_id: int) -&gt; Challenges:<br />        &quot;&quot;&quot;<br />        챌린지 아이디로 챌린지 조회<br />        <br />        Args:<br />            challenge_id (int): 챌린지 아이디<br />            <br />        Returns:<br />            Challenges: 챌린지 객체<br />            <br />        Raises:<br />            ChallengeNotFound: 챌린지가 존재하지 않을 때<br />            InternalServerError: DB 에러 발생 시<br />        &quot;&quot;&quot;<br />        try:<br />            challenge = self.session.query(Challenges).get(challenge_id)<br />            if not challenge:<br />                raise ChallengeNotFound(error_msg=f&quot;Challenge not found: {challenge_id}&quot;)<br />            <br />            return challenge<br />        except SQLAlchemyError as e:<br />            raise InternalServerError(error_msg=f&quot;Error getting challenge by id {challenge_id}: {str(e)}&quot;) from e</pre> <p>테스트 코드는 Pytest를 중점적으로 사용하돼 unittest 모듈도 함께 사용하겠습니다.<br/>pytest에서 지정한 <a href="https://docs.pytest.org/en/6.2.x/xunit_setup.html#method-and-function-level-setup-teardown">setup_method</a> 를 활용하여 클래스 내부에 있는 테스트코드를 실행하기 전에 필요한 변수를 지정합니다.</p> <p>여기서 MagicMock()를 활용하여 mock_session을 선언했습니다. session은 Flask 앱 초기화 시 데이터베이스에서 생성하는 객체입니다. 단위 테스트는 _get_challenge()에 대한 테스트만 수행하는 것이므로 session과 같은 외부 의존성을 임시 객체로 선언하여 사용합니다.</p> <pre>class TestChallengeRepositoryGetChallenge:<br />    def setup_method(self):<br />  # 임시 session을 만듭니다. <br />        self.mock_session = MagicMock()<br /><br />  # mocking된 session을 활용하여 테스트할 레포지토리를 정의합니다. <br />        self.repository = ChallengeRepository(self.mock_session)</pre> <p>mocking된 Challenge 객체를 활용하여 레포지토리가 성공인지 테스트를 수행합니다.<br/>여기서 활용된 test fixture란 중복 발생되는 행위를 고정시켜 한곳에 관리하는 개념이라고 보시면 됩니다. 추후 다른 함수에서도 활용될 예정이므로 fixture를 수행했습니다. <br/>여기서 왜 Fake 객체가 아니라 Mock 객체로 선언했는지 궁금할 것입니다. _get_challenge()는 Challenge 객체를 최종적으로 반환합니다. 이 과정에서 <strong>발생하는 리턴 값, 예외값을 모두 테스트</strong>해야 합니다. 이처럼 다양한 행위를 테스트하기 위해서 Mock를 사용했습니다.</p> <pre>@pytest.fixture<br />def mock_challenge():<br />    &quot;&quot;&quot;Mock challenge object&quot;&quot;&quot;<br />    challenge = MagicMock()<br />    challenge.id = 1<br />    challenge.title = &quot;Test Challenge&quot;<br />    return challenge<br /><br />class TestChallengeRepositoryGetChallenge:<br /> # ... <br />    def test_success(self, mock_challenge):<br />  #  self.repository._get_challenge(1) 반환 값이 mock_challenge<br />        self.mock_session.query().get.return_value = mock_challenge<br />        result = self.repository._get_challenge(1)<br />  <br />  # 결과 값 비교 <br />        assert result == mock_challenge<br /><br /> def test_challenge_does_not_exist(self):<br />  <br />  # 발생할 예외 : ChallengeNotFound<br />        self.mock_session.query().get.side_effect = ChallengeNotFound(&quot;Challenge not found&quot;)<br />  <br />  # 임시로 ChallengeNotFound 발생시키기<br />        with pytest.raises(ChallengeNotFound) as exc_info:<br />            self.repository._get_challenge(1)<br />  <br />  # 결과 확인하기 <br />        assert &quot;Challenge not found&quot; in str(exc_info.value)</pre> <p>레포지토리와 다르게 단순히 값만 반환하는 NameBuilder 함수를 확인해봅시다. <br/>Namebuilder는 challenge_id, user_id를 입력 받아서 build() 함수를 통해 ChallengeInfo라는 객체를 반환합니다.</p> <pre>class NameBuilder:<br />    def __init__(self, challenge_id: int, user_id: int):<br />        self._challenge_id = challenge_id<br />        self._user_id = user_id<br />        <br />    def _is_valid_name(self, name:str) -&gt; bool:<br />        &quot;&quot;&quot;Kubernetes 리소스 이름 유효성 검사&quot;&quot;&quot;<br />        name = name.lower()<br />        if not name or len(name) &gt; 253:<br />            return False<br />        pattern = r&#39;^[a-z0-9]([-a-z0-9]*[a-z0-9])?$&#39;<br />        return bool(re.match(pattern, name))<br />    <br />    def build(self) -&gt; Optional[ChallengeInfo]:<br />        &quot;&quot;&quot;<br />        챌린지 이름 빌더 <br />        <br />        &quot;&quot;&quot;<br />        challenge_info = ChallengeInfo(challenge_id=self._challenge_id, user_id=self._user_id)<br />        if not self._is_valid_name(challenge_info.name):<br />            raise InvalidName(error_msg = f&quot;Invalid challenge name {challenge_info.name}&quot;)<br />        return challenge_info</pre> <p>이럴 경우 ChallengeInfo는 외부 의존성이 없는 단순한 클래스입니다. 그러므로 challenge_stub을 생성하여 값을 고정한 후 build 함수가 적절하게 응답했는지 확인하면 됩니다.</p> <pre>@pytest.fixture<br />def challengeinfo_stub():<br />    return ChallengeInfo(challenge_id=1, user_id=1)<br /><br />class TestNameBuilder:<br />    def setup_method(self):<br />        self.namebuilder = NameBuilder(1, 1)<br />        <br />    def test_success(self, challengeinfo_stub):<br />        info = self.namebuilder.build()<br />        <br />        assert info.challenge_id == 1<br />        assert info.user_id == 1<br />        assert info.name == &quot;challenge-1-1&quot;</pre> <h3>테스트 코드를 사용하는 이유</h3> <ul><li>내가 무엇을 만들고 있는지 정확히 인지</li><li>리팩토링을 진행할 대 부담 덜어주기</li><li>결합도와 의존성이 낮은 코드를 지향</li></ul> <p>결국에는 <strong>요구사항에 적합하고 유지보수성이 높은 소프트웨어를 개발하기 위해서는 테스트 코드 작성이 필수</strong>입니다.</p> <h4>경험담</h4> <p>단순한 웹 애플리케이션을 개발할때는 테스트 코드의 필요성을 느끼지 못했습니다. 코드 전반이 단순하여 오류 발생 시 금방 발견하게 수정할 수 있었습니다. 그러나 2개 이상의 레포지토리, 2개 이상의 플랫폼을 활용하여 개발하면서 오류의 문제점을 재빠르게 확인할 수 없었습니다. <br/>막상 테스트 코드를 짜려고 하니 결합도와 의존성이 높아서 개인적으로 힘들었던 기억이 있습니다. 리펙토링이 아니라 처음부터 다시 개발하는 수준으로 코드를 다시 작성해야 했습니다. <br/>이론적으로는 필요성을 알고 있었습니다. 그러나 실제로 애플리케이션을 클라이언트에게 배포하다보니 테스트 코드의 중요성을 몸소 느끼고 있습니다. 현재 테스트 코드를 조금씩 작성하고 있습니다. 그때마다 고려하지 못한 오류 사항을 마주했고 이에 맞게 코드를 수정하고 있습니다.</p> <h3>References</h3> <ul><li><a href="https://yozm.wishket.com/magazine/detail/1964/">테스트 코드는 왜 만들까? | 요즘IT</a></li><li><a href="https://azderica.github.io/00-test-mock-and-stub/">[Test] Mock 테스트와 Stub 테스트 차이</a></li><li>https://dev-test-hqsw.tistory.com/23</li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0a5f6c5f779c" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/s0okju-tech/testing-pytest%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%98%EC%97%AC-%EB%8B%A8%EC%9C%84-%ED%85%8C%EC%8A%A4%ED%8A%B8-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0-0a5f6c5f779c">Testing: Pytest를 활용한 단위 테스트 구현</a> was originally published in <a href="https://medium.com/s0okju-tech">S0okJu Technology Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">GitOps 전환기 — Part 4. Tekton을 활용한 CI 구성하기</title><link href="https://s0okju.github.io/blog/2025/gitops-part-4-tekton-ci/" rel="alternate" type="text/html" title="GitOps 전환기 — Part 4. Tekton을 활용한 CI 구성하기"/><published>2025-05-05T05:56:04+00:00</published><updated>2025-05-05T05:56:04+00:00</updated><id>https://s0okju.github.io/blog/2025/gitops-part-4-tekton--ci-</id><content type="html" xml:base="https://s0okju.github.io/blog/2025/gitops-part-4-tekton-ci/"><![CDATA[<h3>GitOps 전환기 — Part 4. Tekton을 활용한 CI 구성하기</h3> <h3>Tekton 이란</h3> <p>Tekton이란 클라우드 네이티브 CI/CD 도구입니다. Tekton의 주요 컴포넌트인 Tekton Pipelines, Trigger로 이뤄져 있으며 쿠버네티스 CR(Custom Resource)를 활용해 선언적으로 CI/CD를 구성할 수 있습니다.</p> <h4>Jenkins vs Tekton</h4> <p>Jenkins는 가장 많이 사용되고 있는 CI/CD 도구이며, 다양한 플러그인을 통해 기능을 확장할 수 있습니다. 반면 JVM 기반으로 실행되기 대문에 메모리를 크게 사용하며, 리소스 초과 사용 시 서버 전체가 다운될 수 있습니다.</p> <p>Tekton은 클라우드 네이티브 CI/CD 도구이며, 쿠버네티스 클러스터 내부에서 독립적인 모듈 단위로 작업을 수행할 수 있습니다. 또한 Auto Scaling, Self-Healing, Serverless 기능을 가지고 있어 리소스 사용 효율성과 관리 측면에서 이점을 가지고 있습니다. 그러나 러닝 커브가 높은 편이고 자료가 많이 없다는 단점을 가지고 있습니다.</p> <blockquote>자세한 사항은 <a href="https://velog.io/@sgwon1996/Jenkins-vs-GitHub-Action-vs-Tekton">Jenkins vs GitHub Action vs Tekton</a> 글을 참고하시길 바랍니다.</blockquote> <p>물론 GitHub Actions라는 선택지도 있었습니다. 그러나 프로젝트에서는 3개의 레포지토리를 활용하므로 하므로 GitHub Actions는 선택지에 제외되었습니다.</p> <ol><li>각 레포지토리마다 Git Actions를 설정해야 함 -&gt; <strong>중앙집중화된 관리 체계 필요</strong></li><li>Public만 무료로 제공 -&gt; <strong>Public, Private 레포지토리에 동시에 접근 가능해야 함.</strong></li></ol> <h4>Tekton 선택 이유</h4> <p>이전에는 별도의 인스턴스에 Jenkins를 설치해서 운영하고 있었습니다. 인프라 운영 과정에서 메모리 공간을 확보하기 위해 인스턴스 정리가 필요했고, 젠킨스가 설치된 인스턴스를 삭제하기로 결정했습니다. 그 대신 <strong>쿠버네티스 내에 CI 도구를 활용하는 방향으로 결정</strong>했습니다.</p> <p>Jenkins 또한 Kubernetes 내에서 배포할 수 있습니다. 그러나 <strong>파이프라인 구동 시에 리소스를 사용한다는 Serverless 특정 때문에 Tekton를 최종적으로 선택</strong>하게 되었습니다.</p> <h3>프로젝트에 적용하기</h3> <blockquote>해당 게시글은 Tekton의 사용법이 아닌 <strong>프로젝트에 어떻게 활용했는지에 집중합니다.</strong> 자세한 사항은 <a href="https://tekton.dev/docs/">공식 문서</a>를 참고하시길 바랍니다.</blockquote> <p>Tekton은 크게 <a href="https://tekton.dev/docs/triggers/">Triggers</a>, <a href="https://tekton.dev/docs/pipelines/">Pipelines</a> 컴포넌트로 이뤄져 있습니다. Triggers 컴포넌트는 EventListener 내부에 있는 모든 컴포넌트로 보시면 되고, Pipelines는 PipelineRun 내부 컴포넌트라고 보시면 됩니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/811/1*85HCS-zARzIu8TrCt72wrw.png"/><figcaption>Tekton Resource type 종류</figcaption></figure> <h4>Task 구성하기</h4> <p><strong>개념</strong></p> <ul><li>Step: 워크플로를 구성하는 기본적인 실행 단위로 쿠버네티스 파드 안의 컨테이너로 실행됩니다.</li><li><a href="https://tekton.dev/docs/pipelines/tasks/">Task</a>: 순서가 있는 Step의 모음으로, 각 Task가 실행될때마다 쿠버네티스 파드가 생성됩니다. 파드 내에서는 각 Step이 독립적인 컨테이너로 실행돼 서로 연관된 Step들끼리 환경 설정이나 볼륨을 공유할 수 있습니다.</li><li><a href="https://tekton.dev/docs/pipelines/taskruns/#overview">TaskRun</a>: Task를 인스턴스화하여 실행시킵니다. TaskRuns은 성공 혹은 실패할때까지 Task내의 Step들을 실행시킵니다.</li></ul> <p>Task는 TaskRun이라는 CR를 통해 실행됩니다. 즉, <strong>Task는 작업할 흐름의 정의서로 보고 TaskRun은 Task의 실행 단위라고 보시면 됩니다.</strong></p> <p>단일 Task를 실행하기 위해서 TaskRun을 정의하여 사용할 수 있습니다. 그러나 프로젝트에서 CI를 구성할때는 Task의 상위 흐름 개념(Pipeline)을 활용하여 자동으로 TaskRun이 수행됩니다.</p> <p><strong>구성하기</strong></p> <p>GitOps 과정을 수행하기 위해서는 빌드를 마친 후에 Manifest를 변경시키는 작업이 필요합니다. Manifest 변경 작업은 우리는 이미지의 해시값을 kustomization.yaml에 업데이트하는 걸로 보시면 됩니다.</p> <p>그러기 위해서는 3가지 작업을 수행해야 합니다.</p> <ol><li>특정 브랜치에 해당되는 레포지토리를 clone하여 가져오기</li><li>이미지 빌드하기</li><li>빌드한 이미지의 해시값을 config 레포지토리에 있는 kustomization.yaml에 업데이트하기</li></ol> <p>Task를 직접 만들 수 있지만 <a href="https://hub.tekton.dev/">Tekton Hub</a>를 통해 간단한 파라미터 설정만으로도 Task를 수행할 수 있게 됩니다.</p> <p>그러므로 1, 2번 절차인 git-clone, 이미지 빌드 부분은 Tekton Hub에 있는 것들을 재사용할 예정입니다.</p> <ol><li>git clone -&gt; <a href="https://hub.tekton.dev/tekton/task/git-clone">https://hub.tekton.dev/tekton/task/git-clone</a></li><li>이미지 빌드 -&gt; <a href="https://hub.tekton.dev/tekton/task/buildah">https://hub.tekton.dev/tekton/task/buildah</a></li></ol> <p>다만 3번 절차인 Manifest 변경은 직접 코드를 작성해줍니다.</p> <p>Task는 Step이 순차적으로 이뤄져 있다고 말씀 드렸습니다. Manifest 변경 작업을 더 세분화해서 Step으로 생성하면 됩니다.</p> <ol><li>Config Repository를 clone한다.</li><li>Kustomization.yaml를 찾아서 images 값을 업데이트한다.</li><li>Config Repository의 변경 사항을 반영한다.</li></ol> <p>params 값은 추후 Pipeline에서 값을 지정하게 됩니다. 임시로 값이 들어왔다는 전제로 파라미터를 활용한 코드를 작성해줍니다.</p> <pre>apiVersion: tekton.dev/v1beta1<br />kind: Task<br />metadata:<br />  annotations:<br />    tekton.dev/pipelilnes.minVersion: &quot;0.19.0&quot;<br />    tekton.dev/tags: git <br />  name: git-update-deployment<br />  labels:<br />    operator.tekton.dev/provider_type: community<br />spec:<br />  <br />  params:<br />    - name: GIT_REPOSITORY<br />      description: The URL of the Git repository to clone<br />      type: string<br />      default: &quot;&quot;<br />    - name: GIT_REF<br />      description: The Git revision to check out<br />      type: string<br />      default: &quot;main&quot;<br />    - name: NEW_IMAGE<br />      description: The name of the image to build and push<br />      type: string<br />    - name: NEW_DIGEST<br />      type: string <br />    - name: KUSTOMIZATION_PATH<br />      description: The name of the kustomization path to update <br />      type: string<br />  results:<br />    - name: GIT_COMMIT<br />      description: The commit hash of the updated repository<br />  workspaces:<br />    - name: shared<br />      description: The workspace to share between tasks<br />  steps:<br /> # 1. Config Repository를 clone한다. <br />    - name: git-clone<br />      image: alpine/git:latest<br />      script: |<br />        #!/bin/sh<br />        set -ex<br />  # 기존에 있는 레포지토리 파일 삭제 <br />        if [ -d &quot;$(workspaces.shared.path)/repo&quot; ] &amp;&amp; [ &quot;$(ls -A $(workspaces.shared.path)/repo)&quot; ]; then<br />          rm -rf &quot;/workspace/shared/repo&quot;<br />        fi<br /><br />  # git clone을 수행하여 원하는 브랜치로 Checkout 하기 <br />        git clone $(params.GIT_REPOSITORY) -b $(params.GIT_REF) $(workspaces.shared.path)/repo<br />        cd $(workspaces.shared.path)/repo <br />        git checkout $(params.GIT_REF)<br />        git rev-parse HEAD &gt; /workspace/shared/GIT_COMMIT.txt<br /> <br /> # 2. Kustomization.yaml를 찾아서 images 값을 업데이트한다. <br />    - name: update-kustomization<br />      image: smartive/kustomize:latest<br />      script: |<br />        #!/bin/sh<br />  # Config Repostiory 내 kustomization.yaml를 찾는다. <br />        cd $(workspaces.shared.path)/repo/$(params.KUSTOMIZATION_PATH)<br />        echo &quot;Current directory: $(pwd)&quot;<br /><br />  # Image Hash를 업데이트한다. <br />        kustomize edit set image $(params.NEW_IMAGE)@$(params.NEW_DIGEST)<br />        cat kustomization.yaml<br /><br /> # 3. Config Repository의 변경 사항을 반영한다.<br />    - name: commit-changes<br />      image: alpine/git:latest<br />      script: |<br />        #!/bin/sh<br />        set -e<br />        cd $(workspaces.shared.path)/repo/$(params.KUSTOMIZATION_PATH)<br />        git config user.name &lt;github.username&gt;<br />        git config user.email &lt;github.email&gt;<br />        git status <br />        git add ./kustomization.yaml<br />        git commit -m &quot;Update image to $(params.NEW_IMAGE)&quot;<br />        git push<br />        EXIT_CODE=&quot;$?&quot;<br />        if [ &quot;$EXIT_CODE&quot; -ne 0 ]; then<br />          echo &quot;Error: Failed to push changes to the repository.&quot;<br />          exit 1<br />        fi<br />        echo &quot;Changes pushed successfully.</pre> <h4>Pipeline 구성하기</h4> <p><strong>개념</strong></p> <ul><li><a href="https://tekton.dev/docs/pipelines/pipelines/#overview">Pipeline</a>: CI에 수행할 실행 흐름을 정의한 Task의 모음입니다. Pipeline에 있는 각각의 Task들은 Pod 형태로 실행됩니다.</li><li><a href="https://tekton.dev/docs/pipelines/pipelineruns/">PipelineRun</a>: Pipeline을 인스턴스화하여 실행시킵니다.</li></ul> <p>Pipeline — PipelineRun과 Task — TaskRun은 거의 동일한 관계를 가진다고 보면 됩니다.</p> <p>Pipeline만 실행시키고 싶다면 PipelineRun을 정의하여 실행시킬 수 있습니다. 저희 프로젝트에서는 GitHub의 event에 맞춰 발생한 Trigger를 통해 자동으로 PipelineRun을 생성하도록 구성할 것입니다.</p> <p><strong>구성하기 </strong><br/>지금까지 아래와 같은 Task를 생성했습니다.</p> <ol><li>특정 브랜치에 해당되는 레포지토리를 clone하여 가져오기</li><li>이미지 빌드하기</li><li>빌드한 이미지의 해시값을 config 레포지토리에 있는 kustomization.yaml에 업데이트하기</li></ol> <p>Task에서는 파라미터가 선언되어 있는데요. 해당 파라미터를 지정하여 모든 Task들을 하나의 Pipeline으로 생성합니다.</p> <pre>apiVersion: tekton.dev/v1beta1<br />kind: Pipeline<br />metadata:<br />  name: challenge-api-pipeline<br />  namespace: ci<br />spec:<br />  params:<br />    - name: repo-url<br />      type: string<br />    - name: revision<br />      type: string<br />      default: &quot;main&quot;<br />    - name: image-name<br />      type: string<br />    - name: context-dir<br />      type: string<br />      default: &quot;.&quot; <br />    - name: config_git_repo<br />      type: string<br />    - name: config_git_ref<br />      type: string<br />      default: &quot;main&quot;<br /><br />  workspaces:<br />    - name: shared  # 모든 태스크가 공유하는 워크스페이스<br /><br />  tasks:<br /># 1. 특정 브랜치에 해당되는 레포지토리를 clone하여 가져오기<br />    - name: git-clone<br />      taskRef:<br />        name: git-clone<br />      params:<br />        - name: url<br />          value: $(params.repo-url)<br />        - name: revision<br />          value: $(params.revision)<br />        - name: deleteExisting<br />          value: &quot;true&quot;<br />      workspaces:<br />        - name: output<br />          workspace: shared<br /><br /># 2. 이미지 빌드하기<br />    - name: buildah-build<br />      taskRef:<br />        name: buildah<br />      runAfter: [&quot;git-clone&quot;]<br />      params:<br />        - name: IMAGE<br />          value: $(params.image-name)<br />        - name: CONTEXT<br />          value: &quot;$(workspaces.source.path)/$(params.context-dir)&quot;<br />        - name: TLSVERIFY<br />          value: &quot;false&quot;<br />      workspaces:<br />        - name: source<br />          workspace: shared<br /># 3. 빌드한 이미지의 해시값을 config 레포지토리에 있는 kustomization.yaml에 업데이트하기  <br />    - name: git-update-deployment<br />      taskRef:<br />        name: git-update-deployment<br />      runAfter: [&quot;buildah-build&quot;]<br />      params:<br />        - name: GIT_REPOSITORY<br />          value: $(params.config_git_repo)<br />        - name: GIT_REF<br />          value: $(params.config_git_ref)<br />        - name: NEW_IMAGE<br />          value: $(params.image-name)<br />        - name: NEW_DIGEST<br />          value: $(tasks.buildah-build.results.IMAGE_DIGEST)<br />        - name: KUSTOMIZATION_PATH<br />          value: &quot;challenge-api/overlays/dev&quot;<br />      workspaces:<br />        - name: shared<br />          workspace: shared</pre> <p>실행하면 아래와 같이 pod를 확인할 수 있습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sSdI_oykI2M_TFVhactuxw.png"/><figcaption>kubectl get pod 결과</figcaption></figure> <h4>Trigger 구성하기</h4> <p><strong>개념</strong></p> <ul><li><a href="https://tekton.dev/docs/triggers/triggertemplates/">TriggerTemplate</a>: TaskRun 혹은 pipeline과 같은 리소스를 위한 청사진을 구체화하는 리소스입니다. EventListener에서 이벤트가 감지되면 TaskRun, PipelineRun이 인스턴스화되어 실행됩니다.</li><li><a href="https://tekton.dev/docs/triggers/triggerbindings/">TriggerBinding</a>: 이벤트 값으로부터 값을 추출하여 TriggerTemplate에 정의된 값을 바인딩합니다.</li><li><a href="https://tekton.dev/docs/triggers/eventlisteners/">Trigger</a>: TriggerTemplate과 TriggerBinding를 대응시키면 하나의 Trigger라고 불리게 됩니다.</li></ul> <figure><img alt="" src="https://cdn-images-1.medium.com/max/820/1*OKxMKRoV133pceSpQ4h18A.png"/><figcaption>출처 — <a href="https://tekton.dev/vault/triggers-main/">https://tekton.dev/vault/triggers-main/</a></figcaption></figure> <p><strong>구성하기</strong></p> <p>지금가지 새로 생성한 이미지의 정보(Manifest)를 kustomization.yaml 에 업데이트하는 Pipeline를 정의했습니다.</p> <p>이제부터는 GitHub의 변경 이벤트에 따라 CI가 진행될 수 있도록 구성해야 합니다. 그러기 위해서는 들어오는 GitHub 이벤트에 맞춰서 Trigger가 실행될 수 있도록 TriggerTemplate, TriggerBinding을 정의해야 합니다.</p> <p>이벤트로부터 레포지토리 url, revision 정보를 받아왔습니다. 해당 정보를 TriggerBinding을 통해 받고 TriggerTemplate에 적용할 수 있도록 구성했습니다.</p> <p>프로젝트에서는 main으로 merge하는 이벤트로만 실행할 수 있도록 설계했습니다. 그렇기 때문에 TriggerBinding을 통해 revision을 가져오는 것이 무의하다고 생각할 수 있을 것 같습니다. 추후 브랜치 전략을 체계적으로 짜게 된다면 이벤트로부터 받아들이는 값들이 유의미하게 적용될 수 있을 것 같습니다.</p> <pre>apiVersion: triggers.tekton.dev/v1beta1<br />kind: TriggerBinding<br />metadata:<br />  name: challenge-api-triggerbinding<br />  namespace: ci<br />spec: <br /><br /># GitHub Event json 구조를 참고하셔서 body 파라미터를 정의하셔야 합니다. <br />  params:<br />    - name: repo-url<br />      value: $(body.repository.clone_url)<br />    - name: revision<br />      value: $(body.after)<br />    - name: image-name<br />      value: &lt;image name&gt;</pre> <pre>apiVersion: triggers.tekton.dev/v1beta1<br />kind: TriggerTemplate<br />metadata:<br />  name: challenge-api-triggertemplate<br />spec:<br />  params:<br />    - name: repo-url<br />      description: The URL of the Git repository to clone<br />      default: &lt;default git url&gt;<br />    - name: revision<br />      description: The Git revision to check out<br />      default: &quot;main&quot;<br />    - name: image-name<br />      description: The name of the image to build and push<br />      default: &lt;default image name&gt;<br /><br /># PipelineRun 정의 <br />  resourcetemplates:<br />  - apiVersion: tekton.dev/v1beta1<br />    kind: PipelineRun<br />    metadata:<br />      labels:<br />        tekton.dev/pipeline: challenge-api-pipeline<br />      name:  challenge-api-$(uid)<br />    spec:<br />      pipelineRef:<br />        name: challenge-api-pipeline<br />      serviceAccountName: tekton-trigger-sa<br /><br /># TriggerTemplate은 tt.params를 참조합니다. <br />      params:<br />        - name: repo-url<br />          value: $(tt.params.repo-url)<br />        - name: revision<br />          value: $(tt.params.revision)<br />        - name: image-name<br />          value: $(tt.params.image-name)<br />      workspaces:<br />        - name: shared<br />          persistentVolumeClaim:<br />            claimName: challenge-api-pvc  # PVC 이름</pre> <h4>EventListener 구성하기</h4> <p><strong>개념</strong></p> <ul><li><a href="https://tekton.dev/docs/triggers/eventlisteners/">EventListener</a>: 쿠버네티스 내에서 특별한 포트를 통해 이벤트를 감지하는 쿠번에티스 오브젝트입니다. 들어오는 이벤트를 받아들인 후 하나 이상의 Trigger를 정의합니다.</li><li><a href="https://tekton.dev/vault/triggers-main/eventlisteners/#specifying-interceptors">Interceptor</a>: TriggerBinding 이전에 수행되는 특정 플랫폼에 대한 “catch-call” 이벤트 프로세서입니다. 주로 페이로드를 필터링, 검증 ,변형 등 다양한 작업을 수행할 수 있습니다. 만약에 특정 조건에 충족하게 되면 조건에 맞는 Trigger를 사용하게 됩니다.</li></ul> <blockquote>자세히 알고 싶다면 <a href="https://www.redhat.com/en/blog/filtering-tekton-trigger-operations">Red Hat Blog — Filtering Tekton trigger operations</a>를 확인하시길 바랍니다.</blockquote> <p><strong>구성하기</strong></p> <p>저는 단일 EvenetListener를 정의하여 들어오는 레포지토리에 따라 서로 다른 Trigger를 수행하도록 정의할 것입니다. 왜냐하면 단일 EvenetListener를 정의함으로 관리하기 편하기 때문입니다.</p> <p>현재 온프로미스 환경에서 쿠버네티스 서버를 구동 중이기 때문에 NodePort를 통해 포트를 노출했습니다. GitHub에서 Webhook을 설정할때는 주소와 노출한 포트로 URL을 설정하면 됩니다.</p> <p>프로젝트에서는 크게 3개의 레포지토리를 빌드해야 합니다. 그렇게 때문에 각 레포지토리마다 main으로 merged된 이벤트 발생 시 Trigger가 발생할 수 있도록 설정했습니다.</p> <p>기본적으로 제공되는 GitHub <a href="https://tekton.dev/vault/triggers-main/clusterinterceptors/">ClusterInterceptor</a>를 통해 Interceptor를 구현할 수 있습니다. 그러나 저는 기본적인 <a href="https://github.com/google/cel-spec/blob/master/doc/langdef.md">CEL</a> Interceptor를 활용하여 필터링을 수행해보겠습니다.</p> <blockquote>CEL은 구글에서 제작한 빠르고 이동 가능하며 안전하게 실행하도록 설계된 범용 표현식 언어입니다. <br/>출처 — <a href="https://cel.dev/overview/cel-overview?hl=ko">https://cel.dev/overview/cel-overview?hl=ko</a></blockquote> <p>Trigger가 발생하는 조건은 아래와 같이 정했습니다.</p> <ol><li>레포지토리가 helloworld/challenge-api인 경우</li><li>main에 PR(Pull Request)를 수행한 경우</li><li>Merge가 된 경우 -&gt; PR이 자동으로 closed 상태로 변함</li></ol> <p>실제로 GitHub에 접속하셔서 Webhook&gt;Recent Deliveries에 접속하셔서 필터링하고자 하는 필드의 위치를 찾아 입력합니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/504/1*jImQ8nxxRN1EcRMdhz43PQ.png"/><figcaption>Recent Delivieries 확인</figcaption></figure> <pre>apiVersion: triggers.tekton.dev/v1beta1<br />kind: EventListener<br />metadata:<br />  name: hexactf-listener<br />spec:<br />  resources:<br />    kubernetesResource:<br />      serviceType: NodePort<br />  serviceAccountName: tekton-trigger-sa<br />  triggers:<br />    - name: challenge-api-trigger<br />      interceptors:<br />      - ref:<br />          name: cel<br />        params:<br />        - name: filter<br />          value: &quot;body.repository.full_name == &#39;helloworld/challenge-api&#39; &amp;&amp; body.pull_request.base.ref ==&#39;main&#39; &amp;&amp; body.action == &#39;closed&#39; &amp;&amp; body.pull_request.merged==true&quot;<br />      bindings:<br />      - ref: challenge-api-triggerbinding<br />      template:<br />        ref: challenge-api-triggertemplate</pre> <p>위와 동일하게 프로젝트 이름(body.repository.full_name)에 따라 서로 다른 Trigger를 실행할 수 있도록 설정했습니다. 이를 통해 하나의 Listener만을 가지고 다양한 Trigger를 실행할 수 있도록 구현했습니다.</p> <h3>Tekton 실행하기</h3> <p>GitHub 에서 PR을 수행하면 아래와 같은 변경사항을 확인할 수 있습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/945/1*r6E-AEATchM611Pa6qIOpQ.png"/><figcaption>tekton dashboard</figcaption></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/773/1*r4JbnPlUKkMQf4iUfPBDog.png"/><figcaption>config repository 변경사항</figcaption></figure> <h3>Tekton에 Slack 메세지 보내기</h3> <p><a href="https://hub.tekton.dev/tekton/task/send-to-webhook-slack">Send message to Slack Channel</a>, <a href="https://hub.tekton.dev/tekton/task/send-to-channel-slack">send-to-channel-slack</a> 과 같이 Slack에 메세지를 보내는 Task가 있습니다. 그러나 <strong>ArgoCD처럼 상태에 따라 메세지를 다르게 출력해주는 Task는 없는 것 같습니다.</strong></p> <p>기본적으로 상태에 따라 박스 색과 메세지 내용을 다르게 출력하도록 Task를 만들었습니다.</p> <blockquote>Task 구성 시 <a href="https://tekton.dev/docs/pipelines/variables/">필요 변수가 기본적으로 제공되는지 공식 문서</a>를 꼭 확인하시길 바랍니다.</blockquote> <pre>apiVersion: tekton.dev/v1beta1<br />kind: Task<br />metadata:<br />  name: send-to-channel-slack<br />  labels:<br />    app.kubernetes.io/version: &quot;0.1&quot;<br />  annotations:<br />    tekton.dev/pipelines.minVersion: &quot;0.12.1&quot;<br />    tekton.dev/categories: Messaging<br />    tekton.dev/tags: messaging<br />    tekton.dev/platforms: &quot;linux/amd64,linux/s390x,linux/ppc64le&quot;<br />spec:<br />  description: |<br />    이 태스크는 Slack 채널에 Tekton Bot 결과 메시지를 컬러 블록과 함께 전송합니다.<br />    성공 시 초록색, 실패 시 빨간색 컬러 바가 표시됩니다.<br />  params:<br />    - name: token-secret<br />      type: string<br />      description: secret name of the slack app access token (key는 token)<br />      default: token-secret<br />    - name: channel<br />      type: string<br />      description: channel id <br />    - name: status<br />      type: string<br />      description: Succeeded/Failed 등 파이프라인 상태<br />    - name: pipeline-name<br />      type: string<br />      description: 파이프라인 이름<br />    - name: pipelinerun-name<br />      type: string<br />    - name: username<br />      type: string<br />      description: 슬랙 메시지 발신자 이름<br />      default: &quot;Tekton Bot&quot;<br /><br />  steps:<br />    - name: post<br />      image: curlimages/curl:7.70.0<br />      env:<br />        - name: TOKEN<br />          valueFrom:<br />            secretKeyRef:<br />              name: $(params.token-secret)<br />              key: token<br />      script: |<br />        #!/bin/sh<br />        if [ &quot;$(params.status)&quot; = &quot;Succeeded&quot; ]; then<br />          COLOR=&quot;#4BB543&quot;<br />          EMOJI=&quot;:white_check_mark:&quot;<br />          STATUS_TEXT=&quot;*성공*&quot;<br />        else<br />          COLOR=&quot;#FF0000&quot;<br />          EMOJI=&quot;:x:&quot;<br />          STATUS_TEXT=&quot;*실패*&quot;<br />        fi<br /><br />        cat &lt;&lt;EOF &gt; /tmp/slack_message.json<br />        {<br />          &quot;channel&quot;: &quot;$(params.channel)&quot;,<br />          &quot;username&quot;: &quot;$(params.username)&quot;,<br />          &quot;attachments&quot;: [<br />            {<br />              &quot;color&quot;: &quot;$COLOR&quot;,<br />              &quot;blocks&quot;: [<br />                {<br />                  &quot;type&quot;: &quot;header&quot;,<br />                  &quot;text&quot;: {<br />                    &quot;type&quot;: &quot;plain_text&quot;,<br />                    &quot;text&quot;: &quot;$EMOJI Tekton CI 결과&quot;<br />                  }<br />                },<br />                {<br />                  &quot;type&quot;: &quot;section&quot;,<br />                  &quot;text&quot;: {<br />                    &quot;type&quot;: &quot;mrkdwn&quot;,<br />                    &quot;text&quot;: &quot;*상태*: $STATUS_TEXT\n*파이프라인*: \`$(params.pipeline-name)\`&quot;<br />                  }<br />                }<br />              ]<br />            }<br />          ]<br />        }<br />        EOF<br /><br />        /usr/bin/curl -X POST \<br />          -H &#39;Content-type: application/json&#39; \<br />          -H &quot;Authorization: Bearer $TOKEN&quot; \<br />          --data @/tmp/slack_message.json \<br />          https://slack.com/api/chat.postMessage</pre> <p>pipeline에서는 무조건 실행되어야 하므로 finally 에서 Task를 정의해줍니다. 상태값은 Pipeline이 아닌 Task 단위로 확인해야 하므로 tasks.status 변수를 통해 Task의 상태를 가져옵니다.</p> <pre>  finally:<br />    - name: notify-tasks<br />      taskRef:<br />        name: send-to-channel-slack<br />      params:<br />        - name: token-secret<br />          value: token-secret<br />        - name: channel<br />          value: &lt;channel_id&gt;<br />        - name: status<br />          value: &quot;$(tasks.status)&quot;<br />        - name: pipeline-name<br />          value: &quot;$(context.pipeline.name)&quot;<br />        - name: pipelinerun-name<br />          value: &quot;$(context.pipelineRun.name)&quot;<br />        - name: username<br />          value: &quot;Tekton Bot&quot;</pre> <p>이를 통해 슬랙에서 Tasks들의 상태에 따라서 성공과 실패 메세지를 확인할 수 있습니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/681/1*rAgg_kUdr7TKJTYkIQj4kQ.png"/><figcaption>Tekton 성공 Slack Message</figcaption></figure> <p>다만 위의 내용 만으로 어떤 Task에서 발생했는지 구체적인 정보를 확인할 수 없습니다. 그러므로 추후 Tekton의 성격을 살려서 깔끔하고 정확한 슬랙 메세지를 작성하고 싶습니다.’</p> <h3>기타</h3> <p>Tekton 파일을 보시면 리소스의 재사용성을 고려하지 않고 작성했다는 것을 확인할 수 있습니다. 특히 글을 쓰는 과정에서 잘 작성하지 못했다는 것을 더욱 느낀 것 같습니다. 앞으로 시간이 있으면 지금까지 작성한 스크립트를 정리해야 겠습니다.</p> <h3>References</h3> <ul><li><a href="https://velog.io/@sgwon1996/Jenkins-vs-GitHub-Action-vs-Tekton">Jenkins vs GitHub Action vs Tekton</a></li><li><a href="https://gruuuuu.github.io/cloud/tekton-trigger/">Tekton Trigger를 사용하여 Pipeline 자동으로 돌려보기</a></li><li><a href="https://techblog.lycorp.co.jp/ko/automate-streaming-pipeline-validation-with-kubernetes-native-workflows-2">https://techblog.lycorp.co.jp/ko/automate-streaming-pipeline-validation-with-kubernetes-native-workflows-2</a></li><li><a href="https://tekton.dev/docs/pipelines/">https://tekton.dev/docs/pipelines/</a></li><li><a href="https://tekton.dev/docs/triggers/cel_expressions/">https://tekton.dev/docs/triggers/cel_expressions/</a></li><li><a href="https://medium.com/epam-delivery-platform/part-1-tekton-adoption-d5d47bf1bfc0">https://medium.com/epam-delivery-platform/part-1-tekton-adoption-d5d47bf1bfc0</a></li><li><a href="https://tekton.dev/docs/pipelines/variables/">https://tekton.dev/docs/pipelines/variables/</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f4898483329c" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/s0okju-tech/gitops-%EC%A0%84%ED%99%98%EA%B8%B0-4-tekton%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-ci-%EA%B5%AC%EC%84%B1%ED%95%98%EA%B8%B0-f4898483329c">GitOps 전환기 — Part 4. Tekton을 활용한 CI 구성하기</a> was originally published in <a href="https://medium.com/s0okju-tech">S0okJu Technology Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">GitOps 전환기 - Part 3. GitOps 파이프라인 계획 세우기</title><link href="https://s0okju.github.io/blog/2025/gitops-part-3-gitops/" rel="alternate" type="text/html" title="GitOps 전환기 - Part 3. GitOps 파이프라인 계획 세우기"/><published>2025-05-03T12:41:37+00:00</published><updated>2025-05-03T12:41:37+00:00</updated><id>https://s0okju.github.io/blog/2025/gitops----part-3-gitops---</id><content type="html" xml:base="https://s0okju.github.io/blog/2025/gitops-part-3-gitops/"><![CDATA[<h3>GitOps</h3> <h4>개념</h4> <p>GitOps란 DevOps의 적용하는 실천 방법 중 하나로 클라우드 네이티브 애플리케이션을 대상으로 한 지속적인 배포에 초점을 맞추고 있습니다. 이름에서 나타나듯이 배포와 운영에 관련된 모든 요소를 Git으로 관리하는 것에 초점이 맞춰져 있습니다. <br/>GitOps는 배포와 관련된 모든 것을 선언형 기술 형태로 작성하여 이러한 파일들을 레포지토리 형태로 관리합니다. 그리고 선언형 기술서와 운영 환경 간 상태 차이가 없도록 유지시켜주는 자동화 시스템을 구성합니다.</p> <h4>DevOps vs GitOps</h4> <p>DevOps는 개발과 운영의 합성어로 소프트웨어 개발과 IT 운영 팀 간의 협업을 강화하여 지속적인 통합과 배포를 가능하게 하는 문화 및 철학을 의미합니다. <br/>앞서 GitOps는 DevOps의 문화 중 클라우드 네이티브 환경에 가장 적합한 방법론이라고 보시면 됩니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*G8m6QlAaHXvD5GYY8wKOIw.png"/><figcaption>출처 — <a href="https://www.techtarget.com/searchitoperations/tip/Compare-GitOps-vs-DevOps-for-modern-app-deployments">https://www.techtarget.com/searchitoperations/tip/Compare-GitOps-vs-DevOps-for-modern-app-deployments</a></figcaption></figure> <p>DevOps의 주요 원칙은 아래와 같습니다.</p> <ul><li>지속적인 통합(CI)</li><li>지속적인 배포(CD)</li><li>자동화 및 모니터링</li><li>협업과 소통</li></ul> <p>프로젝트와 함께 설명할때는 <strong>지속적인 통합과 지속적인 배포를 중심</strong>으로 설명하겠습니다.</p> <h3>GitOps 파이프라인 계획 세우기</h3> <h4>GitOps를 선택한 이유</h4> <p>GitOps는 클라우드 네이티브 환경 특히 쿠버네티스에 적합한 방법론입니다.</p> <blockquote>클라우드 네이티브 애플리케이션이 아니어도 깃옵스를 적용할 수 있으나 아래에서 설명드릴 <strong>선언형 모델(Declarative Model)을 지원하는 최근 도구들이 클라우드 네이티브에 중점</strong>을 두기 때문에 어려움을 겪을 수 있습니다. <strong>위브웍스는 아예 쿠버네티스 대상</strong>이라고 못박고 있습니다.<br/>출처 — <a href="https://www.samsungsds.com/kr/insights/gitops.html">데브옵스의 확장 모델 — 깃옵스(GitOps) 이해하기 | 인사이트리포트 | 삼성SDS</a></blockquote> <p>저는 현재 쿠버네티스 환경에서 시스템을 운영하고 있습니다. 그러므로 <strong>쿠버네티스 환경에 가장 적합한 자동화 통합과 배포를 수행하고 싶어 선택</strong>하게 되었습니다.</p> <h4>필요 항목</h4> <p>프로젝트에 적용하기 전에 필요한 항목은 아래와 같습니다.<br/>1. 프로젝트 GitHub <br/>2. 컨테이너 이미지를 만드는 CI 파이프라인<br/>3. 컨테이너 이미지 저장소<br/>4. 쿠버네티스 매니페스트(Manifest) 저장소<br/>5. Manifest를 클러스터에 동기화하고 변화를 감지하는 GitOps 엔진</p> <h4>설계하기</h4> <p><strong>Manifest 저장소</strong><br/>프로젝트에서 3개의 레포지토리를 관리하고 있습니다. <br/>그러나 Manifest 저장소는 하나로 관리하는 것이 편리하다고 판단하여 <strong>프로젝트명으로 폴더를 분리하여 Kustomize 코드를 저장</strong>했습니다.</p> <p><strong>CI/CD 도구</strong><br/>CI/CD 도구는 <strong>쿠버네티스 클러스터 내부에서 동작해야 한다는 기준을 가지고 선정</strong>했습니다. 컨테이너 레지스트리(Harbor), 배포할 대상 모두 쿠버네티스 내부에서 동작하기 때문입니다. 이러한 기준점을 가지고 클라우드 네이티브 CI/CD 도구인 <a href="https://tekton.dev/">Tekton</a>, <a href="https://argo-cd.readthedocs.io/en/stable/">ArgoCD</a>를 선택하게 되었습니다. <br/>Tekton이 GitHub Webhook을 통해 main 브랜치의 변경 이벤트에 맞춰 자동으로 이미지 빌드를 수행합니다. 생성한 이미지의 해시값을 Manifest 저장소에 업데이트를 수행합니다. <br/>ArgoCD가 Manifest 저장소의 주기적으로 변경사항을 감지합니다. 변경 사항이 감지되면 Manifest 저장소에 맞춰 동기화를 수행합니다.</p> <p><strong>Slack Notifications</strong><br/>더 나아가 지속적인 통합(CI)과 배포의 실행 상태(CD)를 파악하기 위해 슬랙 채널에 메세지를 보낼 수 있도록 설계했습니다. 이를 통해 프로젝트 팀원이 CI/CD 실행 현황을 파악할 수 있도록 설정합니다.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/831/1*J7owwscpwWS-IWITaZNxCw.png"/><figcaption>설계도</figcaption></figure> <h3>다음 이야기</h3> <p>다음 글에서는 Tekton을 활용하여 어떻게 CI를 구성했는지 설명합니다.</p> <h3>Refereneces</h3> <ul><li><a href="https://www.samsungsds.com/kr/insights/gitops.html">https://www.samsungsds.com/kr/insights/gitops.html</a></li><li><a href="https://tech.ktcloud.com/entry/What-is-DevOps-DevOps-CICD">What is DevOps? — DevOps &amp; CI/CD</a></li></ul> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4af79e4ed80f" width="1" height="1" alt=""/>&lt;hr&gt;&lt;p&gt;<a href="https://medium.com/s0okju-tech/gitops-%EC%A0%84%ED%99%98%EA%B8%B0-3-gitops-%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8-%EA%B3%84%ED%9A%8D-%EC%84%B8%EC%9A%B0%EA%B8%B0-4af79e4ed80f">GitOps 전환기 - Part 3. GitOps 파이프라인 계획 세우기</a> was originally published in <a href="https://medium.com/s0okju-tech">S0okJu Technology Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.&lt;/p&gt;</p>]]></content><author><name></name></author></entry></feed>