[{"content":" 해당 게시글은 재미를 위해 각종 밈이 포함되어 있습니다.\n인사말 안녕하세요.\n2024년 11월부터 2025년 3월까지 5개월이라는 긴 시간 동안 HexaCTF 시리즈 글이 마무리 되었습니다.\n취업하기 전에 글을 마무리하는 것이 목표였는데 달성했네요. 🥹 HexaCTF 시리즈는 제가 프로젝트를 하면서 작성한 글입니다. 그래서 대회를 마친 시점에서 보면 색다르게 느껴집니다. 제 감상을 기반으로 상을 주려고 합니다.\n수상 목록 설명하기 난감해상 혼란상 구성이 아쉽상 아이디어 실천상 최선의 선택상 수상 발표 설명하기 난감해상 글로 풀어 쓰기 어려운 글에게 주는 상입니다.\n👑 Top 1. HexaCTF 8. 커스텀 컨트롤러를 통한 Challenge 생성 및 삭제 구현 이 부분에서 잠깐 글쓰기를 포기할 뻔 했습니다. Operator 패턴은 클라우드 네이티브 환경에서 많이 쓰이면서 정작 로직 관련 글이 많이 없었습니다. 그래서 제한된 정보를 가지고 글을 써야 했습니다. 확신도 많이 없었구요\u0026hellip; 일단 제가 이해한 내용 기반으로 작성하긴 했지만 나중에 더 깊게 공부하면 수정해야 할 것 같습니다.\n저에게 정말 큰 좌절감을 안겨줬기 때문에 설명하기 난감해상 1등 자리를 줬습니다.\nTop 2. HexaCTF 11. 오픈 스택 서버 네트워크 개선기 - 서버에 NIC 2개 삽입한 이유 머리 속으로는 그림이 그려지는데 글로 절대 안써집니다. 네트워크 인터페이스인데 NIC로 연결해서 생성된 네트워크 인터페이스와 가상 네트워크 인터페이스도 분리해서 써줘야 합니다.\n제일 머리 아픈 것은 같은 사설 IP인데 라우터에서 스위치를 통해 할당받은 IP인지 아니면 가상 네트워크 인터페이스에서 할당된 IP인지 구분하는 것입니다.\n그래서 네트워크 관련 글에는 예시가 들어가 있습니다.\n동일한 네트워크 대역을 가지기 위해 NIC는 모두 동일한 스위치에 연결됩니다. 192.168.50.1/24 대역이라면 서버는 192.168.50.27로 할당받고 Floating IP로도 192.168.50.122 이런 식으로 할당받는 것이죠. 이처럼 Provider Network는 서버와 동일한 대역의 IP를 가지게 됩니다.\n제가 글을 쓰면서 느끼는 것은 네트워크 분야는 글이 아니라 그림으로 표현하는게 훨씬 효율적이라는 것입니다. 왜냐하면 블록(범위) 단위로 시스템을 바라보기 때문이죠.\n머리 속 그림은 그려지지만 적절한 단어 선정과 손이 따라주지 않아서 설명하기 난감해상 2등 자리를 줬습니다.(이게 1등보다 더 열받는 부분일지도\u0026hellip;)\n혼란상 글 쓰는 당시에도 지금도 혼란스러운 글에게 주는 상입니다.\n👑 Top 1. HexaCTF 8. 커스텀 컨트롤러를 통한 Challenge 생성 및 삭제 구현 설명하기 난감해 상과 똑같은 이유입니다. 정보가 많이 없습니다. Kubebuilder 공식 문서에는 \u0026ldquo;너 Operator 패턴 알지?\u0026rdquo; 와 같이 교수님 마인드로 작성되어 있습니다.\n그래서 Kubebuilder에서 제시된 키워드를 일리리 검색해서 각각의 컴포넌트가 무슨 역할을 하는지 다 찾아봐야 합니다.\n정보를 찾고 나서 하나로 정리하는 작업이 필요한데, 여기서 머리가 어지러웠습니다.\n\u0026ldquo;원래는 Builder가 있는데 Kubebuilder 공식 사이트에는 왜 Builder가 없는거지?\u0026rdquo; \u0026ldquo;Manager는 어떻게 Controller를 만드는거야? 이게 코드로 구현이 가능해?\u0026rdquo; \u0026ldquo;왜 누군가는 Operator로 말하고 누군가는 Controller라고 부르는 거야?\u0026rdquo; 원래 시스템 그림을 보고 구현 방식을 어렴풋이 파악하는 편인데, 이번에는 하나도 파악하지 못했습니다. 실제로 KubeBuilder 소스코드 분석을 도전했습니다. 그러나 Go를 제대로 사용한 경험이 없어서 실패하게 되었습니다. Go 공부를 다시 해야겠군요.\n저에게 영원한 혼란스러움을 안겨줘 혼란상 1등을 줬습니다.\n구성이 아쉽상 구성이 아쉬운 글에게 주는 상입니다.\n👑 Top 1. HexaCTF - 1. 인프라 플랫폼 선정과 서버 이전 시 마주한 문제 vlan 네트워크 구성한 이유와 실패했지만 그 대신 어떻게 했는지 설명했으면 괜찮은 글이라고 생각합니다. 그러나 글만 보면 선택의 이유가 명확해 보이지 않습니다.\n글 작성 시점이 5개월 전이라 저때 당시 무슨 기준점을 가지고 저런 선택을 했는지 기억이 나지 않습니다.\n대체 내가 무슨 생각으로 저 글을 썼는지 의문이 가서 구성이 아쉽상 1등을 드립니다.\nTop 2. HexaCTF - 3. Jenkins를 활용한 CI 환경 구성하기 저희 프로젝트에서는 Jenkins를 두 가지 이유로 사용됩니다. 하나는 CTF에 출제할 문제의 이미지를 빌드하기 위해 하나는 CTF 웹 애플리케이션 이미지를 빌드하기 위해 사용됩니다. 글 작성 당시에는 저것까지 생각을 안하고 글을 쓴 것 같다는 생각이 듭니다.\n젠킨스 사용 목적에 대해 명확하게 작성하지 않았으므로 구성이 아쉽상 2등을 드립니다.\nTop 3. HexaCTF 7. Challenge CR 상태 정의와 Operator 개념 정리 Challenge CR 글을 작성하다가 갑자기 Operator의 개념이 나온 글입니다. 이때의 감정은 아직도 생생한데요. 본래 계획이라면 Challenge CR를 설명하고 HexaCTF 8에서 설명한 내용을 정리할 예정이었습니다.\n그러나 글로 서술하는 과정에서 \u0026ldquo;Operator는 근본적으로 무엇을 만드는거지?\u0026rdquo; 라는 의문점을 가지게 되었고, 쿠버네티스 시스템 관점에서 Operator의 개념에 대해 작성하게 되었습니다.\n거의 의식의 흐름으로 글을 전개했으므로 구성이 아쉽상 3등을 드립니다.\n아이디어 실천상 나름 만족한 아이디어를 실천한 글에게 주는 상입니다.\n👑 Top 1. HexaCTF 6. Kubebuilder를 활용한 ChallengeDefinition\u0026amp;Challenge Type 구현 Challenge 구조를 설계할 당시 AWS SysOps 자격증을 준비하고 있었습니다. CloudFormation 범위를 중심으로 공부하고 있었는데, \u0026ldquo;Template이라는 개념을 프로젝트에 대입할 수 있을까?\u0026rdquo; 라는 생각이 들었고 직접 적용하기로 결심했습니다.\n지금은 ChallengeDefinition이지만 초기 이름은 ChallengeTemplate이었습니다. 자주 사용하는 플랫폼 구성을 작은 Template으로 구성하여 CloudFormation 처럼 여러 개의 Template를 참조할 수 있도록 설계했습니다. 그렇다면 사용자는 사용할 템플릿과 필요한 소수의 정보만으로 Challenge를 생성할 수 있게 됩니다.\n그런데 중간 점검에 팀원과 상의하는 과정에서 이러한 방식은 사용자에게 오히려 혼란을 야기할 수 있다는 의견을 받았습니다. 의견을 일부 수용하여 구성 파일에는 쿠버네티스 구성 형식을 따르기로 결정했고 ChallengeTemplate 대신 ChallengeDefinition으로 이름을 바꾸게 됩니다.\n아이디어를 직접 구현한 것과 더불어 아이디어에 서사가 있어 아이디어 실천상 1등을 드립니다.\nTop 2. HexaCTF 10. 아키텍쳐 설계 및 Challenge Control API 제작 Challenge API를 별도로 생성하여 기존의 쿠버네티스 요청 방식이 아닌 독자적인 규격으로 생성을 요청할 수 있도록 구현했습니다. 즉 json 3개의 키-값만으로 하나의 Challenge를 만들 수 있는 것이죠.\nChallenge API와 Operator를 활용하여 Challenge만의 독자적인 오케스트레이터를 만들었으므로 아이디어 실천상 2등을 드립니다.\n최선의 선택상 👑 공동 Top 1. HexaCTF의 모든 글들 시간 차를 두고 글을 쓰다 보면 \u0026ldquo;왜 나는 이런 선택을 했지?\u0026rdquo; 라는 생각을 하곤 합니다. 냉정한 평가는 뒤로 미루고 그때의 선택이 최선이었음을 인정하는 것도 중요하다고 생각합니다.\n부족한 부분이 있으면 앞으로 고치면 되니까요. 😄\n스스로에게 고생했다는 의미에서 HexaCTF의 모든 글에게 최선의 선택상을 드리겠습니다.\n수상 소감 모든 글이 잘 썼다는 것이 아닙니다. 오히려 아쉬운 점을 나열하라고 하면 블로그로 몇편을 더 쓸 수 있을 것 같습니다.\n본래 글쓰기를 잘하는 편이 아니지만 서투른 필력으로 13편(12 + 후기)을 작성한 건 나름 자랑스럽습니다.\n이렇게 꾸준히 하다보면 저의 실력도 글쓰기 실력도 늘어나겠죠.\nHexaCTF 시리즈 끝.\n","date":"2025-03-16T11:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-awards/","title":"HexaCTF Awards. 12개의 HexaCTF 글에게 상을 줍시다."},{"content":"이제부터 워커 노드를 2개 선정한 이유와 쿠버네티스에 웹 애플리케이션을 배포하면서 느낀점을 작성하겠습니다.\n쿠버네티스 워커 노드 구성 초기에 저는 사용자가 배포한 Challenge만의 독립적인 환경이 필요했습니다. 그래서 워커 노드를 2개 설치한 후 nodeSelector를 활용하여 애플리케이션 역할에 따라 각 노드에 배치될 수 있도록 구성했습니다.\n하나의 워커 노드에 웹 애플리케이션과 기타 모니터링 도구를 배치하고 다른 하나에는 Challenge 서버만 배치하도록 설계했습니다.\n질문 1. 왜 워커 노드가 3개가 아닌가요? 아래의 질문을 할 수 있을 것 같습니다.\n\u0026ldquo;프로그램 역할에 따라 노드 별로 배치한다고 하셨잖아요. 그렇다면 노드 3개 배포해야 하는 것 아닌가요? \u0026quot;\n위의 질문은 옳은 말이지만 서버 리소스가 부족하여 노드 3개를 배포하지 못했습니다. 쿠버네티스는 배포할때 Kubespray 플랫폼을 활용합니다. Kubespray에 워커 노드를 배포할때는 모두 동일한 이미지와 리소스 타입이 적용됩니다. 워커 노드에 이미지를 ubuntu:20.04에 리소스 타입으로 2vCPU, 4GB로 선택했다면 워커 노드 모두 동일하게 적용되는 것이죠.\n쿠버네티스 인스턴스에 배치된 리소스 양 워커 노드는 메모리 선정 기준이 확실히 있었으나 나머지는 kublr에서 제시한 기준표의 2배를 산정해서 생성했습니다. 넉넉하면 좋지 않을까 하는 마음으로 리소스를 늘렸던 것 같습니다.\n마스터 노드 : 4vCPU, 8GB 워커 노드: 8vCPU, 16GB 그러나 실제 하드웨어는 64GB의 메모리를 가지고 있으며 Jenkins, NFS 서버, OpenStack 서버의 메모리 양을 고려했을때 워커 노드를 3개를 배치할 수 없었습니다.\n질문 2. 워커 노드의 메모리를 16GB로 선택한 이유 CPU는 연산할때 주로 많이 쓰이는 리소스입니다. 그러나 CTF 대회 문제 중에서 연산 집중적인 문제가 큰 비중을 차지하지 않습니다. 전반적으로 CPU 사용량이 무난하다고 판단해 추가적인 CPU 사용 기준을 설정하지 않았습니다.\nChallenge의 최대 사용량을 계산했을때 16GB 메모리가 적당하다고 생각하고 있습니다.\n출제할 문제의 베이스 이미지는 쿠버네티스 클러스터를 배포할 때 알 수 없습니다. 미리 예측해서 배포해야 합니다. 그러므로 최대 사용 가능한 컴퓨팅 자원량을 계산하여 자원 부족으로 인해 문제가 발생하지 않도록 설계해야 했습니다.\nChallenge 문제에 쿠버네티스 limit quota를 활용해 리소스의 최대 사용량을 설정할 것입니다. 이 정도를 토대로 총 예상 사용량을 계산하기로 결정했습니다.\nChallenge 서버의 특징 Challenge 서버는 특정 기능 하나만 구현되어 있어 단순하다는 특징을 가지고 있습니다. 또한 한명만 서버에 접속할 수 있죠. CPU, 메모리 활용 관점에서 예측하면 다음과 같습니다.\nCPU - 빌드 초기에 많이 쓰인다. 그 이외에는 무난할 것 같다. 메모리 - 메모리 상한선이 정해져 있을 것 같다. 실제 대회를 운영하면서 예상한 패턴이 어느정도 맞긴 했습니다.\n메모리 사용량만 고려한 이유 우선 CPU는 선정 기준을 아직까지도 감을 찾지 못했습니다. 🥹 이건 실무에서 한번 배우고 싶네요.\n메모리는 CPU과 다르게 초기에 데이터를 적재한 후에 실행되는 특징을 가지고 있습니다. CPU가 사용 후 바로 반납하는 느낌이라면 메모리는 누적의 느낌이 강합니다.(개인적인 생각입니다.) 메모리는 관리를 제대로 하지 않으면 빨리 소모되는 자원인 셈이죠. 그러므로 limit, request quota로 사용 가능한 최대 메모리 용량를 지정하여 불필요한 사용량을 줄이는 것이 중요하다고 생각했습니다.\n예상 사용 메모리 용량 계산 Challenge를 활용하는 CTF 문제는 주로 Web, System입니다. System은 가벼운 리눅스 이미지를 활용하며 자원 사용량의 편차가 크지 않습니다. 반면 Web의 경우에는 어떤 이미지를 활용했냐에 따라 편차가 매우 큽니다. 그러므로 상한선을 선택할 때는 스프링부트 플랫폼의 평균 메모리 사용량 256MB를 기준으로 계산했습니다.\n256MB(스프링부트 기준 메모리 양) x 3(사용자 당 동시 실행 컨테이너 수) x 20(사용자) = 15GB\n15GB는 제가 생각한 최대 메모리 사용량이라고 생각합니다. 사용자 수가 2배, 3배로 크게 증가하지 않는 한 예상 범위 내로 메모리를 활용할 것으로 보입니다.\n여담 System 문제와 Web 문제의 메모리 사용량은 2배 ~ 4배 차이로 상당히 큽니다. 대회마다 문제 양과 유형이 크게 변화하지 않기 때문에 메모리 사용량을 비율로 계산할까 고민했습니다. 실제 대회에 참가한 친구들에게 물어보니 Web 문제에 가장 많이 도전하고 시간 투자를 많이 한다고 합니다. 그래서 Web 문제의 최대 메모리 사용량을 기준으로 선택했습니다.\nChallenge request, limit quota 저는 모든 Challenge의 Deployment에 아래와 같은 resource quota를 설정했습니다.\n1 2 3 4 5 6 7 resources: limits: cpu: \u0026#34;500m\u0026#34; memory: \u0026#34;512Mi\u0026#34; requests: cpu: \u0026#34;250m\u0026#34; memory: \u0026#34;256Mi\u0026#34; 왜 최대 사용량을 256MB으로 지정했으면서 실제로는 512MB로 선정했는지 궁금하셨을 것 같습니다.\n왜 DevOps, SRE가 개발 경험이 있어야 하는지 뼈저리게 느꼈습니다. 경험이 없으니 전반적으로 감을 잡을 수 없었습니다.\nChallenge 서버에서 기능은 하나 뿐이고 사용자도 실질적으로 한명이기 때문에 리소스 사용량이 크게 변하지 않을 것이라고 생각했습니다. 그런데 눈으로 직접 보지 않는 이상 모르겠다는 생각을 했습니다.\n마침 참여가 확정된 사용자는 예상보다 적어서 사용 패턴을 확인할 겸 limit를 512MB로 선정하게 되었습니다.\n쿠버네티스 HexaCTF 서비스 아키텍처 설계 및 회고 웹 애플리케이션은 hexactf라는 네임 스페이스에 배포했습니다. hexactf 내에 있는 애플리케이션은 모두 하나의 Helm으로 배포했습니다.\nOperator 배포 방식에 대한 자세한 내용은 HexaCTF9를 참고하시길 바랍니다.\n눈썰미 좋으신 분들은 알겠지만 Queue 또한 메세지를 저장해야 하므로 Statefulset으로 배포해야 합니다.\n아쉬운 점 가장 아쉬운 점은 모든 애플리케이션을 하나의 Helm으로 배포한 것입니다.\n데이터베이스 운영하면서 DB에 진짜 오류가 많았습니다. kubectl logs 로 매일 문제점을 파악했는데요. 로그만으로는 명확한 원인은 파악할 수 없었지만 InterfaceError가 빈번하게 나타나는 것을 보아 Connection leak 문제라고 생각했습니다. 1 pymysql.err.InterfaceError: (0, \u0026#39;\u0026#39;) 그래서 문제의 근거를 찾고자 Prometheus의 ServiceMonitor 연결을 시도했으나 실패했습니다. 시간이 없어서 끝까지 해보지는 못했습니다.\n대회가 끝나고 다시 생각해보니 Operator로 MariaDB를 따로 배포하면 좋지 않았을까 하는 아쉬움이 있습니다. 내부적으로 모니터링 기능을 쉽게 설정할 수 있으며 상황에 따라서 확장과 관리가 용이하기 때문입니다.\n큐 사진에는 Queue라고 적었지만 정확하게 Kafka입니다.\nKafka를 선정한 이유는 한번이라도 써보고 싶었기 때문입니다. ^-^! 쓰고 나서 후회했습니다.\n눈썰미가 좋으신 분들은 알고 계셨겠지만 Queue 또한 StatefulSet으로 배포해야 합니다. 큐에는 메세지가 저장되어 있기 때문에 상태를 저장해야 하기 때문입니다.\n프로젝트에서는 운이 좋게도 큐에 문제가 생기지 않아 데이터 유실이 되지 않았습니다. 그러다 대회를 마무리하고 나서 보니 배포 리소스를 잘못 설정했다는 것을 깨닫게 되었습니다.\nQueue도 프로젝트의 Helm으로 관리하는 것이 아닌 분리해서 Operator로 관리하는 것이 좋다고 생각했습니다. 데이터베이스와 동일한 이유로 확장과 관리가 용이하기 때문입니다.\n배포 프로세스 회고 작은 일도 수작업은 힘들다. 프로젝트 당시 자동화는 하나도 하지 않고 수작업으로 변경사항을 일리리 확인하고 빌드하고 배포했습니다.\n젠킨스 서버는 존재합니다. 그런데 주요 목적은 수동으로 HexaCTF 웹 애플리케이션, Challenge 이미지를 빌드하여 이미지 레지스트리에 전송하는 용도였습니다.\n초기에는 프로젝트가 소규모이므로 CI/CD는 필요없다는 의견이 대다수였습니다. 저도 어느 정도 동의했으며 한 사람(본인)이 모든 배포 프로세스를 수용할 수 있을 것이라고 생각했습니다.\n대회 일주일 전에 git pull, helm upgrade 명령어만 무한 반복으로 치니 다양한 생각이 들었습니다.\nHelm으로 동시에 배포했는데 애플리케이션 하나는 이전 버전으로 롤백하고 다른 애플리케이션은 업그레이드 버전을 활용해야 하는 경우 -\u0026gt; 하나로 배포하는게 아니라 팀 단위로 쪼개서 CI/CD 쓸걸 Helm upgrade 했는데 빌드 과정을 생략하여 변경 사항이 반영되지 않은 경우 -\u0026gt; 젠킨스로 이미지를 자동으로 빌드할걸 branch를 잘못 설정했으면서 수동으로 빌드하기 눌러 구버전으로 이미지 빌드하기 -\u0026gt; 젠킨스로 이미지를 자동으로 빌드했으면 내가 브랜치를 잘못 썼다는 걸 알았을텐데 helm 파일 수정본이 반영되지 않는 경우 -\u0026gt; ArgoCD 쓸걸 문제 발생 시 팀원에게 직접 알리기 -\u0026gt; CI/CD에 있는 알림 기능을 활용할걸 기타 등등 \u0026hellip; 휴먼 이슈로 인해 예상보다 문제는 많았고 자동화를 안한 것을 뼈저리게 후회했습니다.\n마치며 웹 애플리케이션 배포에 대해 기술적으로 작성할만한 것은 없었습니다. 자동화라는 좋은 시스템을 버리고 선사 시대의 방식으로 수행했으니까요. 그만큼 몸소 느낀점이 많으며 제목도 배포가 아닌 \u0026ldquo;회고\u0026quot;로 지은 것 같습니다.\n이제 기술적인 내용은 끝입니다. 5개월 끝에 드디어 글을 모두 작성할 수 있었네요.\n다음에 \u0026ldquo;HexaCTF 시리즈 어워드\u0026quot;를 마지막으로 시리즈를 마치겠습니다.\n","date":"2025-03-16T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-12/","title":"HexaCTF 12. 쿠버네티스 멀티 노드 선정 이유와 웹 애플리케이션 배포 회고"},{"content":" 2025-03-15 - 내용 수정 들어가며 간략한 인프라 구성도입니다.\n여기서 눈에 띄는 점이 2가지가 있습니다.\n한 서버(OpenStack)에 NIC가 2개 일부 사용자는 VPN 사용하여 접속한다 이제부터 왜 오픈스택 서버에 2개의 NIC를 삽입했는지 설명하겠습니다.\nNIC 1개 삽입 시 문제점 OpenStack은 기본적으로 2개의 네트워크 인터페이스가 요구됩니다. 그래서 설치가 비교적 간단한 DevStack이든 kolla-ansible이든 하나의 물리적 NIC로만 구성한다면 운영체제에서 하나의 논리적 네트워크 인터페이스를 만들어서 설정해야 합니다.\n하나의 NIC로만 구성하면 172.24.4.0/24(상황에 따라 다름)와 같은 네트워크 인터페이스를 생성하여 Provider Network를 구성하게 됩니다. 그러나 Provider Network로부터 IP를 사설 IP(172.24.4.x)로 할당받으므로 외부 컴퓨터(Manager)에서 직접 접근할 수 없습니다.\n결국 Floating IP가 할당된 가상머신에 접근하기 위해서는 서버 내에 NAT 설정을 해야 합니다.\n실제 프로젝트에서는 Provider Network를 물리 네트워크 인터페이스에 연결시키고 Management Network는 논리 네트워크 인터페이스로 연결했습니다. 즉 Floating IP를 192.168.50.x로 할당을 받을 것이며 아무런 라우팅 설정 없이 직접 접근할 수 있도록 구성했습니다. 왜냐하면 Management Network에서는 하나의 IP만 활용되지만, 많은 가상 머신을 만들면서 Provider Network를 통해 2개 이상의 IP가 필요하기 때문입니다.\n대신 서버의 IP로 구성된 Resource API에 접근하기 위해서는 별도의 라우팅 설정이 요구됩니다. 라우팅 설정하는 방법은 크게 두 가지입니다.\n라우터에 라우팅 규칙 설정 클라이언트(Manager), 서버의 라우팅 테이블 수정 1. 라우터에 라우팅 규칙 설정의 문제점 그곳에는 많은 서버들이 있다.\n서버가 있는 장소가 제 것만 있으면 1번은 당연한 선택지일 것입니다. 그러나 제 서버 이외에도 다른 사람들의 서버가 있으며 그것들 또한 가상화 솔루션 서버로 이뤄져 있습니다. 공용으로 사용되는 라우터에 라우팅 테이블을 설정하면 문제가 하나 발생하게 됩니다. 다른 서버 이용자가 의도치 않게 OpenStack의 사설 네트워크와 동일한 IP에 접속을 요청하면 OpenStack 서버로 패킷이 전송되는 문제가 발생할 수 있습니다.\n실제 경험해보지 않았지만 제가 예상한 문제 시나리오는 크게 두 가지 입니다.\n다른 사용자(User2) 가 자신의 서버에 접근을 요청할 경우 다른 사용자가 User 2의 서버에 접속하여 가상 머신에 접근을 요청할 경우 시나리오의 모든 전제 조건은 라우팅 테이블의 최고 우선순위가 default(0.0.0.0) -\u0026gt; 라우터 게이트웨이인 경우 입니다. 다시 말해 라우팅 테이블의 우선순위를 수정하면 아래의 문제는 어느 정도 해소할 수 있습니다.\n그러나 4~5대의 각 서버 사용자에게 연락을 드려 일리리 조사를 하는 것도 여간 번거로운 일이 아닙니다. 그래서 다른 방법을 생각해야 합니다.\n2. 클라이언트, 서버에 네트워크 규칙 추가의 문제점 네트워크 규칙 정보는 상태를 영구적으로 저장하지 않는다.\n다른 방법으로는 클라이언트와 서버에 라우팅을 설정하면 됩니다.\n클라이언트 192.168.50.27(서버 IP)를 통해 10.0.10.0/24에 해당되는 패킷을 전송하는 라우팅 룰 추가합니다. 서버 물리 인터페이스에서 패킷을 송신하여 10.0.10.0/24 수신을 허락합니다. 자세한 사항은 서버 구축기 - 4. Kolla-ansible 설치 시 마주한 네트워크 문제 을 참고하시길 바랍니다.\n문제는 iptable, route 룰 모두 영구적으로 상태를 저장하지 않습니다. 컴퓨터, 서버를 한번 끄면 route, iptable 규칙이 사라집니다. 물론 컴퓨터 부팅 후 규칙을 저장하는 init script를 제작하여 영구적으로 만들 수 있으며, iptable의 경우 도구를 활용하여 영구적으로 상태를 저장할 수 있습니다.\n저는 노트북을 통해 서버에 자주 접속합니다. 제 노트북에서도 내부적으로 가상 머신을 활용하곤 합니다. 이런 상황에서 2번처럼 영구적으로 저장하게 된다면 어떻게 될까요? 서버에 설정한 라우팅 규칙을 잊어 동일한 네트워크 대역의 가상머신에 접근을 요청하게 된다면? 결국 유연성이 떨어진다는 문제가 발생하게 됩니다.\nNIC 2개 삽입한 이유 위의 문제를 종합하면 또다른 서버를 공유하고 있는 환경에서 가상 네트워크 인터페이스에 직접 접근하기 위한 적절한 라우팅 설정이 어렵다는 것입니다. 결국 위의 문제를 해결하기 위해서는 OpenStack 관련 모든 네트워크 대역을 물리 네트워크 인터페이스로만 구성하면 됩니다. 그렇다면 추가적인 라우팅 규칙을 설정할 필요가 없습니다.\nNIC 스위치들은 모두 어디에 연결될까? 동일한 네트워크 대역을 가지기 위해 NIC는 모두 동일한 스위치에 연결됩니다. 즉 192.168.50.1/24 대역이라면 서버는 192.168.50.27로 할당받고 Floating IP로도 192.168.50.122 이런 식으로 할당받는 것이죠. Provider Network는 서버와 동일한 대역의 IP를 가지게 됩니다.\n이유 1. Resource API, Floating IP 직접 접근할 수 있다. 대부분의 인프라 리소스는 Terraform을 활용해 배포합니다. Kubernetes의 경우 Kubespray를 활용해서 배포하기 때문에 별다른 선택지는 없었고, 그 이외에는 관리가 편리하여 Terraform을 활용하게 되었습니다.\n만약에 NIC가 2개라면 위에서 설명한 라우팅 설정을 할 필요가 없어집니다. 왜냐하면 Resource API는 서버의 IP(192.168.50.27)이고, Provider Network는 동일한 대역인 192.168.50.x으로 접근할 수 있으니까요.\n이유 2. 포트 충돌을 해소할 수 있다. 이건 NIC 2개를 선택한 이전에 오픈 스택을 선택한 이유이기도 합니다.\nHexaCTF 플랫폼 자체는 2개의 워커 노드를 가진 쿠버네티스 내에서 작동됩니다. 문제를 해결하는 사람(이용자)가 접근해야 하는 사이트는 크게 두 가지 이며 배포된 장소가 다릅니다.\nHexaCTF 웹사이트 - Worker Node 1 Challenge 서버 - Worker Node 2 오픈 스택이 아닌 경우 = 단일 IP로만 구성된 경우 HexaCTF 관련 서비스들은 모두 NodePort를 통해 외부와 통신합니다. 웹사이트는 고정 NodePort 포트 번호를 지정해도 Challenge 서버는 NodePort 범위(30000~32767)내에 랜덤으로 포트 번호가 정해지기 때문에 포트 번호 충돌 가능성이 존재합니다. 예로 웹 사이트를 30000으로 열어도 Challenge 서버가 30000 포트 번호로 생성되면 충돌이 발생하게 되는 것이죠.\nChallenge는 사용자가 동적으로 생성되는 쿠버네티스 커스텀 리소스입니다. 사용자는 Challenge 요청하여 NodePort의 포트 번호를 얻으면 http 혹은 nc을 통해 시스템에 접속하게 됩니다.\n그럼 누군가는 이렇게 생각할 수 있을 것 같습니다.\nQ. Worker Node 1에 배포된 애플리케이션은 NodePort가 고정되어 있으니 다른 포트 번호로 포트 포워딩 시키면 되지 않을까요? 예로 쿠버네티스 워커 노드에서는 애플리케이션을 30010로 열지만 실제 서버에는 5010번으로 iptable 규칙을 수정하면 되잖아요?\nA. 그것도 좋은 방법이라고 생각합니다. 실제로 서버는 정전이 많이 발생되는 환경이기에 iptable 규칙이 영속적으로 유지될 수 있도록 설정해야 합니다. iptable 규칙이 포함된 별도의 스크립트를 작성하면 포트 충돌 문제는 해소할 수 있습니다.\n스크립트를 작성하여 부팅 시 적용시키는 방법이 있습니다. 그러나 이것저것 추가하고 삭제하면서 스크립트를 업데이트 하지 않아 포트 충돌 문제가 발생할 수 있습니다.\n관리를 잘하면 별다른 문제가 발생하지 않습니다. 그러나 유동적으로 추가되고 삭제되는 것들을 일리리 관리하는 것은 실수가 많이 발생한다고 생각하여 스크립트 작성은 좋은 선택지로 보고 있지 않습니다.\n오픈 스택을 활용할 경우 = 개별 IP 소유 개별적으로 IP를 가지게 되면 포트 충돌 문제를 완전히 해소할 수 있습니다. Worker Node 1, 2는 서로 다른 IP를 가지고 있기 때문에 포트 번호가 겹치더라도 서로 다른 서비스로 인식되니까요.\n이유 3. 역할 별 접근 제어가 가능하다. HexaCTF는 크게 3가지 사용자가 있습니다.\n시스템 운영자( or 문제 등록자) 개발자 사용자 오픈 스택이 아닌 경우 = 단일 IP로만 구성된 경우 단일 IP인 경우를 생각해 봅시다. 모든 애플리케이션을 NodePort로 열었습니다. 그리고 나서 라우터에 포트 포워딩을 수행합니다. 문제는 저희가 사용하고 있는 라우터는 포트 범위 기반으로 포트포워딩은 못하고 DMZ로 열어야 합니다. 모든 포트를 열어야 하는 상황인거죠.\nDMZ란 내부 네트워크에 존재하지만 외부에 접근할 수 있는 특수한 영역을 의미합니다. 주로 외부에 많이 노출되는 FTP, Web Server에 DMZ를 구성하며 불필요한 서비스 노출을 최소화하기 위해 주로 활용됩니다.\n만약에 단일 IP로 할경우 포트 포워딩을 할때 단일 IP에 전체의 포트를 외부에 오픈하게 됩니다. 이러한 환경에서 악의적인 사용자가 오픈한 포트를 스캔한 후 관리자 서비스에 접근할 수 있습니다. 이는역할을 명확하게 분리하지 않아 발생한 불필요한 노출 문제이며, 명백하게 보안 문제로 이어지게 됩니다.\nQ. 방화벽을 설치하면 되잖아요!\nA. 이럴 경우에는 하드웨어 방화벽이 필요합니다.. 돈이 없어요.. 🥹\n오픈 스택을 활용할 경우 = 개별 IP 소유 오픈스택을 활용하여 모든 리소스에 IP를 할당하면 위와 같은 문제를 해결할 수 있습니다. 우선 인터넷을 연결할 수 있는 서비스와 그렇지 않는 서비스를 IP를 통해 명확하게 나눌 수 있게 됩니다. 즉 서비스를 분류하는 기준이 포트 번호가 아닌 IP로 확장된 것이죠.\n사용자 Worker Node1, 2에 각각 사설 IP를 할당하여 단일 포트만 오픈해야 하는 서비스는 포트 포워딩을 통해 인터넷과 연결해주고, 범위로 지정해야 할 경우에는 DMZ로 인터넷과 연결해주면 됩니다.\n이런 식으로 구성하면 사용자는 Worker Node 1 내에 배포된 애플리케이션과 Worker Node 2에 배포된 Challenge 서비스만 접근할 수 있게 되어 불필요한 노출을 최소화하게 됩니다.\n운영자 혹은 개발자 하지만 운영자와 개발자도 쿠버네티스 내에 배포된 각종 도구 혹은 다른 서비스에 접근해야 합니다.\n그렇다면 어떻게 접근할까요? 바로 VPN을 활용하는 것입니다. 즉 별도의 인터넷 연결 없이 사설 IP에 접근할 수 있습니다.\n가상사설망(假想私設網) 또는 VPN(영어: virtual private network)은 공중 네트워크를 통해 한 회사나 몇몇 단체가 내용을 바깥 사람에게 드러내지 않고 통신할 목적으로 쓰이는 사설 통신망이다.\n출처: 위키백과 이런식으로 구성하면 개발자와 관리자는 안전하게 관련 서비스에 접근할 수 있으며, 그 이외의 사용자는 해당 서비스에 접근할 수 없게 됩니다.\n정리 NIC를 2개로 삽입한 이유, 오픈 스택을 활용한 이유 등 선택의 이유는 복합적입니다. 그래서 순차적으로 원인과 결과를 명확하게 말하기 어려운 것 같습니다.\n핵심은 아래와 같습니다.\n외부와 통신해야 하는 모든 시스템은 동일한 네트워크 대역에서 직접 접근할 수 있는 사설 IP를 가져야 하며, 필요에 따라 인터넷에 노출시킬 수 있다.\n마치며 이번 글은 여러가지 의미로 작성하기 어려웠습니다. 사설 IP인데 운영체제에서 만든 네트워크 인터페이스를 통해 할당받은 IP는 똑같이 사설 IP라고 설명해야 하나? 여러가지 고민이 많았던 것 같습니다.\n다음 글에서는 쿠버네티스로 애플리케이션을 어떻게 구성하고 배포했는지 설명하겠습니다.\n참고 OpenStack 환경에서의 OVS 네트워크 흐름 심층 분석 - 오픈소스컨설팅 테크블로그 VPN 쉽게 이해하기 What is a DMZ (Demilitarized Zone) Network? - zenarmor.com ","date":"2025-03-14T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-11/","title":"HexaCTF 11. 오픈 스택 서버 네트워크 개선기 - 서버에 NIC 2개 삽입한 이유"},{"content":" 2025-03-13 일부 내용 수정 이전까지 Challenge Operator 관련된 글만 작성했습니다. Challenge의 라이프 사이클을 전반적으로 설명해야 하는 만큼 많은 분량을 할애했습니다. 이번 Challenge Control API 부분은 이번 글로 모두 정리하고자 합니다.\n아키텍쳐 설계 아케텍쳐 전반적인 아키텍쳐는 아래와 같습니다.\nCTF-WEB 그림에 클라이언트 부분이 없다고 생각할 수 있는데 CTF-WEB의 경우 Flask Server로 이뤄져 있으나 html을 리다이렉트 하는 구조이기 때문에 하나로 표현했습니다.\nContainer Control API 본래 CTF-WEB 하나로 병합하여 사용할 예정이었습니다. 그러나 Control API가 처리해야 하는 다른 플랫폼(Queue, Kubernetes)가 많다는 점과 비교적 에러 사항이 많다는 점을 감안하여 다른 서비스 형태로 분리하기로 했습니다. 즉, 불필요한 오류 확산을 막기 위해 분리했다고 보시면 됩니다.\n왜 Control API는 필요한가? 우선 사용자와 상호작용을 해야 하기 때문에 명령어를 받는 서비스가 필요합니다. 클라이언트는 kubectl apply 에 해당되는 명령어를 쿠버네티스 서버에 전송해야 합니다. 클라이언트에서 쿠버네티스 명령어를 실행시킨다는 것은 쿠버네티스 마스터 노드에 대한 엔드 포인트가 그대로 노출된다는 것과 같습니다. 즉, 치명적인 보안 문제로 이뤄지지 않을까요?\n쿠버네티스는 .kube 라는 config 파일을 통해 쿠버네티스 클러스터를 관리하게 됩니다. 여기에 쿠버네티스 클러스터의 엔드포인트를 확인할 수 있습니다.\nControl API-Operator 사이에 큐를 둔 이유 이번 글에서는 지속해서 Control API 입장에서 작성할 것입니다.\nControl API 입장에서는 어떠한 절차를 통해 Challenge의 상태 변화를 얻어서 데이터베이스에 저장해야 합니다. Control API가 할 수 있는 방법은 크게 두 가지입니다.\nControl API가 지속해서 쿠버네티스에게 Challenge 상태 여부를 물어본다. 쿠버네티스가 상태가 변할때마다 Control API에게 물어본다. 결국 누가 먼저 물어볼 것인가를 선정해야 합니다. 선정 기준이 프로젝트마다 다르겠으나 저는 구현 난이도를 고려했을때 2번인 \u0026ldquo;쿠버네티스가 상태가 변할때마다 Control API에게 물어본다.\u0026ldquo;를 선택했습니다.\n이전 편에서도 제가 쿠버네티스를 선택한 이유는 컨테이너를 비동기적으로 탐색하여 실시간으로 상태 정보를 가져오기 어려움을 겪었기 때문입니다. 이와 유사한 문제의 늪에 빠지게 됩니다. 클라이언트는 Challenge가 무슨 상태인지 모르기 때문에 지속적으로 쿠버네티스에게 물어볼 것이고, 동기적으로 처리하면 속도가 느려질 것이니 비동기적으로 처리해야 할 것입니다.\nOperator는 내부적으로 Challenge의 상태 정보를 관리합니다. 그러므로 Challenge의 상태가 변경될때마다 Control API에게 보내는 방법은 어떨까요? 이러한 방식은 하나의 문제점을 가지게 됩니다. 동기로 처리를 하면 응답값이 올때까지 기다려야 하기 때문에 Operator의 속도가 느려질 가능성이 있습니다. 또한 Opeator 내부에는 비동기로 처리되기 때문에 무슨 문제가 발생할지 예상이 가지 않습니다.\n그러나 해결 방법은 있습니다. Operator와 Control API 사이에 큐를 둠으로써 데이터 누락 문제도 해결하고 속도도 향상시킬 수 있습니다. 예로 Operator가 None -\u0026gt; Running 상태로 변경시켰을때 Queue에 Running으로 변경했다는 메세지만 전송한 후 Operator의 나머지 로직을 처리하게 됩니다. 즉, 응답이 올때까지 기다릴 필요가 없겠죠?\nChallenge Controller의 스터디 케이스: Nova Architecture Challenge Controller 시스템의 전반적인 컨셉은 OpenStack의 Nova의 아키텍쳐를 많이 활용했습니다.\nNova는 AWS의 EC2라고 생각하시면 됩니다.\n저희 프로젝트는 쿠버네티스 리소스를 활용한다는 점에서 Nova와 확실히 다르겠으나 API를 통해 컴퓨팅 리소스를 제어한다는 컨셉은 동일하니까요.\n제가 활용한 컨셉은 아래와 같습니다.\n제어부(api)와 리소스 처리부(nova-compute, nova-volume \u0026hellip;) 분리 제어부와 리소스 처리부 사이 이벤트 기반 처리 - Queue 배치 저희 프로젝트에서는 Control API가 nova-api에 해당되는 것이고, Operator가 nova-compute라고 생각하시면 됩니다.\n\u0026ldquo;제어부와 리소스 처리부 분리\u0026rdquo; 관련 간단한 이야기\n무조건 Nova Architecture를 차용했다고 해서 Operator를 별도로 제작한 것은 아닙니다. 그저 분리하여 이벤트 처리 기반으로 시스템을 제어하는 것이 많이 활용되는 방식이라고만 인식했습니다. 즉, Operator를 별도로 제작하는 것이 기술적인 고난도 도전이 아닌 선택지 중 일부로 받아들인 것입니다.\nControl API 제작 자세한 사항은 HexaCTF 6. Kubebuilder를 활용한 ChallengeDefinition\u0026amp;Challenge Type 구현를 참고하시길 바랍니다.\n커스텀 리소스인 ChallengeDefinition과 Challenge를 확인해볼까요?\nChallengeDefinition : 문제 상세 정보를 나타내며 챌린지 정보, 구성 리소스를 컴포넌트 단위로 정의한다. Challenge : 사용자(문제 풀이자)가 생성하는 문제의 단위로 참조된 ChallengeDefinition을 기반으로 리소스를 생성하고 삭제한다. 사용자가 생성하는 문제를 Challenge 단위로 정의함으로써 사용자 요청에 대한 형식이 간소화되고, 사용자 명령을 제어한다는 역할에 충실할 수 있게 됩니다.\n사용자는 요청 시 사용자 정보와 문제를 전송하게 됩니다. 이러한 고유한 정보를 조합하여 Challenge 이름을 생성(challenge-{challenge_id}-{username})했으며 쿠버네티스 python sdk를 통해 쉽게 추가하고 삭제할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 user_challenge_repo = UserChallengesRepository() # Challenge definition 조회 challenge_definition = ChallengeRepository.get_challenge_name(challenge_id) if not challenge_definition: raise ChallengeNotFound(error_msg=f\u0026#34;Challenge definition not found for ID: {challenge_id}\u0026#34;) # Challenge name 생성 및 검증 challenge_name = f\u0026#34;challenge-{challenge_id}-{username}\u0026#34; if not self._is_valid_k8s_name(challenge_name): raise UserChallengeCreationError(error_msg=f\u0026#34;Invalid challenge name: {challenge_name}\u0026#34;) # ... # Challenge manifest 생성 challenge_manifest = { \u0026#34;apiVersion\u0026#34;: \u0026#34;apps.hexactf.io/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Challenge\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: challenge_name, \u0026#34;labels\u0026#34;: { \u0026#34;apps.hexactf.io/challengeId\u0026#34;: str(challenge_id), \u0026#34;apps.hexactf.io/user\u0026#34;: username } }, \u0026#34;spec\u0026#34;: { \u0026#34;namespace\u0026#34;: namespace, \u0026#34;definition\u0026#34;: challenge_definition } } challenge = self.custom_api.create_namespaced_custom_object( group=\u0026#34;apps.hexactf.io\u0026#34;, version=\u0026#34;v1alpha1\u0026#34;, namespace=namespace, plural=\u0026#34;challenges\u0026#34;, body=challenge_manifest ) 포트 번호는 Challenge 상태 정보에 있다. Challenge API는 생성을 요청하면 커스텀 리소스인 Challenge의 포트 번호를 가져와야 합니다. 이 과정을 어떻게 해결했는지 설명하겠습니다.\nOperator 부분으로 돌아와 Challenge는 Deployment, Service로 구성되어 있으며 Service는 NodePort를 통해 외부로 노출합니다. 그렇다면 Control API가 요구하는 포트 번호는 Service에서 노출한 NodePort일 것입니다.\nChallenge는 외부에 노출되는 엔드 포인트는 단 하나 뿐입니다.\n여기서 Control API는 쿠버네티스를 Service를 조회하도록 구현하는 것이 좋을까요? 아니면 Challenge에 정보가 포함되는 것이 좋을까요? 당연히 Challenge 정보에 포트 정보가 포함되는 것이 Control API의 역할에 충실하게 구현할 수 있는 방법일 것입니다.\nOperator에서는 Service가 생성이 완료되면 Challenge 상태값에 저장할 수 있도록 구현했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 // Service가 존재하는지 확인 err := r.Get(ctx, types.NamespacedName{ Name: identifier.GetServicePrefix(), Namespace: challenge.Namespace, }, service) if err != nil { if errors.IsNotFound(err) { log.Info(\u0026#34;Creating new service\u0026#34;, \u0026#34;name\u0026#34;, identifier.GetServicePrefix(), \u0026#34;namespace\u0026#34;, challenge.Namespace) // ... if err := r.Create(ctx, service); err != nil { log.Error(err, \u0026#34;Failed to create Service\u0026#34;) return err } log.Info(\u0026#34;Successfully created service\u0026#34;, \u0026#34;name\u0026#34;, identifier.GetServicePrefix()) if service.Spec.Type == corev1.ServiceTypeNodePort { challenge.Status.Endpoint = int(service.Spec.Ports[0].NodePort) log.Info(\u0026#34;NodePort created\u0026#34;, \u0026#34;port\u0026#34;, service.Spec.Ports[0].NodePort) } if err := r.Status().Update(ctx, challenge); err != nil { log.Error(err, \u0026#34;Failed to update Challenge status with NodePort information\u0026#34;) return err } return nil } log.Error(err, \u0026#34;Failed to get Service\u0026#34;) return err } Control API에서는 Challenge 생성 요청을 하면 Challenge의 상태값에서 포트 번호를 가져오기만 하면 됩니다.\n1 2 3 4 5 6 7 challenge = self.custom_api.create_namespaced_custom_object( group=\u0026#34;apps.hexactf.io\u0026#34;, version=\u0026#34;v1alpha1\u0026#34;, namespace=namespace, plural=\u0026#34;challenges\u0026#34;, body=challenge_manifest ) \u0026ldquo;언제\u0026rdquo; 컨테이너 빌드가 끝날 것인가? = \u0026ldquo;언제\u0026rdquo; 포트 번호가 설정될 것인가? Control API가 Service를 조회를 하든, Operator가 포트 번호를 상태값으로 저장을 하든 똑같은 문제점이 있습니다. 바로 포트 번호가 생성되는 시점이 컨테이너 빌드 시간에 영향을 크게 받는다는 의미입니다. 포트번호 생성 시간이 컨테이너 빌드 시간과 같다고 봐도 무방합니다.\n프로젝트에 대해 간단하게 설명하자면 해킹 문제는 출제자, 문제 유형에 따라 다른 플랫폼을 활용합니다.\n시스템 문제는 대부분 리눅스 기반 베이스 이미지를 활용합니다. 어떤 리눅스 이미지를 사용하더라도 빌드 시간이 비슷하다는 특징을 가지고 있습니다.\n그러나 웹은 상황이 다릅니다. 출제자에 따라서 SpringBoot, Flask 등 다양한 빌드 시간을 가진 플랫폼을 사용하기도 합니다.\n위의 내용을 하나로 정리하자면 \u0026ldquo;문제에 따라서 빌드 시간이 다르다.\u0026ldquo;는 의미이며 이는 \u0026ldquo;문제마다 포트번호를 반환하는 시간이 다르다\u0026quot;라고 해석하셔도 됩니다.\n해결 방안 - 5초 sleep Control API에서 Challenge 요청 이후에 5초 sleep하는 방식을 채택했습니다.\n채택 이유는 구현하기 쉬웠고, 5초라는 기간은 대부분의 문제의 포트번호를 반환하기 충분했기 때문입니다. 다만 부족할 수 있으므로 재시도 로직을 넣었습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 challenge = self.custom_api.create_namespaced_custom_object( group=\u0026#34;apps.hexactf.io\u0026#34;, version=\u0026#34;v1alpha1\u0026#34;, namespace=namespace, plural=\u0026#34;challenges\u0026#34;, body=challenge_manifest ) time.sleep(5) # status 값 가져오기 status = challenge.get(\u0026#39;status\u0026#39;, {}) endpoint = status.get(\u0026#39;endpoint\u0026#39;) # status가 아직 설정되지 않았을 수 있으므로, 필요한 경우 다시 조회 if not status: time.sleep(3) challenge = self.custom_api.get_namespaced_custom_object( group=\u0026#34;apps.hexactf.io\u0026#34;, version=\u0026#34;v1alpha1\u0026#34;, namespace=namespace, plural=\u0026#34;challenges\u0026#34;, name=challenge[\u0026#39;metadata\u0026#39;][\u0026#39;name\u0026#39;] ) status = challenge.get(\u0026#39;status\u0026#39;, {}) endpoint = status.get(\u0026#39;endpoint\u0026#39;) 위의 해결 방안은 최선이 아니다. 5초라는 기간은 대부분의 문제의 포트번호를 반환하기 충분했기 때문입니다.\n위의 대부분이라는 의미는 5초 + 재시도를 하더라도 포트번호가 반환되지 않은 문제가 있을 수 있다는 의미입니다. 실제 저희 프로젝트에서 9개의 문제 중 단 한 해킹 문제가 빌드 시간이 sleep+재시도 기간보다 길어서 포트번호가 바로 반환되지 않는 문제가 발생하기도 했습니다.\n빌드 시간이 긴 컨테이너는 대부분 스프링 부트입니다. ^ㅇ^\n결국 포트번호 제때 얻기 위해서는 실시간 처리가 필요하게 됩니다.\n다만 이 문제는 CTF-WEB 팀과 상의해야 하는 부분이 많기 때문에 보류하기로 했습니다.\n마치며 Control API는 단순히 클라이언트 명령어만 제어하기 위해 제작되었습니다. 그러나 실시간으로 사용자에게 포트번호를 전송하지 못한 점이 아쉽기도 했습니다. 이는 협의 후 수정될 예정이며, 문제가 해결되면 다른 글로 찾아 뵙겠습니다.\n다음 글은\u0026hellip; 다음 글은 전체적인 인프라를 어떻게 구성했으며, 쿠버네티스에 어떻게 애플리케이션을 배치했는지 설명하겠습니다.\n","date":"2025-03-12T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-10/","title":"HexaCTF 10. 아키텍쳐 설계 및 Challenge Control API 제작"},{"content":"Prometheus 왜 promethues Operator를 사용했을까? Prometheus는 다양한 대상(target)에서 시간 경과에 따른 지표를 수집하여 저장하는 시계열 데이터베이스입니다. 이러한 데이터를 우리 눈에 쉽게 보여주는 것이 Grafana입니다.\n프로메테우스 실행 절차는 wlsdn3004님의 Prometheus 란?를 참고하시길 바랍니다.\n여기서 단순한 쿠버네티스가 아닌 쿠버네티스 오퍼레이터를 활용한 이유는 서비스 모니터를 통해 멀티 클러스터 혹은 다른 네임스페이스 환경에 대응할 수 있기 때문입니다.\n여기서 서비스 모니터가 무엇인지 알아야겠죠? 서비스 모니터(ServiceMonitor)는 동적으로 다수 서비스를 모니터링할 수 있는 방법을 선언적으로 정의한 것입니다. 원하는 구성으로 ServiceMonitor를 적용시키면 Prometheus Operator는 자동으로 새로운 서비스를 찾게 됩니다.\n제가 Prometheus Operator를 쓰는 이유는 다른 Namespace에 있는 Challenge Operator의 메트릭 정보를 얻기 위해서입니다. 구체적으로 어떻게 설정할지는 아래에서 설명하겠습니다.\nChallenge 상태를 나타내는 커스텀 메트릭 제작하기 Challenge 상태 정보 메트릭의 특징 빠른 문제 해결 현재 저희 대회에서 필요한 것은 Challenge 성능 정보보다 상태 정보입니다. 왜냐하면 문제 발생 시 빠르게 대처해야 하기 때문입니다.\n여기서 Challenge 내에 있는 Deployment에 대해 이해해야 합니다. 해킹 문제의 경우 취약한 하나의 기능을 구현하여 배포하게 됩니다. 즉, 모든 프로그램 자체가 단순합니다. 또한 한 사람당 하나의 Challenge만 사용하기 때문에 트래픽 문제가 발생할 확률은 높습니다.\n트래픽 항상 낮게 측정되지 않습니다. 간혹 Brute Force 특징을 가진 해킹 도구를 활용하는 경우가 있습니다. 그러나 이러한 행동은 부정 행위로 간주됩니다.\n여기서 말하는 문제는 서버 생성 실패, 서버 삭제 실패 등 오퍼레이터에 발생하는 문제를 말하는 것입니다. 빈번하는 아니지만 드물게 삭제가 실패되는 경우가 있어서 Challenge 상태 정보를 중심으로 시각화를 했습니다. 만약에 삭제가 실패되었는데 실행 중이라면 남아 있는 리소스를 삭제하는 것이 우선이겠죠?\n좀 더 자세하게 구현하기 위해서는 상태 메트릭뿐만 아니라 로깅과 다른 메트릭을 조합해서 구현해야 할 것 입니다. 그러나 시간 관계 상 구현하지 못했으며 추후 구현할 예정입니다.\nChallenge 사용 패턴 확인 제가 상태 정보를 우선 제작한 이유이기도 합니다.\n대회 이전에는 2가지 궁금증을 가지고 있었습니다. 아래의 질문들은 서버 메모리 용량을 산정할때 필요합니다.\n사람들은 한번에 몇개의 서버를 만들고 방치할까? -\u0026gt; 한 사람당 최대 몇개의 서버 문제 분야마다 푸는 패턴이 있을까? -\u0026gt; Challenge 리소스 양과 분야 별 비율 사용 패턴은 현재보다는 미래를 위한 메트릭 지표라고 보면 됩니다.\n여담 글 작성 시점으로 대회를 성공적으로 마무리했습니다. 사용자 패턴에 대해서는 다른 글에서 뵙겠습니다.\n어떤 메트릭을 활용할까 Prometheus 공식 문서에 의하면 메트릭은 4개가 있습니다.\nGuage: 임의로 올라가거나 내려갈 수 있는 단일 숫자값을 나타내느 메트릭 Counter : 누적되는 메트릭 값. 감소가 없음 Histogram :지연 시간, 처리시간과 같이 연속형 값을 다룰때 사용 Summary : 연속적인 데이터 값의 분포를 측정 선택 기준은? 저는 최종적으로 Gauge를 선택하게 되었습니다.\nn시간 마다 수집되어야 함 Running, Deleted, Error 개수의 증감을 표현할 수 있어야 함 1 2 3 4 5 6 7 8 9 10 11 12 13 var ( crStatusMetric = prometheus.NewGaugeVec( prometheus.GaugeOpts{ Name: \u0026#34;challenge_resource_status\u0026#34;, Help: \u0026#34;Tracks the status of the custom resource\u0026#34;, }, []string{\u0026#34;challeng_id\u0026#34;, \u0026#34;challenge_name\u0026#34;, \u0026#34;username\u0026#34;, \u0026#34;namespace\u0026#34;}, ) ) func init() { metrics.Registry.MustRegister(crStatusMetric) } Label은 어떻게 설정할까? Prometheus의 Label은 키-값 쌍으로 이뤄져 있으며, Proemtheus가 시계열 데이터를 식별하는데 메트릭 이름과 더불어서 사용합니다.\n제가 원하는 상태 정보는 크게 세 가지 입니다. 제가 위해서 말한 2가지 목적에 대입하면 아래와 같습니다.\n상태별 Challenge 개수 전체 Challenge 현황을 파악하기 위해 사용자, 문제 별 Challenge 상태 사용자 패턴 파악 Error 문제 발생 시 빠르게 문제를 대처하기 위해 문제별 실행 중/에러 커스텀 리소스 수 사용자 패턴 파악 오류 사항이 있는 Challenge를 찾기 위해 그렇다면 Label로 무조건 지정할 것은 사용자 식별자, 문제 식별자입니다. 이 두가지를 포함하여 Challenge 이름과 Namespace도 추가했습니다.\n본래 종류에 따라 Namespace를 분리할 예정이었습니다. 그러나 시간 관계 상 못하게 되어 코드에 남게 되었습니다.\n그리고 상태 정보를 값으로 설정했습니다. Gauge의 특징 중에 하나는 값을 증감할 수 있다고 했죠? 이 기능을 활용할 예정입니다.\nRunning: 1 Deleted: 2 Error: 3 실제 코드로 작성하면 아래와 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 var ( crStatusMetric = prometheus.NewGaugeVec( prometheus.GaugeOpts{ Name: \u0026#34;challenge_resource_status\u0026#34;, Help: \u0026#34;Tracks the status of the custom resource\u0026#34;, }, []string{\u0026#34;challeng_id\u0026#34;, \u0026#34;challenge_name\u0026#34;, \u0026#34;username\u0026#34;, \u0026#34;namespace\u0026#34;}, ) ) func init() { metrics.Registry.MustRegister(crStatusMetric) } // ... crStatusMetric.WithLabelValues(challenge.Labels[\u0026#34;apps.hexactf.io/challengeId\u0026#34;], challenge.Name, challenge.Labels[\u0026#34;apps.hexactf.io/user\u0026#34;], challenge.Namespace).Set(1) 추가: Deleted 상태 정보는 언제까지 가지고 있어야 할까? 메트릭 구현 부분에서 \u0026ldquo;Deleted 상태 정보는 언제까지 가지고 있어야 할까?\u0026ldquo;가 난제이지 않을까 싶습니다.\nPromethues는 기본적으로 30초 간격으로 /metrics 내용을 스크랩합니다.\n저에게는 두 가지 고민이 있었습니다.\n삭제 요청 시 메트릭을 Deleted(3)로 설정 -\u0026gt; /metrics에 불필요한 데이터가 쌓임. 대회가 끝날때마다 주기적으로 삭제하는 작업이 필요 삭제 요청 시 메트릭을 Deleted 상태로 변경 후 즉시 삭제 -\u0026gt; Prometheus가 Deleted 상태를 스크랩할 수 있도록 시간 조정이 필요 대회 시기에는 Running, Error를 중점적으로 볼 예정이기 때문에 두 번째 방법을 선택하게 되었습니다.\n다만 Prometheus가 Deleted 상태를 한번 스크랩하고 삭제되어야 하기 때문에 고루틴을 활용하여 1분 후에 메트릭이 삭제될 수 있도록 구현했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (r *ChallengeReconciler) handleDeletion(ctx context.Context, challenge *hexactfproj.Challenge) (ctrl.Result, error) { log.Info(\u0026#34;Processing deletion\u0026#34;, \u0026#34;challenge\u0026#34;, challenge.Name) crStatusMetric.WithLabelValues(challenge.Labels[\u0026#34;apps.hexactf.io/challengeId\u0026#34;], challenge.Name, challenge.Labels[\u0026#34;apps.hexactf.io/user\u0026#34;], challenge.Namespace).Set(2) // ... go func() { time.Sleep(1 * time.Minute) // scrape_interval이 30초라면 1분 정도 기다리면 안전 crStatusMetric.DeleteLabelValues(challenge.Labels[\u0026#34;apps.hexactf.io/challengeId\u0026#34;], challenge.Name, challenge.Labels[\u0026#34;apps.hexactf.io/user\u0026#34;], challenge.Namespace) }() log.Info(\u0026#34;Successfully completed deletion process\u0026#34;) // 이 시점에서 finalizers가 비어 있으므로, K8s가 오브젝트를 실제 삭제함 return ctrl.Result{}, nil } Challenge Operator 쿠버네티스에 배포하기 metrics endpoint 설정 kubebuilder는 CNCF 프로젝트인만큼 프로메테우스 엔드포인트를 개방할 수 있는 ServiceMonitor를 자동으로 설정할 수 있습니다. 단, kubebuilder에 내장되어 있는 kustomize를 활용해야 간단한 설정으로 사용할 수 있게 됩니다. 그러나 저는 kustomize를 활용하지 않을 예정이므로 다른 방법을 찾아봐야 합니다.\nkustomize는 배포 도구일뿐 근본적으로 kubebuilder에는 메트릭을 export 해줄 수 있는 함수가 있다고 생각해도 될 것 같습니다.\nmain.go 에 가보시면 metricServer의 속성을 프로그램 속성 값으로 받아서 설정하고 있습니다. 즉 프로그램 실행시킬때 적절한 속성값을 넣어주면 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 var metricsAddr string var enableLeaderElection bool var probeAddr string var secureMetrics bool var enableHTTP2 bool var tlsOpts []func(*tls.Config) flag.StringVar(\u0026amp;metricsAddr, \u0026#34;metrics-bind-address\u0026#34;, \u0026#34;:8080\u0026#34;, \u0026#34;The address the metrics endpoint binds to. \u0026#34;+ \u0026#34;Use :8443 for HTTPS or :8080 for HTTP, or leave as 0 to disable the metrics service.\u0026#34;) flag.StringVar(\u0026amp;probeAddr, \u0026#34;health-probe-bind-address\u0026#34;, \u0026#34;:8081\u0026#34;, \u0026#34;The address the probe endpoint binds to.\u0026#34;) flag.BoolVar(\u0026amp;enableLeaderElection, \u0026#34;leader-elect\u0026#34;, false, \u0026#34;Enable leader election for controller manager. \u0026#34;+ \u0026#34;Enabling this will ensure there is only one active controller manager.\u0026#34;) // Prometheus metrics http 사용 flag.BoolVar(\u0026amp;secureMetrics, \u0026#34;metrics-secure\u0026#34;, false, \u0026#34;If set, the metrics endpoint is served securely via HTTPS. Use --metrics-secure=false to use HTTP instead.\u0026#34;) flag.BoolVar(\u0026amp;enableHTTP2, \u0026#34;enable-http2\u0026#34;, false, \u0026#34;If set, HTTP/2 will be enabled for the metrics and webhook servers\u0026#34;) opts := zap.Options{ Development: true, } opts.BindFlags(flag.CommandLine) flag.Parse() // ... metricsServerOptions := metricsserver.Options{ BindAddress: metricsAddr, SecureServing: false, } Helm 으로 배포하기 위의 글에서 Kustomize를 활용하지 않을 것이라고 말씀 드렸습니다. 이유는 단순하게 Helm이 가장 익숙하기 때문입니다.\n그래도 차이점은 확인해봐야겠죠? SEOWOO님의 블로그 - Helm과 Kustomize, 무엇을 쓸까? 일부 참고해서 핵심적인 차이점을 설명하겠습니다.\nHelm 여러 템플릿의 모음으로 구성할 수 있으며 values.yaml을 매개변수화하여 값을 전달 애플리케이션 전체 패키지 관리 및 배포에 더 중심 Kustomize 상속의 개념을 활용해서 필요한 부분만 작성할 수 있음 환경별 배포 구성 관리에 적합 Challenge Operator를 배포할때 Deployment를 활용했습니다. Operator의 철학에서도 Operator는 단일 Deployment로 배포되어야 한다고 설명했습니다.\nOperators should install as a single deployment e.g. kubectl create -f https://coreos.com/operators/etcd/latest/deployment.yaml and take no additional action once installed. 출처: Introducing Operators: Putting Operational Knowledge into Software 여기서 주목해야 할 부분은 --metrics-bind-address=:8080 부분입니다. 앞서 말했듯이 kubebuilder는 내부적으로 메트릭 서버를 만들었습니다. 이를 외부에 접근할 수 있도록 :8080(http) 를 설정해야 합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Release.Name }}-operator annotations: \u0026#34;helm.sh/hook-weight\u0026#34;: \u0026#34;0\u0026#34; spec: replicas: 1 selector: matchLabels: app: challenge-operator template: metadata: labels: app: challenge-operator spec: nodeSelector: hexactf/env: mgmt serviceAccountName: {{ .Release.Name }}-operator containers: - name: operator image: {{ .Values.operator.image.repository }}:{{ .Values.operator.image.tag }} imagePullPolicy: {{ .Values.operator.image.pullPolicy }} ports: - containerPort: 8080 # Add the args section here args: # Use --metrics-bind-address and substitute the environment variable METRICS_ADDR - \u0026#34;--metrics-bind-address=:8080\u0026#34; ServiceMonitor 설정하기 Prometheus ServiceMonitor는 차례로 해당 엔드포인트를 검색하고 파드를 모니터링하도록 프로메테우스를 구성합니다. 어떤 엔드포인트의 어떤 포트를 통해 메트릭을 스크래핑하고 어떤 매개변수를 활용할지 구성하게 됩니다. 쉽게 말해 검색 대상을 Prometheus Operator가 일괄적으로 처리할 수 있도록 ServiceMonitor라는 리소스에 정의한 것입니다.\nOperator에 메트릭 서버의 포트를 열었다면 Helm에서 Port를 열어주고, Port를 연결해준 ServiceMonitor가 필요하겠죠?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Service metadata: name: {{ .Release.Name }}-operator-metrics namespace: hexactf labels: app.kubernetes.io/name: challenge-operator app.kubernetes.io/instance: challenge-operator app.kubernetes.io/component: metrics spec: selector: app: challenge-operator ports: - name: operator-metrics protocol: TCP port: 8080 targetPort: 8080 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: {{ .Release.Name }}-operator-monitor namespace: monitoring # Prometheus Operator가 설치된 ns labels: release: prometheus # Prometheus Operator의 label select spec: selector: matchLabels: # Challenge Operator service matchlabel app.kubernetes.io/name: challenge-operator app.kubernetes.io/component: metrics namespaceSelector: # 스크랩할 ns matchNames: - hexactf # Challenge Operator가 배포되는 ns - default endpoints: - port: operator-metrics interval: 30s scrapeTimeout: 10s path: /metrics scheme: http 프로메테우스를 확인하면 아래와 같이 서비스를 확인할 수 있습니다.\n시각화 저희가 시각화 할 목록은 크게 3가지입니다.\n상태별 Challenge 개수 사용자, 문제 별 Challenge 상태 문제별 실행 중/에러 커스텀 리소스 수 저는 그 중 상태별 Challenge 개수와 사용자, 문제 별 Challenge 상태를 보여드리겠습니다.\n자세한 사항은 Grafana 공식 문서를 참고하시길 바랍니다. 참고로 저는 하나하나씩 다 해봤습니다. 🥹\n상태별 Challenge 개수 2시간 동안 가장 마지막 값이 1(Running)인 경우를 count 해주는 명령어를 사용했습니다.\n2시간은 임의의 숫자로 실제 대회에서는 6시간을 기준으로 집계했습니다.\n하나의 패널에 2개 이상 메트릭을 설정하게 되면 Value에 알파벳이 붙습니다. 그러므로 맨 아래 Override를 활용하여 임의로 정해진 값을 수정합니다. 더 나은 시각화를 위해 Color scheme을 통해 색도 설정했습니다.\n그리고 설정한 색을 Background에 보일 수 있도록 Stat styles에 설정했습니다.\n아래와 같이 시각화를 할 수 있습니다.\n사용자, 문제 별 Challenge 상태 위와 동일하게 최근 상태 정보만 가져오면 되기 때문에 집계 시간 중 가장 최신의 데이터를 가져옵니다.\n1 last_over_time(challenge_resource_status[2h]) Gauge는 숫자로 값을 설정합니다. 그러므로 Value mappings를 활용하여 문자로 변환합니다.\nPromQL은 쉬운 대신 Transform을 설정해야 합니다.\nOragnaize field by name : challenge_id, username, status 값을 제외하고 전부 제거합니다. Group by: Challenge_id, username을 그룹핑하여 중복을 없애고 Status는 가장 최신 값을 가져옵니다. Grouping to Matrix: Grouping한 값을 가지고 새로운 Matrix를 활용합니다. 테이블 형태로 시각화 하면 아래와 같은 결과를 얻을 수 있습니다.\n마치며 Challenge Operator와 관련된 글을 모두 작성했습니다.\n다음 글에서는 실질적으로 클라이언트가 요청을 보내는 Challenge Control API에 대해 설명하겠습니다.\nReferences Prometheus 란? Metric types | Prometheus Helm과 Kustomize, 무엇을 쓸까? Transform data | Grafana documentation 정현석, 진미란 . (2023). 모니터링의 새로운 미래 관측 가능성. 제이펍. ","date":"2025-03-07T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-9/","title":"HexaCTF 9. Challenge Operator 커스텀 메트릭 설정 및 Prometheus Operator와 연결하기"},{"content":" 2025-03-07 - 내용 수정 이전 글에는 상태를 어떻게 정의했는지 설명했습니다. 지금부터 어떻게 Challenge를 생성, 삭제할 수 있는지 설명하겠습니다.\n조건 생각하기 우선 생성과 삭제 로직을 설명하기 이전에 생성과 삭제 시 무조건 수행되어야 하는 조건을 정리해야 합니다.\n생성 Deployment, Service 생성 삭제 조건에 따라 30분 후 삭제 -\u0026gt; 생성일 기준으로 종료일 지정 Challenge 삭제 시 관련 Deployment, Service 삭제 -\u0026gt; 무언가를 통해 연결되어야 함 주로 삭제 로직을 중심으로 컨트롤러를 구현했습니다. 삭제를 제대로 제어하지 않으면 고아 리소스가 생기는 문제가 발생하기 때문입니다.\nBuilder를 통해 Controller를 설정한다. 공식 문서에 의하면 Builder는 controller-runtime을 감싸며, 일반적인 controller를 구축하기 위한 패턴을 제공한다고 합니다.\n이전 글에서 제가 했던 이야기 생각 나시나요? 오퍼레이터를 만든다는 것은 대상 리소스를 제어하는 controller와 Controller를 관리하는 Mananger를 만든다는 것이고, Kubebuilder는 controller를 빌드하는 프로그램이라고 말했습니다.\nManager controller를 제작하는데 필요하며 client, caches, schemes등 의 공유 디펜던시를 제공한다. 컨트롤러는 Manager.Start를 호출하여 시작되어야 한다. 공식 문서 실제 코드 상에서는 kubebuilder가 Manager라는 객체 생성해줍니다.(cmd/main.go)\n\bManager는 고가용성을 위해 Controller들의 leader election도 수행합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{ Scheme: scheme, Metrics: metricsServerOptions, WebhookServer: webhookServer, HealthProbeBindAddress: probeAddr, LeaderElection: enableLeaderElection, LeaderElectionID: \u0026#34;958dbbf6.hexactf.io\u0026#34;, // LeaderElectionReleaseOnCancel defines if the leader should step down voluntarily // when the Manager ends. This requires the binary to immediately end when the // Manager is stopped, otherwise, this setting is unsafe. Setting this significantly // speeds up voluntary leader transitions as the new leader don\u0026#39;t have to wait // LeaseDuration time first. // // In the default scaffold provided, the program ends immediately after // the manager stops, so would be fine to enable this option. However, // if you are doing or is intended to do any operation such as perform cleanups // after the manager stops then its usage might be unsafe. // LeaderElectionReleaseOnCancel: true, }) if err != nil { setupLog.Error(err, \u0026#34;unable to start manager\u0026#34;) os.Exit(1) } 요구사항에 따라서 하나의 프로젝트에 2개 이상의 controller를 생성할 수 있습니다. 현재 저희 대회에서는 Challenge Controller 하나만 필요하기 때문에 하나만 만든 것 뿐입니다. Manager는 Controller를 제어하는 역할을 가진다고 설명했죠? 처음 Mananger가 여러개의 controller를 제어하기 위해서는 Manager에 Controller 등록 작업이 필요합니다. 그리고 이 부분은 라이브러리가 controller 파일 생성 시 자동으로 제공합니다.\n다시 돌아와서 Builder는 Challenge Controller를 생성하기 위해 어떤 정보가 필요할까요?\nController를 제어할 Manager Controller 대상 리소스 Controller에 삽입할 Reconcile 함수 kubebuilder를 통해 자동으로 controller를 생성하면 아래와 같이 빌드 로직을 확인할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 // SetupWithManager sets up the controller with the Manager. func (r *ChallengeReconciler) SetupWithManager(mgr ctrl.Manager) error { // Builder 초기화 return ctrl.NewControllerManagedBy(mgr) // Challenge라는 대상 리소스 For(\u0026amp;hexactfproj.Challenge{}). Owns(\u0026amp;appsv1.Deployment{}). Owns(\u0026amp;corev1.Service{})./ Named(\u0026#34;challenge\u0026#34;). // Controller에 ChallengeReconciler를 삽입 Complete(r) } Controller는 Reconciler를 통해 Reconcile()를 실행한다. Controller는 타겟 리소스(Challenge)를 모니터링하고 변경 시 Reconciler 함수를 실행시킵니다. Reconciler 함수는 kube-apiserver를 통해 대상 리소스의 변경 사항을 확인합니다. 만약에 변경 사항이 감지가 되었다면 Reconcile 함수를 실행시킵니다.\n맨 첫부분에 말한 조건은 모두 Reconcile 함수 내에 구현하게 됩니다.\n구체적인 내용은 Kubernetes Operator series 6 — controller-runtime component — Controller | by Masato Naka | Medium 읽는 것을 추천드립니다.\nChallenge 리소스 생성 쉽게 삭제할 수 있도록 Challenge를 만들자! Challenge는 Deployment, Service를 하나의 컴포넌트로 구성하여 생성하게 됩니다. 삭제 시 고아 리소스가 발생하지 않도록 생성하는 것이 중요합니다. 위의 과제를 달성하기 위해서 Challenge-Deployment, Service간 부모-자식 관계를 지정했습니다.\nOwns에 대해서 소유 관계라고 말하는 사람이 있고, 어느 책에서는 부모-자식 관계라고 설명하기도 했습니다. 저는 상하 관계를 뚜렷하게 표현할 수 있어 부모-자식 관계로 설명하겠습니다.\n쉽게 말해 커스텀 리소스(Challenge) 부모이고 Deployment, Service 자식 관계라고 생각하시면 됩니다. Challenge가 Deployment과 연관된다는 것은 Deployment에 변경 사항 발생 시 Reconcile()이 발생한다는 것입니다.\n부모-자식 관계를 사용한 이유는 삭제 시 편리하기 때문입니다. 구체적으로 말하자면 자식 리소스가 부모 리소스에 소속된 것으로 설정되어 있다면 부모 리소스 삭제 쿠버네티스의 가비지 컬렉션은 모든 자식 리소스를 자동으로 정리합니다.\n구현 생성 시 아래의 로직을 따릅니다.\nChallenge 객체 초기화 LoadDefinition을 로드 LoadDefinition 기반으로 Deployment, Service를 생성(부모-자식 관계) Challenge에 필요 레이블을 추가 여기서 초기화 부분에 challenge.Status.StartedAt 를 현재 시간으로 저장했습니다. 이 속성값은 초기화 혹은 추후 30분 후에 삭제하기 위해 비교값으로 활용됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // 처음 생성 시 StartedAt 등 Status 초기화 if challenge.Status.StartedAt == nil { if err := r.Get(ctx, req.NamespacedName, \u0026amp;challenge); err != nil { return r.handleError(ctx, req, \u0026amp;challenge, err) } now := metav1.Now() challenge.Status.StartedAt = \u0026amp;now challenge.Status.CurrentStatus = *hexactfproj.NewCurrentStatus() if err := r.Status().Update(ctx, \u0026amp;challenge); err != nil { log.Error(err, \u0026#34;Failed to initialize status\u0026#34;) return r.handleError(ctx, req, \u0026amp;challenge, err) } } // 최신 상태로 갱신 if err := r.Get(ctx, req.NamespacedName, \u0026amp;challenge); err != nil { // NotFound 에러 등은 무시 return r.handleError(ctx, req, \u0026amp;challenge, err) } // 현재 상태에 따라 분기 switch { case challenge.Status.CurrentStatus.IsNone(): // ... if err := r.Get(ctx, req.NamespacedName, \u0026amp;challenge); err != nil { return r.handleError(ctx, req, \u0026amp;challenge, err) } // 실제 Challenge에 필요한 리소스들(Deployment, Service 등) 생성 로직 err := r.loadChallengeDefinition(ctx, req, \u0026amp;challenge) if err != nil { return r.handleError(ctx, req, \u0026amp;challenge, err) } // 한 번 더 재큐(Requeue)하여 바로 다음 단계 확인 return ctrl.Result{Requeue: true}, nil Deployment, Service 생성 시 Challenge에 대한 SetControllerReference 를 설정합니다. 그리고 Client.Create를 통해 Deployment,Service를 생성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // Deployment가 존재하는지 확인 err := r.Get(ctx, client.ObjectKey{ Namespace: challenge.Namespace, Name: deploy.Name, }, deploy) if err != nil { if errors.IsNotFound(err) { log.Info(\u0026#34;Creating Deployment\u0026#34;, \u0026#34;Deployment.Namespace\u0026#34;, deploy.Namespace, \u0026#34;Deployment.Name\u0026#34;, deploy.Name) // Owner Reference 설정 if err := ctrl.SetControllerReference(challenge, deploy, r.Scheme); err != nil { log.Error(err, \u0026#34;failed to set controller reference\u0026#34;) return err } // Deployment 생성 err = r.Client.Create(ctx, deploy) if err != nil { log.Error(err, \u0026#34;failed to create Deployment\u0026#34;) return err } } else { log.Error(err, \u0026#34;failed to get Deployment\u0026#34;) return err } } 그리고 컨트롤러 Builder 초기화 부분에서 소유 관계를 명확히 구현하시면 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // SetupWithManager sets up the controller with the Manager. func (r *ChallengeReconciler) SetupWithManager(mgr ctrl.Manager) error { // Builder를 초기화 return ctrl.NewControllerManagedBy(mgr) // Challenge(부모 리소스) For(\u0026amp;hexactfproj.Challenge{}). // Deployment(자식 리소스) Owns(\u0026amp;appsv1.Deployment{}). // Service(자식 리소스) Owns(\u0026amp;corev1.Service{})./ Named(\u0026#34;challenge\u0026#34;). // Controller에 ChallengeReconciler를 삽입 Complete(r) } 상태를 감지하고 변경하는 방법 완전 새로운 Challenge라면 상태 값과 같은 기타 정보를 초기화하여 필요 리소스를 생성합니다. 이 과정에서 None -\u0026gt; Running 상태로 변경하게 되는데, 코드 상에서 참조하고 있는 challenge의 status 값을 Running를 변경하여 새로운 상태를 쿠버네티스에 업데이트합니다.\n1 2 3 4 5 6 challenge.Status.CurrentStatus.SetRunning() now := metav1.Now() challenge.Status.StartedAt = \u0026amp;now if err := r.Status().Update(ctx, \u0026amp;challenge); err != nil { return r.handleError(ctx, req, \u0026amp;challenge, err) } 컨트롤러 구현에 중요한 것은? - 멱등성 컨트롤러에서 말하는 멱등성은 변경 되지 않는 리소스의 조정 요청이 여러번 있더라도 매번 동일한 결과를 가져와야 한다는 의미입니다. 만약에 하나의 부모 리소스에 Deployment 1개로 구성된 자식 리소스를 가져야 한다고 가정해봅시다. 무슨 일이 발생하더라도 하나의 부모 리소스에 무조건 하나의 Deployment를 가져야 합니다. 중간에 추가적인 요청이 발생하여 Deployment가 2개 생성된다면 이것은 멱등성이 깨진 것입니다.\n멱등성은 Challenge Controller를 구현하면서 현재까지도 가장 어려워하는 부분이기도 합니다. \bChallenge는 쿠버네티스의 기본 리소스를 가지고 있지만 절차에 따라 레이블, 상태 값이 업데이트 됩니다. 변경 사항이 많은 탓에 Reconcile 로직을 구현할 때는 최신의 Challenge를 불러오도록 구현해야 합니다.\n1 2 3 if err := r.Get(ctx, req.NamespacedName, \u0026amp;challenge); err != nil { return r.handleError(ctx, req, \u0026amp;challenge, err) } 그렇지 않으면 Kubernetes Client가 이용하는 Kubernetes Cache에 있는 상태 값과 실제 데이터에 불일치가 발생해 문제가 발생하게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 { \u0026#34;controller\u0026#34;: \u0026#34;challenge\u0026#34;, \u0026#34;controllerGroup\u0026#34;: \u0026#34;apps.hexactf.io\u0026#34;, \u0026#34;controllerKind\u0026#34;: \u0026#34;Challenge\u0026#34;, \u0026#34;Challenge\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;challenge-59-sample\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34; }, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;challenge-59-sample\u0026#34;, \u0026#34;reconcileID\u0026#34;: \u0026#34;bf0eee44-a54d-417d-8322-7dc81e52152e\u0026#34;, \u0026#34;error\u0026#34;: \u0026#34;Operation cannot be fulfilled on challenges.apps.hexactf.io \\\u0026#34;challenge-59-sample\\\u0026#34;: the object has been modified; please apply your changes to the latest version and try again\u0026#34; } Challenge 리소스 삭제 삭제 상태를 알리기 위한 Finalizer 이용 상황에 따라 특별한 정리 로직이 필요한 경우가 있습니다. 이럴 경우 Finzliazer를 통해 리소스를 삭제하지 못하게 할 수 있습니다. 대표적인 예시로 pvc-protection가 있으며 이상 파드가 적극적으로 사용되지 않을때까지 pvc의 삭제가 연기됩니다.\n실제로 Finalizer를 사용하지 않으면 삭제는 빠르게 발생됩니다.\n프로젝트에서는 리소스 간 종속적인 관계를 가지지 않고 모두 독립적으로 사용됩니다. 그러므로 pvc 사례처럼 Finalizer가 필요 없다고 생각할 수 있습니다. 그러나 저희 프로젝트는 삭제 상태 정보를 설정하여 외부에 보내야 합니다. 뒤에서 말씀하겠지만 상태 정보를 큐를 통해 전송하게 되는데, 이때 삭제 요청 -\u0026gt; 상태 메세지 전송 -\u0026gt; 삭제 로직이 구현이 되어야 합니다. 결국 리소스 정리가 아니더라도 삭제가 되었다는 상태 정보를 다른 애플리케이션에게 알려줘야 하는 시간이 필요하게 됩니다. 저는 이러한 로직을 구현하기 위해 Finalizer를 사용하게 되었습니다.\n1 2 3 4 5 6 7 8 9 10 11 const ( challengeFinalizer = \u0026#34;challenge.hexactf.io/finalizer\u0026#34; ) func (r *ChallengeReconciler) addFinalizer(ctx context.Context, challenge *hexactfproj.Challenge) (ctrl.Result, error) { challenge.Finalizers = append(challenge.Finalizers, challengeFinalizer) if err := r.Update(ctx, challenge); err != nil { return ctrl.Result{}, fmt.Errorf(\u0026#34;failed to add finalizer: %w\u0026#34;, err) } return ctrl.Result{}, nil } 리소스가 삭제되는 2가지 방법 리소스가 삭제되는 방법은 크게 두 가지입니다.\n사용자가 삭제를 요청한다. 이용 시간이 30분 이상이면 삭제를 요청한다. 삭제 요청은 \u0026ldquo;삭제 요청하겠습니다!\u0026rdquo; 라는 deletionTimestamp 속성만 기록되는 것 뿐입니다. 실제로는 deletionTimestamp가 설정된 것을 확인하고 Finalizer가 제거된 후에 대상 리소스가 삭제된 것입니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 func (r *ChallengeReconciler) handleDeletion(ctx context.Context, challenge *hexactfproj.Challenge) (ctrl.Result, error) { log.Info(\u0026#34;Processing deletion\u0026#34;, \u0026#34;challenge\u0026#34;, challenge.Name) crStatusMetric.WithLabelValues(challenge.Labels[\u0026#34;apps.hexactf.io/challengeId\u0026#34;], challenge.Name, challenge.Labels[\u0026#34;apps.hexactf.io/user\u0026#34;], challenge.Namespace).Set(2) // Finalizer가 남아있는지 확인 if controllerutil.ContainsFinalizer(challenge, \u0026#34;challenge.hexactf.io/finalizer\u0026#34;) { // 파이널라이저 제거 controllerutil.RemoveFinalizer(challenge, \u0026#34;challenge.hexactf.io/finalizer\u0026#34;) // 메타데이터 업데이트 if err := r.Update(ctx, challenge); err != nil { log.Error(err, \u0026#34;Failed to remove finalizer\u0026#34;) // 재시도 위해 Requeue return ctrl.Result{RequeueAfter: time.Second * 5}, err } // 필요하다면 Deleted 이벤트 전송 err := r.KafkaClient.SendStatusChange( challenge.Labels[\u0026#34;apps.hexactf.io/user\u0026#34;], challenge.Labels[\u0026#34;apps.hexactf.io/challengeId\u0026#34;], \u0026#34;Deleted\u0026#34;, ) if err != nil { log.Error(err, \u0026#34;Failed to send status change message\u0026#34;) // 여기서도 에러 시 재시도 return ctrl.Result{}, err } } go func() { time.Sleep(1 * time.Minute) // scrape_interval이 30초라면 1분 정도 기다리면 안전 crStatusMetric.DeleteLabelValues(challenge.Labels[\u0026#34;apps.hexactf.io/challengeId\u0026#34;], challenge.Name, challenge.Labels[\u0026#34;apps.hexactf.io/user\u0026#34;], challenge.Namespace) }() log.Info(\u0026#34;Successfully completed deletion process\u0026#34;) // 이 시점에서 finalizers가 비어 있으므로, K8s가 오브젝트를 실제 삭제함 return ctrl.Result{}, nil } 사용자가 삭제를 요청한다. 사용자가 삭제를 요청하는 것은 사용자가 삭제 이벤트를 직접 하는 것과 같습니다. 요청 이후에는 자동으로 deletionTimestamp가 기록될 것 입니다. 아래 Reconcile() 로직에 의해서 삭제를 수행할 것입니다.\n1 2 3 if !challenge.DeletionTimestamp.IsZero() { return r.handleDeletion(ctx, \u0026amp;challenge) } 이용 시간이 30분 이상이면 삭제를 요청한다.\nChallenge 생성 부분에 실행 30분 후 삭제를 구현하기 위해 startedAt 를 설정한다고 말했죠? startedAt 기준으로 30분 후에 삭제되도록 구현했습니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 if time.Since(challenge.Status.StartedAt.Time) \u0026gt; challengeDuration { // 아직 DeletionTimestamp가 없다면 Delete 요청 if challenge.DeletionTimestamp.IsZero() { log.Info(\u0026#34;Time exceeded; issuing a Delete request\u0026#34;, \u0026#34;challenge\u0026#34;, challenge.Name) if err := r.Delete(ctx, \u0026amp;challenge); err != nil { log.Error(err, \u0026#34;Failed to delete challenge\u0026#34;) return r.handleError(ctx, req, \u0026amp;challenge, err) } // Delete 요청 후에는 Kubernetes가 DeletionTimestamp를 설정하고 // 다시 Reconcile이 호출되면 handleDeletion()이 수행됨 return ctrl.Result{}, nil } else { // 이미 Delete 진행중이면 handleDeletion으로 return r.handleDeletion(ctx, \u0026amp;challenge) } } 만약 제한 시간 이내에 있다면 requeue를 합니다.\nkube-apiserver를 통해 대상 리소스의 변경 사항 감지하면 이벤트를 work queue에 넣습니다. Controller가 시작되면 Watch 로직을 통해 큐에 있는 데이터를 구독하게 되고 Reconcile() 로직을 수행하게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 case challenge.Status.CurrentStatus.IsRunning(): if err := r.Get(ctx, req.NamespacedName, \u0026amp;challenge); err != nil { return r.handleError(ctx, \u0026amp;challenge, err) } // isOne이 false이면 일정 시간 내에만 작동 if !challenge.Status.IsOne \u0026amp;\u0026amp; time.Since(challenge.Status.StartedAt.Time) \u0026gt; challengeDuration { return r.handleDeletion(ctx, \u0026amp;challenge) } if !challenge.DeletionTimestamp.IsZero() { return r.handleDeletion(ctx, \u0026amp;challenge) } // Running 메세지 전송 err := r.KafkaClient.SendStatusChange(challenge.Labels[\u0026#34;apps.hexactf.io/user\u0026#34;], challenge.Labels[\u0026#34;apps.hexactf.io/challengeId\u0026#34;], \u0026#34;Running\u0026#34;) if err != nil { log.Error(err, \u0026#34;Failed to send status change message\u0026#34;) return r.handleError(ctx, \u0026amp;challenge, err) } } return ctrl.Result{RequeueAfter: requeueInterval}, nil 프로젝트에 대입해보자면 제한 시간 내에 있으면 몇 분 간격으로 30분이 지났는지 확인한다는 것입니다.\nrequeue Interval에 대한 생각\n기본적으로 쿠버네티스는 30초 간격으로 모든 리소스의 상태를 확인합니다. Operator 또한 work queue에 requeue하여 리소스 간격 시간을 설정할 수 있게 됩니다. 여기서 몇 초 간격으로 보는 것이 좋을까요?\n자주 리소스를 관찰하게 되면 큐에 많은 데이터가 쌓이게 되면서 최종적으로 시간 지연이 발생할 수 있습니다. 반면 큰 시간 간격으로 관찰하게 되면 \u0026ldquo;지속적으로 확인하여 상태 정보를 실시간으로 확인한다\u0026quot;라는 장점을 잃을 수 있습니다. 저는 최종적으로 30초 간격으로 Challenge의 상태를 확인했습니다.\n여담 .\n글로 정리하다보니 30초 간격으로 requeue할 필요가 없다고 생각했습니다. 초기에는 무슨일이 일어날지 모르니 30초 간격으로 확인하는게 좋겠다고 생각을 했습니다. 그러나 Challenge 하위 리소스에 변경사항이 감지되어 Reconcile를 수행한다면 30초 간격의 Requeue는 필요 없겠죠?\n정리 위의 내용을 흐름도로 정리하면 아래와 같습니다.\n마치며 글로 정리하다보니 스스로 잘못 생각한 부분을 찾게 되었습니다. 대표적인 예로 Requeue(재시도) 할 필요 없는데 구현한 것처럼 말이죠. 이건 다음에 수정해야 할 것 같습니다.\n이번 글을 끝으로 로직 부분 설명은 끝난 것 같습니다. 다음 글에서는 어떻게 커스텀 메트릭을 통해 Challenge를 시각화할 수 있는지 설명하겠습니다.\n참고 Finalizers, ownerReferences Kubernetes operator 메커니즘 Kubernetes Operator series 5— controller-runtime component — Reconciler | by Masato Naka | Medium Kubernetes Operator series 6 — controller-runtime component — Controller | by Masato Naka | Medium 제시슨 제시슨, 조슈아 우드. (2021). 쿠버네티스 오퍼레이터. 에이콘. ","date":"2025-03-04T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-8/","title":"HexaCTF 8. 커스텀 컨트롤러를 통한 Challenge 생성 및 삭제 구현"},{"content":"2025.03.02 - 내용 수정\n4개월 동안 여러 오류의 신세계의 맛본 결과 드디어 대회를 개최하게 되었습니다. 여타 CTF 대회와 다르게 3일이라는 긴 시간 동안 이뤄졌습니다.\n사전 점검 대회 하루 전에 전반적인 검토를 수행하게 되었습니다. 시간이 여유로울 때는 오류가 보이지 않았는데 왜 하루 이틀 전에 잘 보일까..\n서버가 좀 버벅거려요. 중간에 웹 페이지에 멈춤 현상이 있다고 해서 웹을 담당하는 파드를 확인했습니다.\n다중 접속한 것도 아니고 어떠한 행위를 하지 않았는데 request 한 메모리 양에 근접하게 사용하게 되었습니다.\n웹 부분이 이런건 처음이 아니였습니다. 초기에는 Flask는 메모리를 적게 사용되지 않을까 하고 request, limits를 적게 잡았다가 pod가 강제 종료되기도 했습니다.\n메모리 사용 패턴이 짐작이 가지 않았다. 어쩔때는 팍 튀는 형태로 나타나기도 하고, 어쩔때는 메모리 양이 수직 상승하다가 지속적으로 유지되기도 했습니다.\n그래서 request, limits를 지정하지 않았습니다.(실제로 이렇게 하면 안됩니다.) 근본적으로 저 문제를 해결하려면 애플리케이션을 우선 최적화해야 한다고 생각했습니다. 문제를 즉시 파악하고 해결하는데 시간이 부족하니 메모리 사용 패턴을 확인할 겸 과감하게 리소스 제한을 풀게 되었습니다.\n여담 서버 메모리가 생각보다 널널해서 리소스 제한을 풀었습니다. 그러나 다음 대회에서는 꼭 부하 테스트를 해서 적절하게 리소스를 제한을 설정할 것입니다.\nChallenge가 안열려요.. 금쪽이 Challenge Controller Challenge Controller 부분의 고질적인 문제는 바로 가끔 오류를 반환한다는 점이었습니다. 근본적인 원인은 사전에 파악은 하고 있었지만 관련 팀원과 함께 대공사가 필요하기에 바로 문제를 해결하지 않았습니다.\n대회 당일 저는 전반적인 인프라 관리와 Challenge 관리를 담당하게 되었습니다. 앞서 말했다싶이 Challenge Controller가 오류를 자주 반환한다고 말씀 드렸죠?\n그래서 대회 하루 이틀 동안은 아래와 같은 표정으로 모니터를 바라봤습니다.\n예상보다 Challenge Operator 쪽에는 별다른 문제는 없었습니다.\n가끔 Challenge가 생성이 안된다고 연락이 오다가 \u0026ldquo;다시 해보니 됩니다.\u0026ldquo;라는 메세지를 많이 받았습니다.\n마지막 날에는 사용자도 요령을 익혔는지 Challenge 시스템과 관련된 문의는 없었습니다. 나름 평화로웠습니다.\n대회 후기 - 다음 과제 일단 기간이 긴 대회인만큼 트래픽이 많이 몰린다는 이벤트는 없었습니다. 그래서 큰 문제 없이 대회를 마무리할 수 있었던 것 같습니다.\n첫 대회에 큰 의미가 있지만 기술적으로 부족한 부분을 확실히 파악하게 된 것 같습니다.\nChallenge Controller 고치기 Chllanege Controller 관련해서 두 가지 문제점이 있습니다.\n데이터베이스 스키마의 설계적인 문제점 프론트엔드가 실시간으로 Challenge 정보를 가져오지 못해서 발생하는 문제점 2번 문제는 프론트엔드가 Controller에게 요청을 보내고 5초 동안 대기를 수행한 후에 Challenge 정보를 얻습니다. 문제에 따라서는 빌드 시간이 5초가 넘을때가 있습니다. 이럴 경우에 클라이언트 측에서는 오류가 뜨지만 실제로 Challenge는 생성되게 됩니다.\n이번 대회 이후에는 위의 두 문제를 우선 고쳐야 할 것 같습니다.\n메모리가 계속 증가한다. - Flask 최적화 사전 점검때도 말했지만 메모리가 지속적으로 증가하며 이를 해결하기 위해서는 코드를 수정해야 한다고 말씀 드렸습니다.\n개인적으로 가장 심각하게 봤던 부분이 바로 메모리 사용량입니다.\nCTF 대회 특성상 아래의 문제 패턴이 있습니다.\n문제 파일을 다운로드한 후 다른 도구로 문제를 해결하여 웹사이트에 정답를 입력한다. Challenge를 생성하여 문제를 해결한 후 웹사이트에 정답을 입력한다. 웹사이트는 \u0026ldquo;정답 입력\u0026quot;을 위주로 활용됩니다.\n그렇다면 사용자가 문제를 해결할때 문제를 푸는 시간이 길까요? 입력하는 시간이 길까요? 당연 다른 도구로 문제 푸는 시간이 깁니다. 대회를 운영하면서 애플리케이션은 나름 상승폭이 적고 안정적인 메모리 사용량을 가질 것이라고 예상했습니다.\n아래는 웹사이트 메모리 사용량입니다. 점진적으로 증가하죠? 개인적으로 불필요한 메모리를 활용하고 있다고 생각했습니다. (웹사이트 부분은 프론트와 백엔드가 하나로 합쳐져 있습니다.)\nChallenge Controller 부분도 똑같이 메모리가 쌓이는 문제를 가지고 있습니다.\nFlask를 깊게 공부해서 메모리 상승 원인과 해결책을 찾아야겠습니다.\nChallenge 리소스 용량 산정 대회 끝나고 대시보드를 보니 Challenge 문제마다 메모리 최대 사용량은 일정합니다.\n문제 등록할때는 Request, limits를 넉넉하게 256MiB, 512MiB로 잡았습니다. 다음부터는 문제마다 섬세하게 리소스 용량를 고려해야 할 것 같습니다.\n저는 CPU보다 메모리 용량이 상대적으로 부족하다고 느끼고 있습니다. 그래서 메모리 사용량을 유심히 살펴보고 있습니다.\n여기서 \u0026ldquo;웹 애플리케이션 부분에서는 메모리가 널널하다고 하시지 않았나요?\u0026rdquo; 라고 물어볼 수 있을 것입니다. 쿠버네티스에서 멀티 노드를 구현할때 node1은 웹 애플리케이션과 기타 도구, node2는 Challenge를 위한 공간으로 분리했습니다. 즉 웹 애플리케이션이 있는 공간은 애플리케이션이 추가되지 않는 불변의 환경이고, Challenge는 언제 어디든지 리소스가 생성될 수 있는 가변의 환경입니다. 때에 따라서 메모리가 부족할 수도 널널할 수 있습니다.\n사람들의 문제 푸는 패턴을 보니 Challenge를 생성하고 삭제하는 것을 잊습니다. 즉, 한 사람당 2~3개를 생성한 후 방치한다는 의미입니다. 만약에 10명 이상의 사람이 만든 Challenge가 모두 살아있다면? 이 부분은 고민을 해야봐야겠습니다.\n대회는 계속된다\u0026hellip; 다음 대회 일정이 잡혔습니다. 위의 내용은 주요 문제점일 뿐 실제로 크고 작은 다양한 수정사항들이 있습니다. 열심히 수정해야겠죠?\n","date":"2025-02-28T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf2025-review/","title":"HexaCTF2025 대회 운영 후기"},{"content":"2025-03-04 - 내용 수정\n일반적으로 쿠버네티스의 파드는 라이프 사이클을 가지고 있습니다.\n이처럼 Challenge CR도 자체 라이프 사이클을 가지고 있습니다.\nChallenge 상태 Challenge Lifecycle Challenge의 라이프 사이클은 크게 3가지 형태로 나뉩니다.\nNone Running Error or Deleted Challenge 내부의 쿠버네티스 리소스가 정상적으로 생성, 삭제되었는지를 기준으로 상태가 결정됩니다.\nChallenge의 상태값이 무조건 필요한 이유가 있습니다. 바로 참가자가 자신이 실행한 Challenge의 상태값을 알아야 합니다. 예를 들어볼까요? 만약에 참가자가 특정 문제의 컨테이너를 생성했다고 가정했습니다. 무슨 이유로 서버를 새로고침하게 되어 아래와 같은 서버 생성하기 버튼이 활성화가 될 것입니다. 서버 생성하기 버튼을 아무리 눌러도 쿠버네티스 상에는 컨테이너가 이미 존재하니 계속 오류가 뜨겠죠?\nChallenge의 상태 구현 Kubebuilder에서 CR를 생성하면 metatdata, spec, status 세 가지 필드를 확인할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 // +kubebuilder:object:root=true // +kubebuilder:subresource:status // Challenge is the Schema for the challenges API. type Challenge struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec ChallengeSpec `json:\u0026#34;spec,omitempty\u0026#34;` Status ChallengeStatus `json:\u0026#34;status,omitempty\u0026#34;` } 그 중 ChallengeStatus에 Challenge에 관한 상태 정보를 추가합니다.(CurrentStatus) CurrentStatus는 간단한 구조체로 설계한 4개의 상태를 설정할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // ChallengeStatus defines the observed state of Challenge. type ChallengeStatus struct { // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster // Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file // StartedAt: Challenge 시작 시간 StartedAt *metav1.Time `json:\u0026#34;startedAt,omitempty\u0026#34;` // CurrentStatus: Challenge 현재 상태 CurrentStatus CurrentStatus `json:\u0026#34;currentStatus,omitempty\u0026#34;` // isOne는 영속성을 나타낸다. // +optional IsOne bool `json:\u0026#34;isOne,omitempty\u0026#34;` // Endpoint: Challenge의 Endpoint // 외부에 노출될 포트 번호가 저장됩니다. Endpoint int `json:\u0026#34;endpoint,omitempty\u0026#34;` } Operator의 매커니즘 Operator SDK vs Kubebuilder Kubernetes Operator는 비단 Kubebuilder만 있는 것이 아닙니다. Operator SDK도 있습니다.\nTech-Reset - kubebuilder vs operator-sdk의 내용을 참고하여 차이점을 설명하겠습니다.\nOperator SDK는 기본적으로 namespace-scoped, Kubebuilder cluster-scoped Operator SDK는 helm watcher도 함께 배포되기 때문에 Helm과 통합하기 용이하고, Kubebuilder는 kubernetes-sigs의 프로젝트와 통합이 용이하다고 합니다. 여담 유명한 CNCF 프로젝트 Operator는 대부분 Operator SDK를 사용하는 것 같습니다. 유명 CNCF 프로젝트는 Helm으로 배포하는 경우가 많은데 Operator SDK는 당연한 선택지일 수 있습니다.\n앞으로 Kubebuilder를 기준으로 설명하겠습니다.\n쿠버네티스에서 말하는 Controller 우선 Kubebuilder에 이야기 하기 전에 기본적인 쿠버네티스가 활용하는 kube-control-manager와 controller에 대한 이야기가 필요합니다.\n쿠버네티스는 kube-control-manager 통해 내장 controller들을 제어합니다.(control manager라고 부릅니다.) 기본적으로 쿠버네티스가 가지고 있는 리소스 유형에 따라 controller를 가지고 있습니다. 쿠버네티스 기본 리소스마다 특징이 뚜렷하기 때문에 역할을 확실히 나눔으로써 유지보수성이나 확장성을 높이기 위해 사용된 것으로 보입니다.1\ncontrol manager 내부에 있는 controller들은 kube-apiserver 와 통신하는 client-go등 다양한 공유 디펜던시(shared dependency)를 활용하여 다양한 기능을 수행하게 됩니다.\n쿠버네티스는 controller를 직접 제작함으로써 다양한 기능을 구현할 수 있습니다. 단순하게 내장된 리소스에 국한되지 않고 다양한 종류의 프로젝트를 수용할 수 있었던 것도 custom controller 덕분일 것입니다.\n쿠버네티스는 초기부터 확장성을 고려하여 설계된 것일지도 모릅니다. 즉, control-manager를 직접 구현할 수 있다면 자동화된 운영, 모니터링 등 다양한 기능을 확장시킬 수 있게 됩니다.\nOperator? Controller? Kubebuilder는 오퍼레이터를 빌드하는데 사용되는 도구라고 알려져 있습니다. 오퍼레이터는 CoreOS에서 공개된 디자인 패턴으로, 단어 그대로 운영자의 역할을 소프트웨어에 새긴 개념입니다.2\n위의 설명은 오퍼레이터는 기능적인 관점에서 바라보는 것이고 기술적으로는 운영 대상을 제어하는 control-manager를 제작하는 것이라고 봅니다.\nkubebuilder 구조에 대한 간단한 설명 위의 설명을 보면 kubebuilder 아키텍쳐를 쉽게 이해할 수 있을 것입니다.\nkubebuilder는 Controller를 빌드하는 프로그램이라고 보면 됩니다. 공식 문서 상에서 Kubebuilder는 위와 같은 구조를 가지지만 실제 코드로 구현해야 할 부분은 Manager, Builder, Reconciler 가 됩니다.\nBuilder : controller-runtime 라이브러리를 감싸며, 일반적인 컨트롤러를 구축하기 위한 패턴을 제공한다.(builder package 문서) Manager : controller를 제작하는데 필요하며 client, caches, schemes등 의 공유 디펜던시를 제공한다. 컨트롤러는 Manager.Start를 호출하여 시작되어야 한다.(manager package 문서 ) Reconciler: 조정 동기화 작업으로 대상의 상태를 원하는 상태로 일치시키는 과정을 의미한다. 대부분의 그림에서는 Builder에 대한 내용이 없는데 그 이유는 Operator-SDK가 Builder의 역할을 하기 때문입니다. 자세한 이야기는 Challenge 생성, 삭제 부분에서 자세하게 말씀 드리겠습니다.\nChallenge CR로 Operator를 제작해도 될까? Kubernetes Operators 101 Overview | Red Hat Developer 문서에 의하면 오퍼레이터는 stateful한 애플리케이션을 섬세하게 관리하기 위해 사용됩니다.\nWhile Kubernetes is great at managing stateless applications, operators are useful when you need more complex configuration details for a stateful application such as a database. A stateful workload is more difficult to manage than a stateless workload.\nChallenge는 상태 정보가 유지 되지 않는 Stateless한 애플리케이션입니다. 그러나 위의 설명을 보면 Challenge는 오퍼레이터를 사용하면 안될 것 같습니다.\n주요 활용 대상이 Stateful한 애플리케이션일 뿐 Stateless한 리소스도 적용 가능하다고 생각합니다. lstio도 stateless한 data plane를 제어한다고 합니다.3 오퍼레이터는 운영자의 역할을 소프트웨어에 새긴 개념을 가지고 있는 만큼 모든 애플리케이션에 적용 가능하겠죠?\n다음 이야기 오퍼레이터에 대해 공부를 하다보니 쿠버네티스의 구조에 대해 파악하고 있어야만 이해가 되는 부분이 많다고 생각합니다. 쿠버네티스 기본 구조에 대해 조사해보니 이번 글에는 프로젝트에 대한 많은 이야기를 담지 못했습니다.\n다음 글부터는 kubebuilder를 활용하여 Challenge를 생성, 삭제할 수 있는지 설명하겠습니다.\n참고 Kubernetes Operator series 3 — controller-runtime component — Manager | by Masato Naka | Medium Kubernetes Operator series 4— controller-runtime component — Builder | by Masato Naka | Medium How kubernetes Controller Manager works - SoByte Controllers | Kubernetes How kubernetes Controller Manager works - SoByte\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n쿠버네티스 오퍼레이터 적용하기\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWhat is a service mesh? Architecture, features \u0026amp; relation with Kubernetes\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-02-27T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-7/","title":"HexaCTF 7. Challenge CR 상태 정의와 Operator 개념 정리"},{"content":"이전 이야기 이전 글에는 ChallengeDefinition, Challenge 개념을 정의하고 설계하는 과정을 설명했습니다.\n이제부터 ChallengeDefinition과 Challenge의 속성은 무엇인지 코드와 함께 소개하겠습니다.\nChallengeDefinition\u0026amp;Challenge 구성 HexaCTF는 두 개의 쿠버네티스 CR로 구성되어 있습니다.\nChallengeDefinition : 문제 상세 정보를 나타내며 챌린지 정보, 구성 리소스를 컴포넌트 단위로 정의한다. Challenge : 사용자(문제 풀이자)가 생성하는 문제의 단위로 참조된 ChallengeDefinition을 기반으로 리소스를 생성하고 삭제한다. 이제부터 각각의 리소스가 어떤 구성요소를 가지고 있는지 설명하겠습니다.\nChallenge Flask 서버는 Challenge 생성 요청을 통해 사용자 이름(username)과 문제 번호(challenge_id) 얻게 됩니다. 그리고 파이썬 쿠버네티스 라이브러리를 활용하여 Challenge CR을 실행시키게 됩니다.\n요구사항을 정의하자면 두 개의 정보 만으로도 Challenge를 생성할 수 있어야 하며 모든 문제가 실행 가능하도록 일관된 구조를 가지고 있어야 합니다. 그래야 서버가 파일을 불러오는 형식이 아닌 Kubernetes API에 CR 생성을 요청할 수 있게 됩니다.\n파일을 불러오는 방식 Challenge의 구조가 문제 유형마다 다르면 Challenge CR 파일을 별도로 저장해서 호출해야 합니다. 파일이 아니더라도 어떻게 다른지를 저장해야 할 것입니다.\n구조 Metadata name : 챌린지 이름 namespace : Challenge를 실행시킬 namespace labels username : 사용자 이름 challenge_id : Challenge id Spec definition : ChallengeDefinition 이름 Metadata는 go map 형식으로 이뤄져 있습니다. 그러므로 손쉽게 key를 통해서 값을 가져올 수 있게 됩니다. 직접 구조체로 명시해야 하는 부분은 바로 Spec 입니다.\n1 2 3 4 type ChallengeSpec struct { // Definition: ChallengeDefinition 이름 Definition string `json:\u0026#34;definition\u0026#34;` } Kubebuilder의 명령어를 통해 코드를 CRD로 변환시킨 후 설치하는 작업을 수행합니다. (Kubebuilder 공식문서 Quickstart) 모든 작업이 끝나면 아래의 yaml 파일을 통해 Challenge를 생성할 수 있게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: apps.hexactf.io/v1alpha1 kind: Challenge metadata: name: ubuntu-instance-1 namespace: default labels: apps.hexactf.io/challengeId: \u0026#34;1\u0026#34; apps.hexactf.io/user: \u0026#34;test\u0026#34; spec: # 사용할 ChallengeDefinition의 이름 definition: ubuntu-basic 주된 목표는 서버가 사용자의 Challenge를 만드는 것입니다. 다시 말해 서버가 사용자 정보가 담긴 yaml 파일을 선언하고 실행시켜야 한다는 의미와 같습니다.\n사전에 문제 등록 과정이 있기에 사용자, Challenge에 대한 기본 정보는 저장되어 있습니다. 이후에 설명하겠지만 definition는 ChallengeDefinition의 이름으로 Challenge의 이름(제목)에 해당됩니다. 다른 말로 Challenge id만으로도 definition을 알 수 있어 최소한의 정보로 Challenge를 생성할 수 있게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 user_challenge_repo = UserChallengesRepository() # Challenge definition 조회 challenge_definition = ChallengeRepository.get_challenge_name(challenge_id) if not challenge_definition: raise ChallengeNotFound(error_msg=f\u0026#34;Challenge definition not found for ID: {challenge_id}\u0026#34;) # Challenge name 생성 및 검증 challenge_name = f\u0026#34;challenge-{challenge_id}-{username}\u0026#34; if not self._is_valid_k8s_name(challenge_name): raise UserChallengeCreationError(error_msg=f\u0026#34;Invalid challenge name: {challenge_name}\u0026#34;) # ... # Challenge manifest 생성 challenge_manifest = { \u0026#34;apiVersion\u0026#34;: \u0026#34;apps.hexactf.io/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Challenge\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: challenge_name, \u0026#34;labels\u0026#34;: { \u0026#34;apps.hexactf.io/challengeId\u0026#34;: str(challenge_id), \u0026#34;apps.hexactf.io/user\u0026#34;: username } }, \u0026#34;spec\u0026#34;: { \u0026#34;namespace\u0026#34;: namespace, \u0026#34;definition\u0026#34;: challenge_definition } } challenge = self.custom_api.create_namespaced_custom_object( group=\u0026#34;apps.hexactf.io\u0026#34;, version=\u0026#34;v1alpha1\u0026#34;, namespace=namespace, plural=\u0026#34;challenges\u0026#34;, body=challenge_manifest ) ChallengeDefinition ChallengeDefinition은 Challenge의 인프라 구성을 기술하는 부분입니다. \u0026ldquo;어떤 이미지를 컨테이너로 만들어서 어떻게 네트워크를 구성할건데?\u0026rdquo; 를 명시하는 구간입니다.\n구조 Metadata name: 챌린지 이름(제목) Spec isOne : 영속성 여부입니다. 값이 True인 경우 1:1 문제 유형으로 30분 후에 삭제되어야 합니다. component : 쿠버네티스 리소스인 Deployment \u0026amp; Service를 하나의 Component로 정의합니다. Component 내에 있는 Deployment, Service는 기존의 yaml 선언 형식과 동일합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion: apps.hexactf.io/v1alpha1 kind: ChallengeDefinition metadata: name: web-basic namespace: default spec: isOne: false components: - name: web deployment: spec: replicas: 1 template: spec: containers: - name: nginx image: nginx:1.25 ports: - containerPort: 80 .... service: spec: ports: - name: http port: 80 targetPort: 80 protocol: TCP type: NodePort ChallengeDefinition은 Deployment의 모든 필드를 지원하는 것이 아니다. 앞서 Component 내에 있는 Deployment는 기존의 yaml 선언 형식과 동일하다고 말씀을 드렸지만 일부는 지원하지 않습니다.\nKubebuilder를 활용하여 코드를 제작할때 라이브러리를 통해 쿠버네티스의 리소스를 활용합니다. Service를 예로 들면 yaml에 정의된 필드가 구조체 형식으로 선언되어 있습니다. 쿠버네티스의 리소스는 라이브러리의 구조체를 활용하는 것이라고 보면 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type Service struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` // Standard object\u0026#39;s metadata. // More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata // +optional metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34; protobuf:\u0026#34;bytes,1,opt,name=metadata\u0026#34;` // Spec defines the behavior of a service. // https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status // +optional Spec ServiceSpec `json:\u0026#34;spec,omitempty\u0026#34; protobuf:\u0026#34;bytes,2,opt,name=spec\u0026#34;` // Most recently observed status of the service. // Populated by the system. // Read-only. // More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status // +optional Status ServiceStatus `json:\u0026#34;status,omitempty\u0026#34; protobuf:\u0026#34;bytes,3,opt,name=status\u0026#34;` } 라이브러리를 적절하게 사용하여 Component의 Deployment, Service를 선언합니다. 이번 프로젝트의 경우 Service는 라이브러리에서 제공하는 구조체를 모두 사용했지만 Deployment는 필요한 부분만 추출해서 선언했습니다. \b라이브러리에서 제공하는 Deployment를 사용하면 Deployment의 수많은 옵션이 포함된 구조체를 Copy하는 과정에서 용량이 초과되어 CRD 생성에 실패하게 됩니다.\nChallengeDefinition의 Deployment에서는 컨테이너 구성 부분인 Containers와 Replicas만으로 구성했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import ( corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; ) // ChallengeDefinitionSpec defines the desired state of ChallengeDefinition. type ChallengeDefinitionSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster // Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file // IsOne: 하나만 생성할 경우 // False일 경우 일정 시간 내에서만 작동된다. IsOne bool `json:\u0026#34;isOne,omitempty\u0026#34;` // Components: Challenge를 구성하는 컴포넌트들 Components []Component `json:\u0026#34;components,omitempty\u0026#34;` } // Component 는 이름과 리소스를 정의 type Component struct { Name string `json:\u0026#34;name,omitempty\u0026#34;` Deployment *CustomDeployment `json:\u0026#34;deployment,omitempty\u0026#34;` Service *corev1.Service `json:\u0026#34;service,omitempty\u0026#34;` } // Deployment 관련 구조체 // CustomDeploymentSpec 는 Replicas와 Template을 정의 // 자세한 내용은 Kubernetes Deployment API 문서 참고 // https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ type CustomDeployment struct { Spec CustomDeploymentSpec `json:\u0026#34;spec,omitempty\u0026#34;` } type CustomDeploymentSpec struct { Replicas int32 `json:\u0026#34;replicas,omitempty\u0026#34;` Template CustomPodTemplateSpec `json:\u0026#34;template,omitempty\u0026#34;` } type CustomPodTemplateSpec struct { Spec CustomPodSpec `json:\u0026#34;spec,omitempty\u0026#34;` } type CustomPodSpec struct { Containers []corev1.Container `json:\u0026#34;containers,omitempty\u0026#34;` } Component 구성 이유 이전 글에서 언급한 SQL Injection 사례 기억나시나요? 문제를 배포하기 위해서는 server- db가 필요하다고 말씀드렸습니다.\n문제를 실행시키기 위해서는 server는 db가 어디에 있는지 알아야 합니다. 어떤 문제를 사용하면서 사용자가 누구인지 알았지만 어떤 서비스인지 알 수 있는 방법이 있을까요? 단순히 Deployment, Service를 나열하는 구조라고 했을때 애플리케이션의 특징에 맞는 prefix를 추가하면 됩니다. 그러나 일관성이 떨어질 수 있다고 생각하여 Component 단위로 Deployment, Service를 묶게 되었습니다.\nComponent 내 Label Selector 쿠버네티스에서 label은 중요한 역할을 합니다. 리소스 간 연결해주는 다리 역할을 합니다.\n기본적으로 쿠버네티스는 Deployment의 네트워크를 정의하기 위해서는 label selector를 통해 Service와 연결합니다. 그렇다면 ChallengeDefinition을 정의할때 label을 사용자가 지정해야 할까요?\n답은 Operator가 자동으로 지정해줍니다.\n리소스의 이름에 대해 이야기를 해보겠습니다. Challenge을 통해 생성된 리소스는 Challenge의 challenge_id, user 정보와 ChallengeDefinition에서 얻은 Component 이름을 조합한 고유한 prefix를 가지고 있습니다. 역할에 따라 deploy, svc를 추가로 붙여서 리소스의 이름을 정하게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // ChallengeIdentifier // 도메인에 맞는 식별자를 생성해주는 구조체 type ChallengeIdentifier struct { prefix string labels map[string]string } func NewChallengeIdentifier(challenge *hexactfproj.Challenge, component hexactfproj.Component) *ChallengeIdentifier { // prefix 생성 (리소스 이름에 사용) prefix := fmt.Sprintf(\u0026#34;chall-%s-%s-%s\u0026#34;, challenge.Labels[\u0026#34;apps.hexactf.io/challengeId\u0026#34;], component.Name, challenge.Labels[\u0026#34;apps.hexactf.io/user\u0026#34;]) // 단일 레이블 맵 사용 labels := map[string]string{ \u0026#34;apps.hexactf.io/instance\u0026#34;: prefix, \u0026#34;apps.hexactf.io/name\u0026#34;: component.Name, \u0026#34;apps.hexactf.io/part-of\u0026#34;: challenge.Name, \u0026#34;apps.hexactf.io/managed-by\u0026#34;: \u0026#34;challenge-operator\u0026#34;, } return \u0026amp;ChallengeIdentifier{ prefix: prefix, labels: labels, } } # ... func (c *ChallengeIdentifier) GetDeploymentPrefix() string { return c.prefix + \u0026#34;-deploy\u0026#34; } func (c *ChallengeIdentifier) GetServicePrefix() string { return c.prefix + \u0026#34;-svc\u0026#34; } 또한 Challenge로부터 생성된 Deployment 리소스는 아래와 같은 레이블을 추가로 가지게 됩니다. 그 중 apps.hexactf.io/instance Service의 selector 로 선언하여 서로 연결해줍니다.\n1 2 3 4 5 6 7 8 9 10 11 12 prefix := fmt.Sprintf(\u0026#34;chall-%s-%s-%s\u0026#34;, challenge.Labels[\u0026#34;apps.hexactf.io/challengeId\u0026#34;], component.Name, challenge.Labels[\u0026#34;apps.hexactf.io/user\u0026#34;]) // 단일 레이블 맵 사용 labels := map[string]string{ \u0026#34;apps.hexactf.io/instance\u0026#34;: prefix, \u0026#34;apps.hexactf.io/name\u0026#34;: component.Name, \u0026#34;apps.hexactf.io/part-of\u0026#34;: challenge.Name, \u0026#34;apps.hexactf.io/managed-by\u0026#34;: \u0026#34;challenge-operator\u0026#34;, } 컴포넌트 간 통신은 어떻게해요? SQL Injection 문제를 다시 생각해보겠습니다. Server는 데이터베이스와 통신하기 위해서 데이터베이스의 host 주소를 지정해줘야 합니다. host 주소를 어떻게 알 수 있을까요?\n자동으로 찾아준다. : 🥹 직접 지정해준다. : IaC 도구처럼 서비스 이름을 지정해준다. 가변 인자를 삽입할 수 있는 로직을 구현해야 한다. localhost로 설정한다. : 일관된 host를 가질 수 있다. 이걸 다르게 해석할 수 있습니다.\n자동으로 찾아준다. : 🥹 직접 지정해준다. : 다른 컴포넌트의 Service를 지정할 수 있기 때문에 컴포넌트 간 통신이 가능합니다. localhost로 설정한다. : 컴포넌트 간 통신이 불가능합니다. Pod 내 여러개의 컨테이너를 배포합니다. 3번인 Pod 내에 다양한 컨테이너를 삽입하는 방향을 선택했습니다. 서비스를 지원하려면 ChallengeDefinition 구조를 크게 바꿔야 합니다. 시간 여유도 없지만 기술적으로 부담이 됩니다. 또한 2개 이상의 컴포넌트를 제공하는 경우는 드물고 대부분 제한 시간 후에 삭제되는 유형(1:1)입니다. 현재로서 급한 기능은 아니기 때문에 제외했습니다.\n다음 이야기 이번 글에서는 ChallengeDefinition과 Challenge type에 대한 구성 요소와 관련 코드를 보여드렸습니다. 이제부터 Operator 구현 시작이라고 생각합니다. \u0026ldquo;CR을 어떻게 상태 관리할래?\u0026ldquo;를 구현한 controller를 제작해야 합니다. 다음 글에는 controller 구현 부분인 \u0026ldquo;상태 관리 파트\u0026quot;로 찾아오겠습니다.\n","date":"2025-01-24T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-6/","title":"HexaCTF 6. Kubebuilder를 활용한 ChallengeDefinition\u0026Challenge Type 구현"},{"content":"이전 이야기 이전글에서는 문제 컨테이너(Challenge)의 특징과 그에 따른 요구사항을 정의했습니다.\n컨테이너 상태를 지속적으로 확인할 수 있어야 함 다양한 컨테이너를 하나의 구조로 직접 정의하고 관리할 수 있어야 함 Docker로 위의 기능을 구현하는 것은 기술적인 한계가 있으므로 Kubernetes를 선택했다고 말씀드렸습니다. Kubernetes를 통해 어떻게 CTF를 제작했는지 작성하겠습니다.\nKubernetes를 선택한 이유 글 초반에 두 가지의 요구사항은 충족되어야 한다고 말씀 드렸습니다. 쿠버네티스를 사용하면 요구사항을 충족할 수 있습니다.\n컨테이너 상태를 지속적으로 확인할 수 있어야 함 쿠버네티스는 기본적으로 리소스의 상태를 주기적으로 확인하며 오류 발생시 재시작을 수행하는 특징을 가지고 있습니다. 별도의 구현 없이 지속적으로 컨테이너 상태를 확인하는 기능을 활용할 수 있습니다.\n다양한 컨테이너를 하나의 구조로 직접 정의하고 관리할 수 있어야 함\nNOTICE! 물론 쿠버네티스가 쉽다는 것은 아닙니다. 현재로서 위의 요구사항을 직접 구현한 것보다는 쿠버네티스를 활용하는 것이 현실적으로 가능한 방법이라고 생각했습니다.\nChallenge 개념 재정의 기술적인 이야기를 말하기 앞서 도메인 정의가 필요합니다.\nCTF 사용자 분류 대회 운영자 CTF 대회 사이트를 운영하는 사람 문제 발생 시 즉시 해결해야 함 문제 출제자 문제 파일을 제작 후 운영자에게 제공 대회 참가자 CTF 사이트에서 제공한 문제를 해결 대회 운영 방식마다 각 역할이 하는 범위는 달라질 것입니다. 그러나 저희 프로젝트에서는 문제 등록과 리소스 관리를 대회 운영자가 하기로 결정했습니다.\n개념 정의 쿠버네티스를 적용하기 이전에 개념을 다시 정리해봅시다.\n사전에 등록한 문제를 사용자가 실행시키기 위해서는 어떻게 해야할까요?\n등록하는 사람은 문제(Challenge) 컨테이너에 대한 구성을 정의하고 사용자는 그 정의한 내용을 그대로 실행시키는 것입니다. 생성된 문제들은 사용자 정보를 기준으로 식별되어야 합니다.\n그렇다면 문제를 생성하기 위해서는 최종적으로 두 가지가 필요하게 됩니다.\n문제 정의서(ChallengeDefinition) 사용자의 정보가 포함된 문제(Challenge) 이제부터 문제 정의서와 사용자가 풀어야 하는 문제를 ChallengeDefinition과 Challenge라고 부르겠습니다.\nCR로 ChallengeDefinition과 Challenge 정의하기 프로젝트에서 Kubernetes Operator를 사용하는 방법 Kubernetes Operator는 사용자가 정의한 리소스(CR)를 관리하는 프로그램입니다. 즉, 오퍼레이터를 제작한다는 것은 사용자가 정의한 리소스를 모니터링하는 루프를 실행시키는 프로그램을 제작한다는 의미입니다.1\n리소스를 모니터링한다는 것은 상태를 관리한다는 의미와 같습니다. 그 대상은 상태 라이프사이클을 가질 것입니다 Kubernetes Operator SDK (kubebuilder, kopf \u0026hellip;)를 활용하여 상태 라이프사이클을 정의하고 구현할 수 있게 됩니다. Kubernetes Operator의 활용은 요구사항 모두 충족할 수 있습니다.\n이번 프로젝트에서는 Challenge의 Controller로서 Kubernetes Operator를 구현할 예정입니다.\nCR(Custom Resource)\u0026amp;CRD(Custom Resource Definition) 직접 정의한 리소스의 스펙을 CRD(CustomResourceDefinition)으로 정의한 후 스펙에 맞게 CR(CustomResource)를 생성하는 YAML를 실행시키면 커스텀 리소스를 실행시킬 수 있습니다.\nCR\u0026amp;CRD에 대한 기술적인 내용은 ccambo님의 Kubernetes 확장인 CRD와 CR 에 대한 개념 정리 를 참고하시길 바랍니다.\n원하는 리소스를 직접 정의해서 배포할 수 있다는 특징은 요구사항 2번인 \u0026ldquo;다양한 컨테이너를 하나의 구조로 직접 정의한다\u0026quot;를 가능하게 만들어줍니다. 이번 프로젝트에서는 CR\u0026amp;CRD를 적극적으로 활용할 예정입니다.\nChallengeDefinition\u0026amp;Challenge 간단하게 정의하기 이번 프로젝트에서는 두 개의 커스텀 리소스를 생성할 것입니다.\nChallengeDefinition : 문제 상세 정보를 나타내면 챌린지 정보, 구성 리소스를 컴포넌트 단위로 정의한다. CTF 문제를 등록할때 ChallengeDefinition을 등록한다. Challenge : 사용자 정보를 metadata에 주로 저장하고 spec으로 ChallengeDefinition를 참조하여 정의한다. 주로 사용자가 챌린지 컨테이너를 생성하고 실행할때 Challenge를 정의하고 실행시킨다. 왜 저렇게 구성했을까요? 구체적인 이유는 뒤에 구현과 함께 설명할 예정입니다. 간단하게 말씀드리자면 모든 리소스가 사용자마다 식별 가능해야 합니다.\n사용자의 고유 데이터, 문제의 고유 데이터를 가지고 ChallengeDefinition에 정의된 리소스들이 서로 식별 가능하도록 이름을 정할 것입니다. 그래야 동일한 리소스더라도 충돌이 발생하지 않을 것입니다.\n다음 이야기 지금까지 Challenge라는 개념을 재정의하고 Kubernetes 커스텀 리소스로써 어떻게 구성할 것인지 작성했습니다.\n다음 글에서는 어떻게 코드로 ChallengeDefinition, Challenge를 구현했는지 설명하겠습니다.\nReference Kubernetes 확장인 CRD와 CR 에 대한 개념 정리 간단하게 알아보는 Kubernetes Operator의 개념과 Kopf 프레임웍 쿠버네티스 오퍼레이터. 제이슨 도비스, 조슈아 우드 지음, 에이콘\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-01-18T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-5/","title":"HexaCTF 5.  Kubernetes CR를 활용한 Challenge 설계"},{"content":" 2025.01.18: 제목 수정 CTF의 문제 특징 문제 종류 Challenge를 어떻게 구성할 것인가를 생각하기 전에 문제 배포 관점에서 CTF 문제 종류를 파악해야 합니다. 크게 5가지의 카테고리가 있으며 문제를 출제하는 방식은 크게 두 가지있습니다.\nLive : 실제로 운영중인 서버를 통해 문제를 푸는 경우 File : 파일을 다운받아서 문제를 푸는 경우 Live 방식 중에서 두 가지를 더 나눌 수 있습니다.\n1:1 : 한 사람당 고유한 서버에 접속해야 하는 경우, 주로 상태가 있는(Stateful) 애플리케이션 문제에 적합 1:N : 하나의 서버에 여러 사람이 접속해야 하는 경우, 주로 상태가 없는(Stateless) 애플리케이션의 문제에 적합 예를 한번 들어보겠습니다. 단순하게 nginx, server로만 구성된 간단한 XSS 문제가 있다고 생각해봅시다.\n자바스크립트 코드를 삽입해 악의적인 행위를 하는 공격 방법입니다.1 XSS CTF 문제에서는 의도한 악의적인 행위를 수행하면 플래그를 반환하게 됩니다.\n이때 여러 사람들이 동일한 XSS 문제 사이트 접속했을때 서로 영향을 받을까요? 그렇지 않습니다. 사용자는 자신만의 독립적인 환경에서 nginx가 반환하는 자바스크립트를 사용하고 있기 때문에 서로 영향을 받지 않게 되는 것입니다. 이런 문제 유형은 위에서 말한 1:N 방식을 사용하게 됩니다.\nSQL Injection 문제를 생각해봅시다. SQL Injection을 시도하기 위해 다양한 명령어를 시도하게 됩니다. 그 과정에서 문제를 해결한 사람의 데이터를 확인할 수 있게 됩니다. 이는 대회에 큰 영향을 주기 때문에 사용자마다 격리가 필요합니다.(서로 독립된 데이터베이스를 가져야 합니다.) 이런 문제 유형은 위에서 말한 1:1 방식을 사용하게 됩니다.\n주목할만한 특징 Live 1:1 서비스는 자원 낭비를 최소화하기 위해 문제가 실행된지 30분 후에 자동으로 삭제가 됩니다. 대부분의 CTF 문제 풀이 사이트는 특정 문제에 제한 시간을 두는 등 많이 사용하는 방식입니다.\nLive 1:1 서비스 : 실행한지 30분이 지나면 삭제되어야 한다. CTF 대회를 많이 경험해본 친구에게 제한 시간을 걸어두는 것에 대해 의견을 나눈 적이 있습니다. 친구가 말하길 중간에 시간 만료로 문제 사이트가 다운되더라도 일반적인 현상으로 받아들인다고 합니다.\n문제 컨테이너 초기 구성 (1) - Docker 사용 Python Docker SDK를 활용해서 도커를 생성하고 삭제할 수 있습니다. 초기에는 간단하게 Docker SDK를 사용하는 방식을 채택했습니다.\n컨테이너의 무한 증식을 막아라 동일한 프로젝트를 수행한 동아리원을 만나게 되었습니다. 프로젝트 관련된 이야기를 하다가 컨테이너의 무한 증식으로 인한 서버 과부하 현상 해결이 필요하다는 의견을 받았습니다.\n제가 예상한 문제의 원인은 다음과 같습니다.\n문제 생성을 요청할때 문제 컨테이너 실행 상태를 검증하지 않거나 저장된 상태와 실제 상태에 불일치가 발생했다. 컨테이너 이름에 사용자 식별값 + 난수가 섞여 있다. 문제 생성을 요청할때 문제 컨테이너 실행 상태를 검증하지 않았다면 데이터베이스에 값을 저장하면 됩니다. 문제는 저장된 상태와 실제 상태에 불일치가 발생을 어떻게 해결할 것인가 입니다.\n상태 불일치 해결방안은 간단하지만 구현하기 어렵다. 데이터베이스에 저장된 상태와 실제 상태의 불일치를 해결하는 방법은 주기적으로 대상을 확인하는 것이라고 생각합니다. 단순하게 하나의 애플리케이션만 주기적으로 상태를 확인하는 것은 비교적 구현이 쉽습니다. 그러나 컨테이너가 여러개인 경우에는 병렬 처리로 인해 구현 난이도가 올라가게 됩니다.\n상태 불일치를 확인해야 하는 대상은 Live이면서 한사람당 하나의 컨테이너가 배포되어야 하는 1:1 서비스일 것입니다. 그런데 시간 만료로 인해 문제 컨테이너는 자주 삭제되고 생성됩니다. 2개의 1:1 문제를 출제한다고 가정했을때 10명이 문제에 접속한다고 한다면 컨테이너의 총합은 절대 작은 숫자는 아닙니다.\n이러한 상황에서 한정된 자원 내에 일반적인 방법(순차적)으로 서비스를 찾고 대상을 지속적으로 확인하는 것은 비효율적입니다. 그러므로 병렬처리 또는 동시성 처리를 통해 효율성을 높이는 작업이 필요하게 됩니다.\n컨테이너 패키지 문제 SQL Injection 문제를 출제한다고 가정하겠습니다. nginx, server, db 컨테이너 3개가 필요하게 됩니다. 관리하는 사람 입장에서는 3개를 일괄적으로 관리하는 것이 효율적입니다.\n이와 같은 기능을 하는 것이 docker-compose입니다. docker-compose를 사용하여 여러개의 컨테이너를 하나의 서비스로 구성해서 관리할 수 있습니다.\n그러나 별도의 파이썬 라이브러리로 제공되지 않고 subprocess에 명령어 형태로 구현해야 합니다. 구현하더라도 잘 안되는 경우가 있습니다.\n사용자마다 고유한 컨테이너를 가져야 한다는 것은 다른 말로 각각 고유한 컨테이너 이름을 가져야 한다는 것과 같습니다. 결국 일반적인 docker-compose 만으로는 문제 컨테이너를 만드는데 한계가 있으며 원활한 처리를 위해서는 직접 구조를 정의해야 합니다.\n요구사항 컨테이너 상태를 지속적으로 확인 다양한 컨테이너를 하나의 구조로 직접 정의하고 관리 위의 요구사항을 충족하기 위해서는 기능을 처음부터 구현해야 합니다. 시간이 많이 소요되며 궁극적인 목표인 CTF 대회 개최와 멀어질 수 있다고 생각했습니다.\n다음화 - Kubernetes 사용 지금까지 CTF 문제 컨테이너를 docker로 설계하면서 느낀 한계점에 대해 설명했습니다.\n팀원들과 상의해본 결과 docker만으로는 우리가 원하는 기술을 구현할 수 없다고 결론을 지었습니다. 그 대신 주기적으로 리소스의 상태를 확인할 수 있으며 CR를 통해 커스텀으로 리소스를 정의할 수 있는 쿠버네티스를 선택하게 되었습니다.\n다음 글에서는 쿠버네티스를 통해 어떻게 설계했는지 설명하겠습니다.\nXSS(크로스 사이트 스크립트)란? 공격 유형부터 보안대책까지! | SK쉴더스\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-01-16T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-4/","title":"HexaCTF 4. Docker를 활용한 Challenge 설계 및 한계점"},{"content":"클라우드 로그 관리 사례를 보면서 문득 운영 중인 Openstack 서버에 로그가 얼마나 쌓였는지 궁금해졌습니다. 지금부터 그 과정을 알아가보는 시간을 가지겠습니다.\nOpensearch란? Elasticsearch 기반 오픈소스 검색 엔진 솔루션입니다. 검색 및 분석을 위한 강력한 도구를 제공하며 기존의 Elasticsearch의 기능을 사용할 수 있습니다. Elasticsearch가 제품의 라이센스를 변경하면서 Amazon이 기존 프로젝트를 fork하여 OpenSearch를 만들게 되었습니다. Elasticsearch 측에서 OpenSearch와 Elasticsearch 비교한 글이 있으니 참고 바랍니다.\nOpensearch 설치 Kolla-ansible은 Openstack를 구성하기 위한 다양한 배포 도구입니다. Ansible를 활용하여 컨테이너 형태로 컴포넌트를 설치합니다.\n자세한 사항은 공식문서를 참고하길 바랍니다.\n기본적으로 중앙 로깅 도구로 Opensearch를 지원해줍니다. globals.yaml 에 설정값을 수정하고 다시 배포하면 됩니다.\n1 enable_central_logging: \u0026#34;yes\u0026#34; 시각화 인터넷에서 OpenSearch 사용법과 관련된 글을 검색하면 생각보다 검색 결과가 적습니다. 하지만 OpenSearch 대신 Kibana로 검색하면 많은 내용을 확인할 수 있습니다.\n로그 데이터를 중심으로 두 개의 그래프를 시각화 할 것입니다.\nLog Level 분포도 시간에 따른 로그 총 개수 Log Level 분포도 Log Level 분포도는 다른 의미로 각각의 Log Level의 총 개수를 하나의 그래프에 표현하는 것과 같습니다. 그러므로 Metrics의 집계 기준을Count로 잡았습니다.\nlog_level.keyword 용어를 기준으로 그룹화해줍니다.\n아래와 같은 분포도를 확인할 수 있습니다.\n시간에 따른 로그 총 개수 시간 순서대로 집계가 되어야 하면서 매일 생성되는 로그 총합과 누적 로그 총합이 구분되면서 보여야 합니다. Area Chart로 설정하여 시각화를 수행했습니다. 크게 두 가지가 필요하므로 집계 기준을 Count, Cumulative Sum으로 설정했습니다.\n하루 단위로 데이터를 보여줄 수 있도록 Bucket를 설정했습니다.\n대략 2주치 로그의 누적 개수를 확인했습니다. 7,000,000개가 넘어가는 것을 보고 많이 놀랐습니다. 저장 공간에 불필요한 로그들이 쌓이는 것 같다는 생각이 들었고 관리 방안을 생각해야 했습니다.\n로그 관리 방안 방향성 잡기 로그는 서버의 로컬 저장소에 저장되기 때문에 저장 용량 관리 측면에서 중요합니다.\nKolla-ansble의 opensearch 로그 파일 경로/var/log/kolla/opensearch 를 보면 시간별로 로그 파일이 압축되어 있습니다.\n2달치 압축된 로그의 총 용량을 보면 31MB로 상당히 적습니다.\n1 du -ch *.gz | grep total 로그를 보니 Public Cloud의 로그 관리 사례가 생각났습니다. AWS 경우에는 Object Storage인 S3에 로그를 저장하여 Lifecycle를 관리하기도 합니다.\nOpenstack도 swift라는 Object Storage가 있습니다. SSD로 구성된 로컬 저장소에 로그가 쌓이는 것을 원치 않기 때문에 HDD로 구성된 swift에 로그를 옮기고 싶었습니다. 기본적인 구성으로는 swift에 전송할 수 없으며 추가적인 플러그인 사용을 고려해야 합니다. 그러나 현재까지 공식적으로 swift로 데이터를 전송하는 플러그인은 없는 것 같습니다. 그렇다면 보유(retention) 정책을 확인하는 것으로 방향을 잡았습니다.\nState management policies 아래 보유 정책은 크게 3가지 상태가 있습니다.\nopen close delete 이름만 다를 뿐이지 Hot-Warm-Cold 라고 보시면 됩니다. 기본 설정을 보면 open 상태로 로그가 생성되다가 30일 후에 close 상태로 변화하고 60일 후에 delete 상태로 변화가 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 { \u0026#34;policy\u0026#34;: { # ... \u0026#34;states\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;actions\u0026#34;: [], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;close\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;30d\u0026#34; }, }, ], }, { \u0026#34;name\u0026#34;: \u0026#34;close\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;retry\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;backoff\u0026#34;: \u0026#34;exponential\u0026#34;, \u0026#34;delay\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;close\u0026#34;: {}, }, ], \u0026#34;transitions\u0026#34;: [ { \u0026#34;state_name\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;min_index_age\u0026#34;: \u0026#34;60d\u0026#34; }, }, ], }, { \u0026#34;name\u0026#34;: \u0026#34;delete\u0026#34;, \u0026#34;actions\u0026#34;: [ { \u0026#34;retry\u0026#34;: { \u0026#34;count\u0026#34;: 3, \u0026#34;backoff\u0026#34;: \u0026#34;exponential\u0026#34;, \u0026#34;delay\u0026#34;: \u0026#34;1m\u0026#34; }, \u0026#34;delete\u0026#34;: {}, }, ], \u0026#34;transitions\u0026#34;: [], }, ], } } 최종적으로 보유 정책을 수정하지 않았습니다. 로그는 성능 정보에 대한 집계나 디버깅할때 유용합니다. 이때 적절한 과거 데이터를 보관하는 것이 중요하다고 생각합니다. 생각보다 close 상태의 압축된 로그 크기가 작았기 때문에 더욱 줄일 필요는 없었다고 봅니다.\n마무리 이 글은 취준생일때 작성한 글입니다. 실무에는 어떻게 다룰지는 모르겠습니다. 언젠간 취업하고 나서 잘못된 부분을 확인하면 새로운 글로 찾아오겠습니다.\n","date":"2025-01-15T00:00:00+09:00","permalink":"https://s0okju.github.io/p/server-story/","title":"서버 이야기. Kolla-ansible 를 활용한 기본 Openstack 로깅 대시보드 구성"},{"content":"Jenkins 환경 구성 어디에 젠킨스를 설치해야 할까? 쿠버네티스를 처음 공부했을때 “쿠버네티스 내에 데이터베이스를 운영하는 것이 과연 안전할까?”에 대해 의문점을 가지게 되었습니다. 데이터 손실이 우려된다는 과거의 의견과 달리 현재는 다양한 기업들이 DBaaS를 도입하고 있었습니다.12\n이런 흐름으로 젠킨스 또한 쿠버네티스 내에 운영할까요? 레딧과 같은 지식 공유 플랫폼을 종합해보면 쿠버네티스 내에 젠킨스를 운영하는 경우가 많이 보입니다.3\n그러나 저희 프로젝트는 CI를 담당하는 팀원은 따로 있습니다. 완전한 역할 분리를 위해 인스턴스에 따로 젠킨스를 설치하게 되었습니다.\nJenkins 배포 아래는 대략적인 아키텍쳐입니다. 여기서 중요한 점은 Jenkins 인스턴스와 쿠버네티스 노드와 연결되어야 합니다. 각각의 서브넷이 라우터로 연결하여 서로 통신할 수 있도록 설정해야 합니다.\nJenkins를 인스턴스로 배포하고 외부 IP(Floating IP)로 연결하는 부분은 생략하겠습니다. 이전 글에서 언급한 것처럼 하나의 라우터에 여러 개의 포트를 연결하고 싶다면 openstack_networking_port_v2 함수를 활용해야 한다고 말했습니다. 이도 동일하게 Jenkins가 포함되어 있는 서브넷에 포트를 생성하여 쿠버네티스 라우터에 연결합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 resource \u0026#34;openstack_networking_port_v2\u0026#34; \u0026#34;devops_kubernetes_router_port\u0026#34; { name = \u0026#34;kubernetes-devops-port\u0026#34; network_id = openstack_networking_network_v2.devops_network.id admin_state_up = \u0026#34;true\u0026#34; fixed_ip { subnet_id = openstack_networking_subnet_v2.devops_subnet.id ip_address = var.devops_port_address } } resource \u0026#34;openstack_networking_router_interface_v2\u0026#34; \u0026#34;devops_kubernetes_interface\u0026#34; { router_id = openstack_networking_router_v2.kubernetes_router.id port_id = openstack_networking_port_v2.devops_kubernetes_router_port.id } 배포를 모두 끝내면 아래와 같은 토폴로지를 확인할 수 있습니다.\nHarbor 쿠버네티스 환경에서 CI 쿠버네티스 환경에서 CI 과정은 컨테이너를 빌드하고 이미지를 저장하는 프로세스가 포함됩니다. 그 중 이미지를 저장하기 위해서는 도커 레지스트리가 필요합니다.\n도커 레지스트리 도커 레지스트리는 도커 이미지를 관리하는 저장소 입니다. 저장소는 크게 public과 private이 있습니다. 내부적으로 사용할 예정이므로 private 레지스트리를 선택하게 되었습니다.\n후보는 크게 두 가지입니다.\nDocker Registry Harbor 도커 레지스트리 선정 기준은 “깔끔한 UI를 가지고 있는가?” 입니다. 후보 중에는 기준에 부합한 플랫폼은 Harbor였습니다.\n설치 공식 문서를 참고해서 helm 파일을 다운로드 받습니다.\n1 2 helm repo add harbor https://helm.goharbor.io helm fetch harbor/harbor --untar 가장 먼저 해야하는 것은 Volume에 대한 설정입니다. Harbor는 다양한 PVC를 요청합니다. 프로젝트는 Dynamic Provisioning을 위한 NFS 환경을 구축했습니다. 그러므로 values.yaml 에 strageClass만 값을 입력하면 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 persistentVolumeClaim: registry: # Use the existing PVC which must be created manually before bound, # and specify the \u0026#34;subPath\u0026#34; if the PVC is shared with other components existingClaim: \u0026#34;\u0026#34; # Specify the \u0026#34;storageClass\u0026#34; used to provision the volume. Or the default # StorageClass will be used (the default). # Set it to \u0026#34;-\u0026#34; to disable dynamic provisioning storageClass: \u0026#34;nfs-client\u0026#34; subPath: \u0026#34;\u0026#34; accessMode: ReadWriteOnce size: 5Gi annotations: {} jobservice: jobLog: existingClaim: \u0026#34;\u0026#34; storageClass: \u0026#34;nfs-client\u0026#34; subPath: \u0026#34;\u0026#34; accessMode: ReadWriteOnce size: 1Gi annotations: {} 환경 설정을 다 하고 배포하면 정상적으로 작동됩니다.\nJenkins — Harbor 연결하기 HTTP로 연결하기 젠킨스 인스턴스가 쿠버네티스 내부에 있는 Harbor에 이미지를 저장해야 합니다. 저희 환경에서는 Harbor를 nodePort로 연결하여 도커 레지스트리에 연결할 수 있도록 설정하겠습니다.\nHarbor와 같은 도커 레지스티의 경우 HTTPS로 연결하는 것을 권장합니다. 하지만 저희는 HTTP로 설정하겠습니다. Harbor의 우선 nodePort를 30002로 설정한 후 externalURL 에 http가 포함된 URL를 설정해야 합니다.\n1 externalURL: http://10.0.10.156:30002 젠킨스 인스턴스에서 /etc/docker/daemon.json 파일에 HTTP를 적용할 harbor의 host를 명시해줍니다.\n1 { \u0026#34;insecure-registries\u0026#34;: [\u0026#34;http://10.0.10.156:30002\u0026#34;] } 도커를 재시작하면 위의 설정이 적용됩니다.\nJenkins Pipeline 설정 Credential 설정등은 생략하겠습니다. 도커를 빌드하기 위해서는 크게 3가지 과정이 필요합니다.\n도커 빌드 도커 이미지 Tagging 도커 이미지 Push 젠킨스는 프로젝트 단위로 이미지를 관리합니다. 아래의 코드는 ubuntu 이미지를 빌드하여 test 프로젝트에 이미지를 push하는 것을 보여줍니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 pipeline { agent any environment { DOCKER_REGISTRY = \u0026#34;10.0.10.156:30002\u0026#34; IMAGE_NAME = \u0026#34;ubuntu\u0026#34; TARGET_REPO = \u0026#34;test/ubuntu\u0026#34; } stages { stage(\u0026#39;Pull Ubuntu Image\u0026#39;) { steps { echo \u0026#34;Pulling the Ubuntu image from Docker Hub...\u0026#34; sh \u0026#34;docker pull ${IMAGE_NAME}:latest\u0026#34; } } stage(\u0026#39;Tag Image\u0026#39;) { steps { echo \u0026#34;Tagging the image for Harbor...\u0026#34; sh \u0026#34;docker tag ${IMAGE_NAME}:latest ${DOCKER_REGISTRY}/${TARGET_REPO}:latest\u0026#34; } } stage(\u0026#39;Push Image to Harbor\u0026#39;) { steps { echo \u0026#34;Pushing the image to Harbor...\u0026#34; withDockerRegistry([credentialsId: \u0026#39;harbor-credentials\u0026#39;, url: \u0026#34;http://${DOCKER_REGISTRY}\u0026#34;]) { sh \u0026#34;docker push ${DOCKER_REGISTRY}/${TARGET_REPO}:latest\u0026#34; } } } } post { always { echo \u0026#34;Cleaning up Docker images...\u0026#34; sh \u0026#34;docker rmi ${IMAGE_NAME}:latest || true\u0026#34; sh \u0026#34;docker rmi ${DOCKER_REGISTRY}/${TARGET_REPO}:latest || true\u0026#34; } } } 이미지를 push하면 정상적으로 작동됨을 확인할 수 있습니다.\nhttps://sungchul-p.github.io/db-on-k8s-case-study\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://engineering.linecorp.com/ko/blog/declarative-cloud-db-service-using-kubernetes\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.reddit.com/r/devops/comments/m3lc3r/jenkins_inside_kubernetes_or_not/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-01-05T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-3/","title":"HexaCTF - 3. Jenkins를 활용한 CI 환경 구성하기"},{"content":"대학교 마지막 학년이자 첫 회고록이기도 하다. 작년 보안 공부를 해오면서 시스템에 대한 기초가 부족하다고 생각했었다. 그래서 보안이 아닌 개발 공부를 처음부터 다시 시작했다.\n상반기 개인 공부 상반기에 주로 했던 공부는 Go 언어, 쿠버네티스 그리고 오픈스택이다. Go는 한번이라도 쿠버네티스 코드를 분석하고 싶다는 마음으로 시작했다. 기본적인 이론을 익히고 웹 서버를 개발했다. 비록 시작을 마이크로서비스로 구현해서 깔끔하게 마무리 하지 못했지만 Go 언어에 대한 애정은 생겼다.\nTodopoint 서버코드 개선기 - 1. 공동 에러 처리하기 내가 만든 Go 웹 애플리케이션을 쿠버네티스에 배포하고 싶었다. 처음에는 클라우드를 활용하고 싶었지만 생각보다 CSP의 쿠버네티스는 비싸서 포기했다. 마침 집에 쓰지 않은 데스크톱이 있었고 전부 포맷하여 오픈 스택 서버를 구축하게 되었다. Multipass -\u0026gt; devstack -\u0026gt; Kolla ansible 순으로 중간에 배포 플랫폼을 바꾸는 과정에서 내가 리눅스의 리도 모르구나를 알게 되었다. 환경 세팅을 마무리하는데 정말 많은 시간이 걸렸다. 대략 한달 정도 걸렸지만 중간에 다양한 오류를 마주하면서 문제를 해결하는 방법을 터득한 것 같다.\nCategory: Ops - S0okJu.dev 쿠버네티스는 긴 시간동안 천천히 책을 읽어가면서 수행했던 것 같다. 지금 생각해보면 가장 아쉬운 공부가 쿠버네티스이지 않을까 싶다.\n스터디 - 대규모 시스템 설계 1,2 위의 책은 현직자가 많이 읽는 것으로 알고 있다. 그래도 큰 그림을 알고 싶었고, 현직자분들로 구성되어 있는 대규모 시스템 설계 책 스터디에 참여하게 되었다. 솔직히 말하자면 어려웠다. 다른 스터디원들은 자신의 경험담과 함께 내용을 설명했지만 본인은 함께 설명할 만한 것이 없었다. 그래서 관련 기술 블로그를 찾아서 내용을 첨부했다. 학생때는 기술적인 것만 집중해서 수행한다면 현직에 종사하시는 분들은 서비스의 도메인 특징에 맞게 기술을 선택한 것이 큰 차이점이라고 생각했다. 그래서 스터디원들의 의견 하나하나가 중요했고 배울점이 많았다.\n기타 활동 아는 대학교 지인의 웹 서버를 개발하고 있으며 인증 로직이 필요하다고 말씀하셨다. 그래서 짧게나마 Spring Boot를 공부하고 Google OAuth 처리를 개발했다. 해당 프로젝트 팀장이 스프링 부트에 열정적인 사람이었는데 친절하게 알려준 덕분에 스프링 부트 공부를 수월하게 할 수 있었다.\nSpring - Spring Boot 간략하게 알아보기 자바와 같은 클래스 지향 언어는 자주 사용해보지 않아서 Interface는 언제 쓰며 어떤 것을 클래스로 작성해야 할까?에 대해 고민을 많이 했었다. 그래서 다른 사람들의 깃허브 레포지토리를 찾아가면서 어떻게든 해결했다.\n하반기 상반기에 했던 모든 것들을 하나로 \b직무로는 SRE가 되고 싶었다. 학교를 다니고 있는 입장에서 어느 기업에 소속되어 활동하는 것은 한계가 있었다. 그래서 시나리오 기반으로 시스템 아키텍쳐를 구성하고 필요한 기술들을 활용해보자! 라는 생각을 했다. 그리고 쇼핑몰이라는 가상의 도메인을 설정하여 차근차근 한 단계씩 시스템을 성장해보자는 마음으로 프로젝트를 수행했다.\nProjects – S0okJu Tech Labs – Medium 의 OpsAthlan 시리즈 여담으로 마이크로서비스 웹 서버 개발 프로젝트는 선택에 근거는 무조건 있어야 한다는 뼈저린 조언을 남겼다. 그래서 차근차근 성장하는 시나리오를 설정하게 된 것이다.\n왜 TodoPoint 프로젝트를 실패했는가? 클라우드 자격증 그 이전까지 프라이빗 클라우드(OpenStack)을 집중적으로 공부했지만 현실적으로 많은 기업들이 퍼블릭 클라우드 사용 기술을 요구한다. 다행히 프라이빗 클라우드를 통해 전반적인 내용은 알고 있어서 공부하는데 수월했다. 공부의 일환으로 관련 자격증을 취득했다.\nNCP Associate AWS Solution Architect 자격증 공부는 퍼블릭 클라우드를 이해하는데 큰 도움이 되었다. AWS는 상황을 우선 제시하고 해결 방안을 선택해야 하는 문제가 많았다. 수 많은 리소스에 대한 포괄적인 이해와 활용 능력이 필요했고 이를 연습하게 된 계기가 되었다.\n아쉬운 부분을 보완하다. 상반기를 보면 쿠버네티스 공부가 부족한 것에 대해 아쉬움이 남았다고 말했다. 한 해가 마무리 되던 시기에 쿠버네티스 관련 프로젝트를 수행하게 되었다. 쿠버네티스 오퍼레이터 제작과 컨테이너 모니터링 환경 구성 담당이 되었다. 자세한 내용은 2025년 블로그에서 확인할 수 있을 것이다.\n올해의 평점 올해는 모든 과정이 의미가 있었다. 그래서 별점으로 ⭐️⭐️⭐️⭐️ 4점을 주고 싶다.\n","date":"2024-12-23T00:00:00+09:00","permalink":"https://s0okju.github.io/p/2024-review/","title":"2024년 회고"},{"content":"전체 아키텍쳐 구성도 저희 프로젝트는 크게 3개의 서브넷을 가지게 됩니다.\nKubernetes Subnet Kubernetes Master, Worker nodes Storage Subnet NFS Server DevOps Subnet Jenkins Server NFS 아키텍쳐 구성 왜 필요할까? 상태값 저장 저희 프로젝트는 쿠버네티스 환경에서 수행하게 됩니다. 그 과정에서 데이터베이스와 같이 상태를 저장해야 하는 애플리케이션이 필요하게 됩니다. 관리자는 PV로 스토리지를 정의하고 상태 저장이 필요한 애플리케이션의 PVC를 활용하여 데이터를 영구적으로 저장하게 됩니다. PV는 로컬 디스크에 저장하거나 NFS와 같은 통신이 필요한 스토리지 시스템으로 정의할 수 있습니다. 로컬 디스크에 저장한다는 것은 Worker node의 디스크에 저장한다는 말과 같습니다. 그러므로 데이터 저장소를 외부와 분리하고자 NFS를 사용하게 되었습니다.\nDynamic Provisioning NFS를 선택한 핵심적인 이유입니다. 애플리케이션이 스토리지 사용을 위해 PVC를 요청하게 되면 요구사항에 맞는 PV에 바인딩하게 됩니다. 모든 작업마다 그에 상응하는 PV-PVC 쌍을 만들어야 한다는 것과 같습니다. 불편함을 해소하기 위해서 PVC 요청에 대한 PV를 생성하는 과정을 자동화는 Dynamic Provisioning을 활용하게 되었습니다.\nNFS in Nova vs Manila 데이터베이스를 구축하는 방법으로 EC2 Instance에서 DB 엔진을 직접 설치하듯이 NFS로 직접 인스턴스에 직접 설치할 수 있는 방법이 있습니다. Openstack에는 NFS를 직접 설치할 수 있도록 Manila라는 파일 공유 서비스를 제공해줍니다.\n공식문서에 의하면 NFS는 아래와 같은 기능을 목표로 삼습니다.1\nComponent based architecture: Quickly add new behaviors Highly available: Scale to very serious workloads Fault-Tolerant: Isolated processes avoid cascading failures Recoverable: Failures should be easy to diagnose, debug, and rectify Open Standards: Be a reference implementation for a community-driven api 그 이외에도 통합적인 대시보드, 모니터링, 로깅 도구를 활용할 수 있다는 것이 큰 장점입니다. 그럼에도 Manila가 아닌 Nova에 NFS를 설치하는 방안을 선택했습니다. 선택한 가장 큰 이유는 어렵다는 것입니다. 프로젝트 이전에 데이터베이스 시스템인 trove를 설치한 경험이 있었습니다. 원인을 찾았지만 해결하지 못한 경험이 있었습니다. 클라우드에 적합한 시스템이라 네트워크, 인증 등 고려사항이 많다고 느꼈습니다. 소규모에 쉬운 방법으로 하나의 인스턴스에서 nfs-server에 설치하는 방식을 선택했습니다.\nNFS 설치하기 설치하기 이전에 NFS 요구사항에 대해 생각해보겠습니다.\nNFS 네트워크 요구사항 쿠버네티스 서브넷과 통신해야 한다. nfs server를 설치하기 위해 외부와 통신해야 한다. 위의 요구사항에 맞게 storage subnet은 public, kubernetes subnet의 router로 연결했습니다.\nTerraform 으로 구성하기 네트워크 storage라는 prefix를 가진 network와 subnet를 만들어줍니다. 그리고 라우터를 정의하여 서로 다른 네트워크를 연결합니다. Openstack의 Neutron의 경우에는 크게 가상 라우터와 서브넷의 연결 관계로 구성됩니다. 이는 실제 물리적인 네트워크 환경처럼 라우터 인터페이스에 네트워크 케이블을 꽂는 형식과 유사합니다. 그래서 Neutron 내부에서는 서브넷이 할당된 네트워크에 논리 포트를 만들고 이를 라우터에 연결하게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Get the existing public network data \u0026#34;openstack_networking_network_v2\u0026#34; \u0026#34;public_network\u0026#34; { name = var.public_network_name } # Get the existing public router data \u0026#34;openstack_networking_router_v2\u0026#34; \u0026#34;public_router\u0026#34; { name = var.public_router_name } # Create the storage network resource \u0026#34;openstack_networking_network_v2\u0026#34; \u0026#34;storage_network\u0026#34; { name = var.storage_network_name } # Create the storage subnet resource \u0026#34;openstack_networking_subnet_v2\u0026#34; \u0026#34;storage_subnet\u0026#34; { name = var.storage_subnet_name network_id = openstack_networking_network_v2.storage_network.id cidr = var.storage_subnet_cidr ip_version = 4 dns_nameservers = var.dns_nameservers } # Connect the storage subnet to the public router resource \u0026#34;openstack_networking_router_interface_v2\u0026#34; \u0026#34;storage_router_interface\u0026#34; { router_id = data.openstack_networking_router_v2.public_router.id subnet_id = openstack_networking_subnet_v2.storage_subnet.id } 서브넷을 라우터에 연결하기 위해서는 openstack_networking_router_interface_v2함수를 사용하게 됩니다. 서브넷을 연결할 때 subnet_id를 사용할 수 있고 port_id를 사용할 수 있습니다.\nsubnet_id를 사용하게 되면 자동으로 해당 CIDR의 x.x.x.1 의 주소의 포트로 연결됩니다. 그러나 하나의 서브넷이 2개 이상일 경우 subnet_id를 두 번이상 사용하게 된다면 사용중인 port IP라는 에러 메세지를 확인할 수 있습니다.\n1 2 3 4 5 # Subnet_id를 사용하는 방법 resource \u0026#34;openstack_networking_router_interface_v2\u0026#34; \u0026#34;storage_router_interface\u0026#34; { router_id = data.openstack_networking_router_v2.public_router.id subnet_id = openstack_networking_subnet_v2.storage_subnet.id } 그럴 경우 openstack_networking_port_v2 에서 사용할 port_ip를 지정해서 사용할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Port를 사용하는 방법 resource \u0026#34;openstack_networking_port_v2\u0026#34; \u0026#34;storage_kubernetes_router_port\u0026#34; { name = \u0026#34;kubernetes-storage-port\u0026#34; network_id = openstack_networking_network_v2.storage_network.id admin_state_up = \u0026#34;true\u0026#34; fixed_ip { subnet_id = openstack_networking_subnet_v2.storage_subnet.id ip_address = \u0026#34;10.0.50.20\u0026#34; } } resource \u0026#34;openstack_networking_router_interface_v2\u0026#34; \u0026#34;storage_kubernetes_interface\u0026#34; { router_id = openstack_networking_router_v2.kubernetes_router.id port_id = openstack_networking_port_v2.storage_kubernetes_router_port.id } 컴퓨팅 자원 NFS 서버와 관련된 인스턴스를 제작해보겠습니다. NFS 서버는 2049, 111 포트를 활용합니다. NFS 서버는 쿠버네티스 서브넷과 주로 통신하므로 Kubernetes subnet인 10.0.10.0/24에서 요청하는 2049, 111 TCP, UDP 네트워크를 모두 허락해줍니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # Create the NFS security group resource \u0026#34;openstack_networking_secgroup_v2\u0026#34; \u0026#34;nfs_secgroup\u0026#34; { name = \u0026#34;${var.storage_instance_name}-nfs-secgroup\u0026#34; description = \u0026#34;Security group for NFS access\u0026#34; } resource \u0026#34;openstack_networking_secgroup_rule_v2\u0026#34; \u0026#34;nfs_ingress\u0026#34; { direction = \u0026#34;ingress\u0026#34; ethertype = \u0026#34;IPv4\u0026#34; protocol = \u0026#34;tcp\u0026#34; port_range_min = 2049 port_range_max = 2049 remote_ip_prefix = \u0026#34;10.0.10.0/24\u0026#34; security_group_id = openstack_networking_secgroup_v2.nfs_secgroup.id } # ... secgroup 일부 생략 # Create a port in the storage network for the instance resource \u0026#34;openstack_networking_port_v2\u0026#34; \u0026#34;storage_port\u0026#34; { name = \u0026#34;${var.storage_instance_name}-port\u0026#34; network_id = openstack_networking_network_v2.storage_network.id security_group_ids = [openstack_networking_secgroup_v2.nfs_secgroup.id] fixed_ip { subnet_id = openstack_networking_subnet_v2.storage_subnet.id } } # Create a floating IP for the storage instance resource \u0026#34;openstack_networking_floatingip_v2\u0026#34; \u0026#34;storage_fip\u0026#34; { pool = var.public_network_name } # Associate the floating IP with the storage instance\u0026#39;s port resource \u0026#34;openstack_networking_floatingip_associate_v2\u0026#34; \u0026#34;storage_fip_assoc\u0026#34; { floating_ip = openstack_networking_floatingip_v2.storage_fip.address port_id = openstack_networking_port_v2.storage_port.id } 그리고 50GB 블록 스토리지(Cinder)를 NFS 인스턴스에 붙여서 생성합니다. 인스턴스를 만들때 이전에 정의한 보안 그룹도 지정해줍니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 resource \u0026#34;openstack_blockstorage_volume_v3\u0026#34; \u0026#34;nfs-volume\u0026#34; { name = \u0026#34;bfs-volume\u0026#34; size = 50 } # Create the storage instance resource \u0026#34;openstack_compute_instance_v2\u0026#34; \u0026#34;storage_instance\u0026#34; { name = var.storage_instance_name flavor_name = var.storage_flavor_name image_name = var.storage_image_name key_pair = openstack_compute_keypair_v2.nfs_keypair.name security_groups = [openstack_networking_secgroup_v2.nfs_secgroup.name] network { port = openstack_networking_port_v2.storage_port.id } metadata = { ssh_user = \u0026#34;ubuntu\u0026#34; } } resource \u0026#34;openstack_compute_volume_attach_v2\u0026#34; \u0026#34;nfs-volume-attached\u0026#34; { instance_id = openstack_compute_instance_v2.storage_instance.id volume_id = openstack_blockstorage_volume_v3.nfs-volume.id } NFS는 Private Network 환경에만 사용합니다. 그러나 nfs 설정 시 외부와의 접근이 필요하므로 floating ip를 할당했습니다.\nNFS 서버 설정 환경 설정 NFS 서버는 50GB 블록 스토리지와 연결된 인스턴스로 이뤄져 있습니다. Horizon에서 인스턴스의 정보를 보면 부착된 볼륨을 확인할 수 있습니다.\nssh에 접속해보면 마운팅된 볼륨을 확인할 수 있습니다.\n추후 쿠버네티스에서 사용될 모든 데이터는 볼륨 스토리지에 저장할 것입니다. 그러므로 볼륨을 ext4 형태로 포맷하여 /mnt/data 경로에 마운팅합니다.\nnfs-server 라이브러리를 설치한 후 /etc/exports 파일을 수정합니다. 쿠버네티스 서브넷이 블록 스토리지에 있는 /mnt/data 에 읽고 쓸 수 있도록 권한을 설정합니다.\n1 /mnt/data 10.0.10.0/24(rw,sync,no_subtree_check) 자세한 사항은 [Ubuntu] NFS 서버 및 클라이언트 구성 - Dongle’s 개발노트 를 참고하시길 바랍니다.\n테스트하기 우선 쿠버네티스에서 NFS 서버 IP인 10.0.50.170가 정상적으로 통신한지 확인합니다.\nNFS 서버에 Hello World 문자열이 있는 test.txt 파일을 정의합니다.\n마운팅에서 확인해보면 데이터가 정상적으로 저장되었음을 확인할 수 있습니다.\n1 sudo mount 10.0.50.170:/mnt/server /mnt 활용하기 - Harbor in Kubernetes Harbor를 설정하기 이전에 Dynamic provisioner를 설치합니다. nfs-subdir-external-provisioner 를 사용했으며 helm을 통해 쉽게 설치했습니다.\n1 2 3 helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=10.0.50.170 \\ --set nfs.path=/mnt/data Harbor Kubernetes Helm 설치 — Jeongchul Kim 블로그를 참고하여 harbor를 설치했습니다.\nvalues.ym의 persistence 에서 nfs-subdir-external-provisioner에서 생성한 storageClass를 정의합니다. 그리고 실행되면 정상적으로 작동됩니다.\n만약에 설치하다가 아래와 같이 helper program이 필요하다는 에러 메세지를 볼 수 있습니다.\n1 Output: mount: /var/lib/kubelet/pods/.../volumes/kubernetes.io~nfs/nfs-pv-test: bad option; for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.\u0026lt;type\u0026gt; helper program. 실제로 쿠버네티스에서 Mounting을 할때 mount 명령어를 사용합니다. 이는 nfs-common 패키지에 있는 명령어로 노드에 nfs-common 를 직접 설치해야 합니다.\n마치며 지금까지 NFS를 구축하기 위해 Terraform으로 인프라 환경을 설정하고 인스턴스에 nfs 서버를 설정했습니다. 다음에는 Jenkins 인프라 환경을 구성하고 어떻게 Harbor Registry에 빌드한 도커 이미지를 Push하는지 작성하겠습니다.\nOpenStack Shared Filesystems (manila) documentation — manila 19.1.0.dev52 documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-11-29T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-2/","title":"HexaCTF - 2. Openstack Terraform을 활용한 NFS 서버 설치 및 활용"},{"content":"CTF 프로젝트 CTF란 해킹 대회로 참가자가 의도적으로 취약한 프로그램이나 웹사이트에 숨겨진 플래그를 찾는 게임1입니다. 대표적인 CTF 사이트로는 드림핵이 있습니다. 현재 해킹 동아리 소속으로 주변에 CTF에 활발하게 참가하는 친구들이 많이 있습니다. 저 또한 CTF는 아니지만 문제 제작에 관심이 많습니다. 해킹이라는 관심사를 가진 인원이 모여 CTF에 대한 아이디어를 나누게 되었고, 이후 직접 사이트를 구축하고 대회를 개최하기로 결정했습니다.\nCTF 제작 필수 요소 - 컨테이너 CTF 문제 중에는 호스팅된 취약한 웹사이트가 필요할 수 있고, 취약한 운영체제 환경이 필요할 수 있습니다. 각각 하나의 환경을 구성해서 사용자에게 배포할 수 있습니다. 이럴경우 다른 사용자의 문제 풀이를 알 수 있고 다른 사용자가 발빠르게 문제를 해결하면 다른 사용자는 문제를 풀 수 없게 됩니다. 그렇다면 해킹 대회의 의미가 없어질 것입니다. 문제 환경의 분리가 필요하다고 판단했고 경량화된 실행 환경인 컨테이너를 사용하기로 결정했습니다.\n쿠버네티스 쿠버네티스는 러닝 커브가 있는 플랫폼입니다. 저 또한 쿠버네티스의 기본 개념만 알지 온전히 사용해본 경험은 없었습니다. 하지만 컨테이너를 관리한다는 측면에서 쿠버네티스만큼 좋은 플랫폼이 없어서 선택하게 되었습니다.\nPython Docker SDK에서 서비스를 개발하고 있을때 제가 고민했던 것이 있었습니다.\n30분 후에는 컨테이너를 중단시켜야 하는데 어떻게 해야하지? 2개 이상의 컨테이너가 필요한 경우에는 어떻게 논리적으로 분리해서 관리하고 모니터링하지? Health Check 어떻게 하지? 모든 고민의 해결방안은 직접 구현해야 한다는 것이었습니다. 직접 구현하는 것보다는 증명된 플랫폼을 활용하는 것이 적절하다고 판단했고 쿠버네티스를 선택하게 되었습니다.\n운영 플랫폼 선정 - Openstack OpenStack이란이란 풀링된 가상 리소스를 사용하여 프라이빗 및 퍼블릭 클라우드를 구축하고 관리하는 오픈소스 플랫폼를 뜻합니다.2 이번 프로젝트에서 인프라 담당을 맡게 되면서 홈서버를 가지고 CTF를 운영하게 되었습니다. 저는 홈서버에 Openstack를 구축하고 활용한 경험이 있습니다. Openstack를 사용하면서 가장 편리했던 부분은 IaC를 통해 인프라를 쉽게 관리할 수 있다는 점이었습니다. 또한 다양한 스토리지 종류가 있으며 역할에 따라 활용할 수 있다는 것도 큰 매력이었습니다. 운영 난이도는 높지만 클라우드만이 가질 수 있는 장점을 활용하고 싶어 Openstack을 선택하게 되었습니다.\n서버 구축 - 제한된 환경에서 느낀 것 집에 있는 홈서버를 다른 장소로 이전하게 되었습니다. 이후 Openstack 서버를 현재 환경에 맞게 설정해야 했습니다. 작업에는 네트워크 설정이 큰 비중을 차지했습니다. 설정 과정에서 느낀점을 적어보고자 합니다.\nNAT Gateway 의 필요성 Public Cloud에서 Private Subnet 내에 있는 인스턴스가 인터넷과 접속하기 위해서는 NAT Gateway가 필요합니다.\n위의 상황과 유사하게 서버의 사설 네트워크 대역에 접근해야 할 일이 있었습니다. 예로 172.19.0.2에 접근하고 싶은데 같은 네트워크 대역에 있지 않으므로 접근할 수 없습니다. 그러나 enp2s0 네트워크 인터페이스를 NAT로 사용하면 172.19.0.2에 접근할 수 있게 됩니다.\n서버에 enp2s0가 NAT이며 사설 네트워크인 enp2s0.191에 트래픽을 허용해주는 방화벽 규칙을 추가합니다.\n1 2 3 4 5 6 # enp2s0 인터페이스를 통해 나가는 모든 트래픽의 소스에 NAT 수행 sudo iptables -t nat -A POSTROUTING -o enp2s0 -j MASQUERADE # enp2s0에 들어와 br-ex로 나가는 ICMP 트래픽을 허용한다. sudo iptables -A FORWARD -i enp2s0 -o enp2s0.191 -p icmp -j ACCEPT # br-ex로 들어와서 enp2s0로 나가는 ICMP 트래픽을 허용한다. sudo iptables -A FORWARD -i enp2s0.191 -o enp2s0 -p icmp -j ACCEPT Private Subnet 내에 있는 리소스가 외부 인터넷과 통신할 수 있도록 NAT Gateway가 필요합니다. 이는 온프로미스 환경에서도 해당됩니다. 온프로미스 환경에서 사설 네트워크에 접근하기 위해서는 NAT 기능이 필요하며 이는 공용(Public) 네트워크 인터페이스로 구성되어 있어야 합니다. 그래야 외부와 연결하여 통신할 수 있게 됩니다. Public Cloud에서 NAT Gateway를 Public Subnet에 배치하는 이유도 온프로미스와 동일할 것입니다.\n현재 프로젝트에서는 NAT를 활용하지 않습니다. vlan을 외부 네트워크 인터페이스로 활용하고 싶었으나 실패했습니다. 결국 enp2s0를 Public IP 할당에 사용되는 외부 네트워크 인터페이스(br-ex)로 사용할 것입니다.\nIP 부족 새로운 장소에서 가장 큰 고민거리는 IP Pool 설정이었습니다. Openstack은 Floating IP Pool 내에서 외부에 노출할 IP를 할당받게 됩니다. 실제 운영을 위한 IP, 개발을 위한 IP 등 생각보다 IP를 할당받아야 하는 경우가 많습니다. 현재 환경에서는 다양한 서버의 IP가 불규칙적으로 할당되어 있습니다. 또한 많은 사람이 인터넷을 활용하는 공간이기에 무분별하게 IP를 할당할 수 없었습니다.\n꼭 필요한 IP 개수에서 일시적으로 할당받을 것을 고려해 10개를 할당했습니다.\n쿠버네티스 : Master, node 2개 개발 관련 : Jenkins 기타(일시적) : MySQL 관리용 등등.. 마치며 지금까지 프로젝트가 왜 이러한 플랫폼을 선택했는지 설명했습니다. 또한 서버를 다른 곳에 이전하게 되면서 생긴 경험을 보여드렸습니다. 다음편에는 전체적인 구성도와 함께 찾아오겠습니다.\nhttps://en.wikipedia.org/wiki/Capture_the_flag_(cybersecurity)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.redhat.com/ko/topics/openstack\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-11-25T00:00:00+09:00","permalink":"https://s0okju.github.io/p/hexactf-1/","title":"HexaCTF - 1. 인프라 플랫폼 선정과 서버 이전 시 마주한 문제 "},{"content":"시나리오 데이터베이스를 설치하고 읽기와 쓰기를 탐색해보자\n구성도 Jenkins를 활용한 컨테이너 자동 빌드 및 docker-compose 배포 Nova에 Mysql 서버 설치 및 volume에 데이터 저장 Trove 설치 실패 초기 설계에서는 RDS와 유사하다고 생각한1 서비스인 Trove를 활용해서 구성할 예정이었습니다. 그런데 따로 네트워크를 구축하고 설치해보면서 다양한 오류사항을 마주했습니다. 원인은 대략적으로 찾았으나 구체적인 해결 방안을 찾지 못해서 Nova 인스턴스에 mysql 서버를 사용하는 방식으로 전환했습니다.\n오류사항 우선 Trove가 무엇인지 알아봅시다. 공식문서에 의하면 trove는 다른 퍼블릭 클라우드의 데이터베이스 서비스과 유사한 기능을 가지고 있습니다. 안정성을 위한 대부분의 기능을 제공합니다.\nTrove is Database as a Service for OpenStack. It\u0026rsquo;s designed to run entirely on OpenStack, with the goal of allowing users to quickly and easily utilize the features of a relational or non-relational database without the burden of handling complex administrative tasks. \u0026hellip; Initially, the service will focus on providing resource isolation at high performance while automating complex administrative tasks including deployment, configuration, patching, backups, restores, and monitoring.\n저는 Kolla-Ansible를 사용하고 있어 쉽게 리소스를 추가할 수 있습니다. globals.yml 에서 trove 리소스 부분을 yes로 바꾸기만 하면 됩니다. 설정을 추가하고 재배포하면 총 3개의 trove 컨테이너가 생성됩니다.\nTrove는 Nova 인스턴스 내에서 작동됩니다. 관련 이미지를 다운로드 받아서 glace에 업로드한 후 datastore version을 정의할때 사용합니다.\n1 2 3 4 5 6 7 8 # Image 다운로드 wget https://tarballs.opendev.org/openstack/trove/images/trove-master-guest-ubuntu-jammy.qcow2 # 이미지 Glance로 업로드 openstack image create Trove-Ubuntu --disk-format qcow2 --container-format bare --public --tag trove --tag mysql --file trove-master-guest-ubuntu-jammy.qcow2 # datastore version 정의 openstack datastore version create 5.7.29 mysql mysql \u0026#34;\u0026#34; --image-tags trove,mysql --active --default --version-number 5.7.29 위의 명령어를 모두 수행한 후 아래의 명령어를 확인해보면 mysql에 해당되는 datastore를 확인할 수 있습니다.\n그리고 trove 관리 네트워크를 따로 만든 후에 데이터베이스를 생성했습니다.\n1 2 3 4 5 6 7 8 9 openstack database instance create test_db \\ --flavor m1.medium \\ --size 10 \\ --nic net-id={custom trove manangement network id} \\ --database testdb \\ --users tester:mypassword \\ --datastore mysql \\ --datastore-version 5.7.29 \\ --allowed-cidr 0.0.0.0/0 명령어 수행 이후 빌드 시간이 길었습니다. Timeout 에러가 발생할 것이라고 예상했고, 생각한대로 오류가 발생했습니다.\ntrove - api 에서 taskmanager로 작업을 수행할때 message bus 부분에서 문제가 생긴 것 같습니다.\n우리의 목적은 간단하게 API를 테스트하는 것이기 때문에 대규모에 적합한 데이터베이스 시스템이 아니여도 괜찮습니다. 그러므로 간단하게 Nova 인스턴스 내에 Mysql 서버를 설치하는 방법을 선택했습니다.\n프로비저닝(Provisioning) 프로비저닝(Provisioning) 이란 의미는 영어 직역한 그대로 \u0026ldquo;제공하는것\u0026rdquo; 이다. 어떤 종류의 서비스든 사용자의 요구에 맞게 시스템 자체를 제공 하는 것을 프로비저닝이라고 하며 제공해줄 수 있는 것은 인프라 자원이나 서비스, 또는 장비가 될 수도 있다.\n출처 - Jins\u0026rsquo; Dev Inside - 프로비저닝이란?\n앞으로는 인프라를 생성, 환경을 설정 과정을 프로비저닝이라는 용어로 칭하겠습니다.\n리소스 생성 저희는 크게 2가지의 Nova 인스턴스가 필요합니다.\nDocker, Jenkins 서버가 설치되어 있는 인스턴스 -\u0026gt; Nova 인스턴스 Block Storage(Cinder)와 연결되어 있으며 MySQL Server가 설치된 인스턴스 -\u0026gt; Block Storage(Cinder)가 연결된 Nova 인스턴스 Openstack에서는 Nova 인스턴스를 생성할 때 Volume(Block Storage)을 추가할 수 있습니다. 쉽게 AWS의 EBS라고 생각하시면 됩니다.\n앞서 말했듯이 Trove 서비스 설치를 실패하게 되면서 대안책으로 Nova에 MySQL 서버를 설치하는 방안을 선택하게 되었습니다. Volume이 연결된 Nova Instance를 활용하여 데이터베이스의 데이터가 Block Storage에 영구적으로 저장할 수 있게 구성했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Block Storage Volume for MySQL Instance resource \u0026#34;openstack_blockstorage_volume_v3\u0026#34; \u0026#34;db_volume\u0026#34; { name = \u0026#34;${var.project_name}-db-volume\u0026#34; size = var.volume_size } # Nova Instance with Volume (MySQL Server) resource \u0026#34;openstack_compute_instance_v2\u0026#34; \u0026#34;db_instance\u0026#34; { name = \u0026#34;${var.project_name}-db-instance\u0026#34; image_name = var.image flavor_name = var.flavor key_pair = openstack_compute_keypair_v2.keypair.name network { port = openstack_networking_port_v2.db_port.id } metadata = { ssh_user = \u0026#34;ubuntu\u0026#34; } } # Attach Volume to MySQL Instance resource \u0026#34;openstack_compute_volume_attach_v2\u0026#34; \u0026#34;db_volume_attach\u0026#34; { instance_id = openstack_compute_instance_v2.db_instance.id volume_id = openstack_blockstorage_volume_v3.db_volume.id } 위의 코드를 포함한 2개의 Nova Instance를 생성하면 아래와 같은 네트워크 토폴로지를 확인할 수 있습니다.\n본래라면 관리 네트워크와 외부 네트워크가 분리되어야 하지만 통합되어 있는 관계로 모두 floating ip를 할당하게 되었습니다.\n만약에 네트워크가 역할에 따라 분명하게 분리되어 있다면 web-instance는 외부 네트워크로 구성하고 db-instance는 관리 네트워크 ip를 할당받아야 할 것입니다.\n데이터 베이스 서버를 설치할 OpsAthlan-03-db-instance 인스턴스를 확인해 보겠습니다. 해당 정보에 들어가보면 OpsAthlan-03-db-volume volume이 인스턴스의 /dev/vdb 경로로 마운팅되었음을 확인했습니다.\nMySQL 서버 설치 처음에는 Ansible를 활용하여 한번에 MySQL 서버를 설치할 예정이었습니다. 그러나 저장 경로 수정 작업이 요구되면서 자동화는 힘들다고 판단했습니다.\n상당히 작업이 번거로운 관계로 JONGSKY님의 블로그(시간의 농도) - MySQL 저장경로 변경하기(feat. ubuntu) 로 대체하겠습니다.\n위의 방법을 모두 따라하면 mysql 서버 설치가 완료됩니다.\nweb-instance 에 mysql client를 설치하여 테스트를 수행해 보겠습니다. mysql 서버에 계정을 만든 후 테스트를 해보면 연결할 수 없다는 오류를 볼 수 있습니다. db-instance security group에 mysql 서버 포트인 3306 규칙을 추가해도 똑같은 오류가 발생됩니다.\n/etc/mysql/mysql.conf.d/mysqld.cnf mysql 설정 파일에 외부 통신을 허용해주면 정상적으로 연결할 수 있게 됩니다.\n1 bind-address = 0.0.0.0 다음 이야기 데이터베이스 서버를 구성했으니 jenkins를 통한 CI/CD를 구축해보겠습니다.\n완전 관리형 서비스는 아닙니다!\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-11-04T00:00:00+09:00","permalink":"https://s0okju.github.io/p/opsathlan-3-1/","title":"OpsAthlan 3-1. Hello MySQL"},{"content":"LAN이란 LAN(Local Address Network)은 위키백과에 의하면 네트워크 매체를 이용하여 가까운 지역을 한데 묶은 컴퓨터 네트워크1입니다. 그런데 이 말 자체는 상당히 추상적이라고 생각합니다. 여러 글을 찾던 중 네트워크 엔지니어 환영의 기술 블로그에서 LAN에 대해 잘 설명한 글을 알게 되었습니다.\n집과 같은 소규모 네트워크에서부터 사무실, 회사와 같은 중규모 이상의 네트워크에 이르기까지 동일한 IP 대역과 동일한 Subnet Mask를 사용한다면 크기와 상관 없이 LAN이라고 부를 수 있다.\n출처 - 네트워크 엔지니어 환영의 기술블로그 : LAN 쉽게 이해하기 예시를 들어보겠습니다. 저는 현재 홈 서버를 운영하고 있습니다. 기기의 IP주소를 확인하게 되면 192.168.50.x라는 동일한 IP 대역을 가지고 있습니다. 다시말해 저는 LAN을 쓰고 있다고 보면 됩니다.\n여기 문제가 발생하게 됩니다. 만약에 다양한 IP 대역을 쓰고 싶다면 어떻게 해야할까요? 물리적인 장치를 추가하는 방법이 있습니다만 상황에 따라서는 불가능할 수 있습니다. 또 다른 해결 방안으로 VLAN이 있습니다.\nVLAN 하나의 단일 네트워크는 하나의 브로드캐스트 영역이며 이러한 구간을 LAN이라고 합니다. 이때 VLAN은 LAN 논리적으로 브로드캐스트 도메인을 나누는 영역이라고 표현할 수 있습니다. 즉 하나의 스위치 만으로도 여러 개의 네트워크 대역을 사용할 수 있게 되는 것입니다.\nVLAN은 네트워크를 분할만하지 서로 통신하는 기능을 제공하지 않습니다. 위의 그림처럼 VLAN2의 패킷을 VLAN3에 보낼 수 없게 됩니다. 그러므로 L3 스위치 또는 라우터를 사용하여 VLAN 간의 라우팅을 수행해야 합니다.\nL3 스위치와 라우터는 초기에는 기술적인 차이가 있었으나 기술의 발전으로 차이가 거의 없다고 합니다.\nL3 스위치가 L2 스위치 + 라우터의 특징을 가지고 있으며, VLAN을 사용하기가 더 효율적이라고 합니다. 2\nVLAN은 이와 같은 설명이 전부는 아닙니다. 스위치 간 어떻게 통신할지 등등 관련된 이야기가 많습니다. 그러나 이해를 위해 네트워크를 분할한다는 점만 알아주세요.\nVLAN과 Cloud Network 퍼블릭 클라우드에서는 VPC를 정의한 후 Private, Public 서브넷을 정의하게 됩니다. 당연한 말이지만 물리적인 장치와 1:1 매칭하다보면 \u0026ldquo;어떻게?\u0026rdquo; 라는 물음만 남는 것 같습니다. 자료를 찾아보면서 내린 결론은 클라우드 네트워킹의 핵심은 가상화라는 것이었습니다.\nCloud에서는 가상 라우터를 통해 Network(VPC)를 구성하고, VLAN을 활용하여 Subnet을 구성하게 됩니다. 그리고 클라우드 리소스는 Subent에서 ip를 할당받아 네트워크 범위 내에서 통신하게 됩니다.\n다음 이야기 - Openstack를 통해 알아보는 클라우드 네트워크 Openstack은 Cloud OS로 오픈소스입니다. AWS와 같은 퍼블릭 클라우드 내부구조는 공개되지 않았지만 Openstack를 통해 큰 틀의 클라우드 구조를 파악할 수 있습니다.\nOpenstack은 네트워크 구성 시 OVS를 사용합니다. Openstack에서도 다른 퍼블릭 클라우드처럼 Subnet 개념이 존재하며 VLAN 기술을 사용하게 됩니다.\nOVS는 다중 계층 스위치의 오픈소스 구현체로 여러 프로토콜 및 표준을 지원하면서 하드웨어 가상화 환경을 위한 스위칭 스택을 제공합니다.3\n현재 홈서버로 Openstack를 활용하고 있습니다. 다음 글에서 홈서버를 통해 어떻게 네트워크가 구성되었는지 자세히 말씀 드리겠습니다.\nReference https://aws-hyoh.tistory.com/75 https://ko.wikipedia.org/wiki/%EA%B7%BC%EA%B1%B0%EB%A6%AC_%ED%86%B5%EC%8B%A0%EB%A7%9D\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n그림으로 배우는 네트워크 원리, Gene 저자, 영진닷컴\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Open_vSwitch\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-10-27T00:00:00+09:00","permalink":"https://s0okju.github.io/p/cloud-network-1/","title":"클라우드 네트워크 - 클라우드와 VLAN"},{"content":"Introduction 하나의 프로젝트에는 하나의 상태 파일이 존재한다. 그러면 궁금증을 가지게 된다.\nQ. 만약에 개발, 운영 코드를 짠다고 한다면 어떻게 짤까?\nA. 공통의 기능이 포함된 root 모듈을 만든 후 운영, 개발 프로젝트를 만든다.\n공통의 인프라 리소스가 포함된 모듈 생성할 것이다. 별도의 모듈을 생성하게 된다면 공통 모듈의 상태 관리 문제를 빼놓을 수 없을 것이다.\n개발, 운영 코드를 하나의 프로젝트에 구현하게 되면 환경에 따라 다르게 실행되는 부분이 존재하게 된다. 결국 이를 해결하기 위한 조건문이 코드를 더 복잡하게 만들 수 있을 것이다.\n운영, 개발 프로젝트를 분리하는 것도 문제이다. 개발이 완료된 후에는 운영에 옮겨야 하는데 개발자가 일리리 코드를 옮기는 것은 바람직하지 않을 것이다.(사람은 누구나 실수를 하게 된다.) Terraform은 이러한 문제를 해결하기 위해 Workspace라는 개념을 도입했다.\nWorkspace Terraform은 위와 같은 문제점을 해결하기 위해 Workspace을 도입하게 되었다. Workspace는 하나의 프로젝트에 여러개의 상태파일을 관리할 수 있게 되었다. 간단하게 말해서 Workspace는 상태 파일을 관리하는 그릇이라고 보면 된다.\n자세한 명령어는 공식 홈페이지를 참고하길 바랍니다.\nMultiple Workspace Backend 상태 파일을 관리하기 위해 Remote Backend를 사용한다. 공식 홈페이지에 의하면 아래의 서비스만 multiple Backend를 지원해준다.\nAzureRM Consul COS GCS Kubernetes Local OSS Postgres Remote S3 Example 요구사항 S3에 Multiple Backend를 설정한다. 실수를 줄이기 위해 default을 dev로 설정한다. 변수를 활용하여 dev, prd Workspace를 분리한다. 코드 terraform block에는 어느때와 다름없이 S3 Backend를 설정하면 된다. S3로 Multiple Backend를 설정하게 되면 env:/ 하위 폴더에 워크 스페이스에 맞게 상태 파일이 저장된다. 하지만 경우에 따라서는 env 파일명을 바꾸고 싶을 것이다. 그럴때 workspace_key_prefix를 사용한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;tf-backend-d7mekz\u0026#34; key = \u0026#34;terraform.tfstate\u0026#34; region = \u0026#34;ap-northeast-2\u0026#34; workspace_key_prefix = \u0026#34;temp\u0026#34; } } 변수를 활용해 default를 dev Workspace로 지정한다.\n1 2 3 4 variable \u0026#34;env\u0026#34; { type = \u0026#34;string\u0026#34; default = \u0026#34;dev\u0026#34; } workspace 명을 변수로 지정하여 공통의 모듈에 대입한다.\n1 2 3 4 module \u0026#34;main_vpc\u0026#34; { source = \u0026#34;./custom_vpc\u0026#34; env = terraform.workspace } 변수를 활용해 workspace에 맞는 리소스 이름을 지정했다.\n1 count = var.env == \u0026#34;default\u0026#34; ? 1 : 0 Count를 사용하여 특정 Workspace에 리소스를 실행하지 않도록 구현할 수 있다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } } # Configure the AWS Provider provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } # Create a VPC resource \u0026#34;aws_vpc\u0026#34; \u0026#34;default\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; tags = { Name = \u0026#34;terraform_default_vpc_${var.env}\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public_subnet_1\u0026#34; { vpc_id = aws_vpc.default.id cidr_block = \u0026#34;10.0.0.0/24\u0026#34; availability_zone = local.az_a tags = { Name = \u0026#34;terraform_public_subnet_1_${var.env}\u0026#34; } } resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;public_nat\u0026#34; { connectivity_type = \u0026#34;public\u0026#34; subnet_id = aws_subnet.public_subnet_1[0].id } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private_subnet_1\u0026#34; { vpc_id = aws_vpc.default.id cidr_block = \u0026#34;10.0.10.0/24\u0026#34; availability_zone = local.az_a tags = { Name = \u0026#34;terraform_private_subnet_1_${var.env}\u0026#34; } } Conclusion 다양한 환경을 하나의 상태 파일로 관리하는 것은 쉽지 않다. 그래서 Terraform은 이러한 문제를 위해 환경에 따른 상태 파일을 보관할 수 있도록 Workspace을 도입하게 되었다. 다만 Workspace을 무조건 서로 다른 환경을 분리할때 사용하는 것이 아니다. 유사하지만 다른 환경에서 인프라를 운용해야 할 때 사용해야 한다.\nReference Terraform의 Workspace를 이용해 배포 환경 분리하기 Terraform Workspaces ","date":"2024-10-23T00:00:00+09:00","permalink":"https://s0okju.github.io/p/terraform-workspace/","title":"Terraform - Workspace"},{"content":"RestTemplate vs WebClient 둘은 Rest 방식 API를 호출할 수 있는 클래스이다. RestTemplate = Blocking, WebClient = Non Blokcing 방식이라고 한다. 물론 WebClient도 Blocking 방식은 지원한다.\n여러 이유 중에서 특정 임계점을 넘게 되면 RestTemplate는 급격하게 속도가 저하된다. 그러므로 규모가 있는 프로젝트에서 WebClient를 쓰는게 더 좋다.\n요즘은 WebClient를 쓰는 추세라고 한다.\n물론 모아리움 프로젝트는 소규모라 Blocking 방식으로 트래픽을 감당할 수 있다. 그러므로 RestTemplate를 채택했다.\nAPI 개발 시 마주한 문제들 이슈 2. CamelCase로 인한 Google API Bad Request 문제 Google API 요청은 확인했으나 access_token이 필요하다며 bad request를 보여줬다. 문제가 무엇이나 고민하던 찰나 공식 문서를 봤더니, request, response가 snake case였다. 그래서 Request, Response DTO는 JsonNaming을 활용를 Snake Case로 변경시켰다.\n1 2 3 4 5 6 7 8 9 10 11 12 @AllArgsConstructor @NoArgsConstructor @Data @Builder @JsonNaming(PropertyNamingStrategies.SnakeCaseStrategy.class) // camel case -\u0026gt; snake case public class GoogleOAuthLoginReqDto{ private String clientId; private String redirectUri; private String clientSecret; private String grantType; private String code; } 이슈 2. [[Spring - @Value]] getter가 null이 되는 경우 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Component @Data public class GoogleOAuthLoginApiClient implements OAuthLoginApiClient\u0026lt;GoogleOAuthProfile\u0026gt;{ @Value(\u0026#34;${google.auth.url}\u0026#34;) private String googleAuthUrl; @Value(\u0026#34;${google.login.url}\u0026#34;) private String googleLoginUrl; @Value(\u0026#34;${google.auth.scope}\u0026#34;) private String scopes; private RestTemplate restTemplate = new RestTemplateBuilder().errorHandler(new RestTemplateResponseErrorHandler()).build(); private ObjectMapper objectMapper = new ObjectMapper(); @Override public String getTokenURI() { System.out.println(\u0026#34;Auth: \u0026#34; +getGoogleAuthUrl()); return UriComponentsBuilder.fromHttpUrl(getGoogleAuthUrl()+\u0026#34;/token\u0026#34;).toUriString(); } getGoogleAuthUrl() 가 계속 null을 출력해서 해결책을 모색하고자 했다.\nProperties가 잘 적용되었는가? -\u0026gt; Yes @Value를 쓰는 클래스에 Bean을 잘 등록했는가? -\u0026gt; Yes GoogleOAuthLoginApiClient의 의존성 주입을 잘 수행했는가? -\u0026gt; No 결국 3번의 문제였다. 바로 의존성 주입하지 않고 new 생성자를 만든 것이었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 @Service public class OAuthLoginService { public GoogleOAuthProfile requestGoogleOAuthLogin(String authCode){ OAuthLoginApiClient\u0026lt;GoogleOAuthProfile\u0026gt; client = new GoogleOAuthLoginApiClient(); try { String accessToken = client.requestOAuthClientAccessToken(authCode); return client.requestOauthInfo(accessToken); } catch (JsonProcessingException e) { throw new RuntimeException(e); } } } Spring에서 싱글톤으로 관리되는 빈이 아닌 새로운 객체로 생성하게 되면 @Value 애노테이션으로 설정 파일을 읽어들이는 변수는 null로 값이 저장되지 않습니다.\n출처 - gyucheolk 티스토리, [Spring] @Value 애노테이션 null 점검\n그래서 새로운 객체가 아닌 의존성 주입으로 코드를 수정했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @Service public class OAuthLoginService { private final GoogleOAuthLoginApiClient authLoginApiClient; public OAuthLoginService(GoogleOAuthLoginApiClient authLoginApiClient) { this.authLoginApiClient = authLoginApiClient; } public GoogleOAuthProfile requestGoogleOAuthLogin(String authCode){ OAuthLoginApiClient\u0026lt;GoogleOAuthProfile\u0026gt; client = authLoginApiClient; try { String accessToken = client.requestOAuthClientAccessToken(authCode); return client.requestOauthInfo(accessToken); } catch (JsonProcessingException e) { throw new RuntimeException(e); } } } 팀원에게 조언을 얻었는데, 서비스 컨트롤러가 아닌 것을 주입할때 명시하는 것이 좋다고 한다.\n","date":"2024-10-16T00:00:00+09:00","permalink":"https://s0okju.github.io/p/moarium-backend-1/","title":"모아리움 백엔드 - API 개발 시 마주한 문제들 "},{"content":"나는 소규모 작문 스터디를 통해 글또를 알게 되었다. 스터디 모집 설명글에는 글또 합격을 위한 글쓰기 연습이라고 적혀 있었다. 남은 한 기수가 마지막이 될 것이다는 모집 페이지를 보고 모집 알림 메일을 신청했다.\n잊혀질때 쯤 글또 모집 메일을 받게 되었다. 나의 일생, 자기소개 등 다양한 질문에 대한 답을 글로 써가면서 몇년동안의 노력을 정리했고 나의 바램과 함께 글또에 합격하게 되었다.\n글또에서 나의 방향 우선 글또는 소모임부터 시작해서 커피챗까지 다양한 활동이 있다. 글쓰기와 사람들과의 소통 두 마리 토끼를 잡을 수 있다면 좋겠지만 시간 관계상 모두 챙길 수 없을 것 같았다. 그래서 나는 글쓰기에 초점을 맞추기로 결심했다.\nOT를 들으면서\u0026hellip; OT를 들으면서 가장 기억에 남은 문장이 있다.\n공부하고 글쓰기\n지금까지 기술 관련된 글을 써왔다. 글쓰는 과정에서 공부를 했고 그 과정에서 많은 수정 작업이 수행되었다. 단점은 뒤로 갈수록 힘이 빠진다는 점이었다. 분명히 논리적으로 맞지 않음에도 수정을 멈추고 바로 블로그에 업로드했다. 이러한 패턴을 알아서 그런지 위의 문장은 나의 잘못된 글쓰기의 태도를 상기시켜줬다.\n지금까지 나는 어떤 글쓰기를 해왔나? 초반에는 완벽하게 써야한다는 생각이 글쓰기를 어렵게 만들었다. 해결책으로 기술 관련된 것만큼은 가급적이면 가볍게 정리하려고 노력했다. 추가적으로 공부할때마다 내용을 추가했고 page를 임베딩하여 내용을 연결하는 형식을 사용했다.\n단점으로는 블로그에 글을 쓸때마다 내가 정리해둔 이론을 보지 않는다는 점이었다. 시간이 지날수록 내용을 추가하지 않은 점도 문제였다.\n메모라는 제목에 파일만 계속 생겨나면서 온라인 노트는 혼잡스러운 메모장이 되어 있었다.\n글또에서 쓰고 싶은 글 지금까지 해왔던 것처럼 사이드 프로젝트를 통해 배운 내용 위주로 작성할 것 같다.\n기본적인 이론은 책과 다른 사람들의 정성스러운 글만으로도 충분하다고 생각한다. 좋은 내용은 출처를 제대로 남기면서 문제를 해결하는 과정을 글로 작성하고자 한다.\n글또할 결심 나는 6개월 간의 글쓰기를 수행하면서 지켜야 하는 나만의 규칙을 생각했다.\n즉각 정리하기 새로운 이론을 공부하게 되면 그냥 넘어가는 것이 아니라 중요하다고 생각하는 부분을 작성하는 것이다. 또한 기존에 있는 내용을 정리하면서 남겨진 파일을 정리하는 것이다. 즉각 정리하는 것이 글또 활동에 필요하다고 생각한다.\nOT에서 말했듯이 충분한 학습을 위해서는 내용을 글로 정리하는 것이 필요하다. 단순한 눈과 귀로 학습하는 것은 단편적으로 이해할 수 있어도 다른 지식과의 연결성을 제때 파악하지 못할 수 있다. 지금까지의 글쓰기에서도 연결성을 파악하지 못해 중간에 중단된 경험이 많아 이번 기회에 제대로 실천하고 싶다는 생각이 들었다.\n어려운 것을 쉽게 풀어쓰기 화려한 언어로 글쓰는 것은 의외로 쉬울 수 있겠다는 생각이 든다. 오히려 쉬운 예시로 설명하는 것이 많은 학습량을 요구하는 것 같다. 반대로 독자라고 생각했을때 전문성 있는 간결한 문장은 이해가 안될때가 많았다. 길지만 쉬운 내용이 머리 속에 잘 남았다.\n이번 글쓰기에는 어렵고 복잡한 내용이 있으면 쉽게 풀어써보려고 한다. 물론 같은 조가 인프라를 전문적으로 공부하는 사람들 밖에 없다. 그래도 인프라 직군은 알아야 하는 범위가 많고 조 내에 관심 많은 사람이 많기에 더욱 쉽게 써야 한다고 생각한다.\n글의 완성 = 3번 읽어보기 글의 완성을 논하면 끝도 없다고 생각한다. 주관적인 영역이라서 답도 없다. 그래서 나는 글의 완성은 3번은 검토하고 글을 업로드할 것 같다.\n약속 잘 지키기 이건 글 제출 뿐만 아니라 모임에서 약속한 모든 것이 해당된다. 더 나아가 \u0026ldquo;같은 조에 있는 사람들의 글을 읽고 코멘트 남기기\u0026quot;라는 약속도 하고 싶다.\n글을 쓰는 것도 중요하지만 읽고 의견을 말하는 것도 중요하다고 생각한다. 인상 깊은 글이라면 감상평을 표현할 수 있는 사람이 될 수 있도록 코멘트 남기기 약속도 지키고 싶다.\n","date":"2024-10-12T00:00:00+09:00","permalink":"https://s0okju.github.io/p/geultto-1/","title":"글또할 결심"},{"content":"TodoPoint 프로젝트에 대한 소개 Todopoint란 문자 그대로 할 일을 완수하면 포인트를 지급하는 애플리케이션 입니다. 계획을 제대로 완수하지 못한 사람들에게 강제성을 부여하고자 기획한 프로젝트입니다.\n프로젝트 기획의 창의성의 별개로 개인의 MSA에 대한 학습과 기술 역량을 향상하기 위해 시작한 프로젝트였습니다. 실제로 애플리케이션을 작동시키기 위해 쿠버네티스 홈 서버를 구축하기도 했습니다. 그러나 3개월이란 긴 시간 동안 예상치 못한 삽질로 제대로 마무리하지 못했습니다.\n지금부터 프로젝트 실패의 원인을 소개하겠습니다.\n잘못된 언어 선택 저는 웹 애플리케이션을 제작하면서 Go라는 언어와 Gin 웹애플리케이션 프레임워크를 사용했습니다. 그때 당시 Go 언어를 한번도 못해봤습니다. 단순하게 해보고 싶은 언어라서 선택했습니다. 이러한 선택으로 인해 예상치 못한 시간을 쓰게 되었습니다.\n새로운 기술을 사용할때는 고민해보자 앞서 말했듯이 그때 당시 Go언어는 처음이었습니다. 준비단계로 간단하게 Go 언어 문법만 익히고 바로 Gin을 쓰게 되었습니다.\nGo는 독특한 문법 체계를 가지고 있어 다른 언어에 비해 응용하기 어려웠습니다. 특히 구조를 짜는데 어려움을 겪었습니다. 객체지향 언어지만 흔히 알고 있는 클래스 기반 객체지향 패턴과는 다르다는 점이 어려웠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 //go:generate mockery --name Store --case underscore type Store interface { Create(ctx *gin.Context, info *data.UserInfo) error FindOne(ctx *gin.Context, uid int) (*data.Me, error) Update(ctx *gin.Context, uid int, me data.Me) error } type UserService struct { store Store } func NewUserService(store Store) *UserService { return \u0026amp;UserService{ store: store, } } func (s *UserService) Update(ctx *gin.Context, uid int, me data.Me) (*httputils.BaseResponse, *httputils.NetError) { err := s.store.Update(ctx, uid, me) if err != nil { return nil, httputils.NewNetError(codes.UpdateFailed, err) } return httputils.NewSuccessBaseResponse(nil), nil } 결국 깃허브에 베스트 케이스를 찾아다니면서 대략 한달 동안 코드를 2번 새로 만들었습니다. 주로 Error 구조체, 패키지 구조 등 부족하다고 생각하는 부분을 전부 다 수정했습니다.\n각각의 서비스를 만드는 것은 어렵지 않았습니다. 그러나 공통 모듈이라는 것을 만들려고 했을때 모든 것이 꼬이기 시작했습니다. 공통 모듈에는 필요한 인증 미들웨어나 전체적으로 통일된 형식의 구조체(http response값, 오류 메세지 타입 등)가 포함되어 있습니다. Go는 외부 라이브러리 저장소로 github를 주로 사용합니다. 공통 모듈을 github에 업로드하여 애플리케이션을 빌드할때마다 제작한 공통 모듈을 다운로드 받고 사용해야 할까요? 개발 초기 단계라 공통 모듈 수정이 잦습니다. 즉, 수정사항을 반영할때 비효율적입니다.\n이때 당시 \u0026ldquo;공통 모듈이 필요한가?\u0026ldquo;에 대해 많이 생각했던 것 같습니다. 저는 개인적으로 필요하다고 생각했습니다. 요구사항에 따라 다르겠지만 공통적인 부분이 많을수록 모듈화하여 관리하는 것이 효율적이라고 생각했기 때문이었습니다. 그대신어느정도 구조가 완성된 상태에서 사용해야 한다고 생각이 들었습니다.\n외부 다운로드 외에 다른 방법이 있었습니다. Go는 Workspace라는 개념이 있으며 go.work 파일을 통해 명시적으로 모듈을 포함시킬 수 있습니다.\n1 2 3 4 5 6 7 8 go 1.22.1 use ( . ./../../modules/v2/common ../../modules/v2/database/d7mysql ../../modules/v2/database/d7redis ) Workspace는 로컬 사용에 적합하다는 의견이 있습니다.1\n만든 Go 애플리케이션을 컨테이너로 실행시키기 위해서는 Dockerfile로 만들어야 합니다.\n우선 go 애플리케이션으로 빌드하기 위해서는 공통 모듈을 모두 복사하고 go work use 를 수행한 후 $GOPATH/src에 모듈을 저장하는 작업이 필요합니다. 그러나 개발 환경과 도커 내부 환경이 달라서 도커 내부에 go work use를 할 수 있도록 스크립트를 제작한 후 Dockerfile를 만들었습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # Stage 1: Build modules FROM golang:1.22 as modules-build COPY modules/v2/common /modules/common COPY modules/v2/database/d7redis /modules/database/d7redis COPY modules/v2/database/d7mysql /modules/database/d7mysql WORKDIR /app COPY scripts/init_workspace.sh /app/init_workspace.sh COPY auth-service/v2/workspace.packages.json /app/workspace.packages.json RUN apt-get update \u0026amp;\u0026amp; apt-get install -y jq RUN chmod +x init_workspace.sh \u0026amp;\u0026amp; ./init_workspace.sh RUN cat go.work COPY auth-service/v2 /app COPY auth-service/v2/go.mod /app/go.mod COPY auth-service/v2/go.sum /app/go.sum RUN go mod download RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o /bin/main main.go # stage 2 FROM scratch COPY --from=modules-build /bin/main /bin/main EXPOSE 3001 CMD [\u0026#34;/bin/main\u0026#34;] 결국 해결하지 못했습니다. 경로 문제, 모듈이 제대로 설정되지 않는 문제 등 원인은 다양했습니다.\n해결책을 모색하다가 느낀건 \u0026ldquo;내가 Go를 적절하게 사용한 것이 맞나?\u0026rdquo; 였습니다. 그리고 거슬러 올라가 MSA를 채택한 것이 문제였다는 결론에 도달했습니다.\n마이크로서비스 하지 말았어야 했다. 결국 모든 원인은 아무 생각 없이 마이크로서비스를 도입했기 때문이라고 생각합니다. 그때 당시 MSA라고 불리는 용어가 트렌드처럼 떠올랐고, 규모가 있는 기업이 해당 능력을 요구하면서 능력이 필수라고 생각했습니다.\n왜 규모가 있는 기업이 MSA를 사용하는가? 에 대한 역질문을 생각하지 못한 것 같습니다.\n기업이 MSA를 채택한 이유는 서비스 간 결합도를 낮춰 요구사항에 대한 빠른 대처하기 위해서 라고 생각합니다. 제 프로젝트는 MSA로 할만큼 큰 서비스가 아니였고 이를 구현할 능력을 가지고 있지 않았습니다. 즉 오버 엔지니어링을 한 것이었습니다.\n실패 이후의 새로운 시작 이 프로젝트를 수행하면서 많은 시간을 사용했습니다. 노력 끝에 완성된 것이 하나 없으니 한편으로는 아쉽다는 생각이 들었습니다.\n하지만 이 프로젝트가 저에게는 새로운 시작을 알렸던 것 같습니다.\n\u0026ldquo;기초부터 공부할 예정이라면 기본적인 구성부터 공부해보자.\u0026rdquo; 라는 생각이 들었습니다. 이후로 저는 기초부터 공부할 수 있도록 작은 프로젝트를 기획하게 되었습니다.\nhttps://www.reddit.com/r/golang/comments/19517hq/what_are_you_using_go_workspaces_for_if_at_all/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-10-09T00:00:00+09:00","permalink":"https://s0okju.github.io/p/todopoint-final-review/","title":"왜 TodoPoint 프로젝트를 실패했는가?"},{"content":"DevStack의 한계 DevStack으로 Openstack을 배포하면서 큰 문제점을 느끼게 되었습니다.\nNova Instance에 직접 접근하는 것이 어렵다. 리소스를 추가 및 삭제하는 것이 어렵다. 위의 문제점은 네트워크로 인해 발생한 것입니다. 예로 제가 Nova Instance를 생성해 Floating ip를 할당한다고 가정해 봅시다. External Network의 IP 범위가 외부와 연결이 불가능해 직접적으로 접근할 수 없게 됩니다. 그럼 ssh로 서버에 직접 접속해서 관리하는 방안도 있습니다. 그럼 Openstack을 왜 쓴거지? 키는 어떻게 관리하지? 등 다양한 꼬리 질문이 따라오게 됩니다.\nKolla Ansible 방안을 모색하는 중 Openstack은 DevStack 이외에도 다양한 배포 방안이 있다는 것을 알게 되었습니다.\nKolla Ansible이란 Ansible 플레이북을 통해 OpenStack의 모든 서비스를 컨테이너로 배포하고 유지 보수하며 운영할 수 있도록 도와주는 프로젝트이다. 이를 통해 복잡한 설치 과정과 수동 설정을 최소화하고, 재현 가능한 환경을 제공하여 효율적이며 안정적인 클라우드 인프라 운영을 가능하게 한다.1\n즉 우리가 설정한 시스템 서비스를 컨테이너 형태로 구축되는 것입니다.\n설치 설치는 파란돌님의 블로그 - Kolla-ansible로 Openstack All-in-one 설치하기(Installing Openstack All-in-one with Kolla-ansible)를 적극 참고했습니다.\n설치 목록 Kolla-ansible의 장점은 원하는 리소스 선택과 설정이 쉽다는 것입니다. 그래서 저는 기본적인 리소스를 선택하여 설치했습니다.\n설치할 리소스\nKeystone Glance Nova Neutron Cinder Swift Horizon all-in-one or node? 오픈스택은 크게 3가지 노드가 있습니다.\nController node : 전체 오픈스택 서비스를 관리하기 위해 사용됩니다. 컨트롤러 관리 및 노드 간 연결을 의해서는 최소한 2개 이상의 인터넷 인터페이스가 필요합니다. Compute node : Nova 기반의 인스턴스를 작동하기 위해 사용되는 하이퍼바이저를 실행하는 노드입니다. Network node : 다양한 네트워크 서비스 에이전트를 실행하며, 이를 가상 네트워크에 인스턴스를 연결합니다. 여러 가이드를 보면 노드들은 물리적으로 분리되어 있습니다. 서버 내 가상머신을 구축하여 구현하는 방법이 있습니다. 그러나 복잡한 관계로 서비스 기능을 하나의 호스트에 설치할 수 있는 all-in-one2을 선택하게 되었습니다.\nOpenstack 네트워크 가장 어렵고 앞으로 계속 공부해야 하는 분야인 것 같습니다. 완전히 이해한 것은 아니지만 알고 있는 그대로 작성하겠습니다.\n오픈 스택에서 네트워크는 Management, Tunnel, External 네트워크가 있습니다.\nManagement Network : 관리용 네트워크로 각 컴포넌트와 관련된 API를 호출하는데 사용됩니다. Tunnel Network : vm instance 간 네트워크를 구축하는데 사용됩니다. External Network : vm instance가 인터넷과 통신하기 위한 네트워크입니다. 네트워크 서비스는 크게 두 가지 옵션이 있습니다.\nProvider Network : 가상 네트워크를 물리적 네트워크로 연결합니다. 즉 물리적인 네트워크가 vm 인스턴스가 활용하는 네트워크가 됩니다. Self-Service Network : 오픈스택을 사용하는 사용자가 직접 자신만의 네트워크를 구축할 수 있는 네트워크 입니다. Provider 네트워크는 부하분산 서비스 혹은 방화벽 서비스 등 고급 기능을 지원하지 않습니다.\n제가 사용하는 옵션은 비교적 간단한 Provider Network 입니다.\n테스트용 Openstack 서버의 네트워크 접근 문제 kolla-ansible에서 제공해주는 globals.yml 를 확인해보겠습니다. 환경 파일을 보면 Internal, External Network로 구분되어 있습니다.\nnetwork_interface는 Internal network에 해당되는 인터페이스를 지정하는 것으로 저는 외부 인터넷과 연결되지 않는 enp2s0 인터페이스를 사용했습니다. 반면 neutron_external_interface는 provider로 제공할 인터페이스로 인터넷과 연결할 수 있는 enp3s0 인터페이스를 사용했습니다.\n1 2 3 4 5 6 7 8 9 ############################## # Neutron - Networking Options ############################## # ... # followed for other types of interfaces. network_interface: \u0026#34;enp2s0\u0026#34; --- # ... neutron_external_interface: \u0026#34;enp3s0\u0026#34; 그림을 그려보면 아래와 같습니다.\n(아래의 그림은 172.17.0.250/24이 아니라 172.16.0.250/24 입니다. )\n하지만 이번 오픈스택 서버는 어디까지나 개인으로만 사용되는 공간입니다. 즉 클라우드 서비스를 쓰는 것도 저 혼자이고, 관리하는 것도 저 혼자인 것입니다. 관리를 위한 네트워크 접속(Internal Network)도, 오픈스택 리소스 접근할 수 있는 네트워크 접속(External Network)도 가능해야 합니다.\n그러나 저는 클라우드 리소스의 API를 접근할 수 없습니다. 왜냐하면 서로 다른 네트워크 대역을 가지고 있기 때문입니다.\n이러한 문제를 해결하기 위해서는 라우팅 테이블을 설정해야 할 것입니다. 라우터에 직접 설정하는 방법과 운영체제에서 처리하는 방법이 있는데, 물리적으로 라우터가 하나만 존재하기 때문에 운영체제에서 처리해야 합니다.\n저는 ip forward를 사용했습니다.\nIP-Forward란 커널 기반 라우팅 포워딩으로 하나의 인터페이스로 들어온 패킷을 다른 서브넷을 가진 네트워크 인터페이스로 패킷을 포워딩시키는 것이다.\n1 2 3 # Kernel parameter update $ sudo sysctl -w net.ipv4.conf.all.forwarding=1 $ sudo sysctl net.ipv4.conf.all.forwarding net.ipv4.conf.all.forwarding = 1 br-ex 송신되어 enp2s0 인터페이스에 수신되는 것을 허락한다는 의미입니다.\nbr-ex은 OpenVSwitch(OVS) 적용시 물리 네트워크 인터페이스와 직접 연결되는 가상 스위치입니다. 외부 네트워크에서 가상 머신을 접근할때 사용됩니다.\n1 2 sudo iptables -I FORWARD -i br-ex -o enp2s0 -j ACCEPT sudo iptables -nL FORWARD 클라이언트도 설정이 필요합니다.\n192.168.50.27를 통해 172.16.0.0/24(enp2s0)에 접근한다는 routing rule를 추가합니다.\n1 sudo route -n add 172.16.0.0/24 192.168.50.27 여기서 왜 192.168.50.27를 통해 172.16.0.0/24 네트워크에 접근한다고 설정한 것일까요?\n오픈 스택 서버에서 ip_forward를 시켰습니다. 즉 서버가 라우터의 역할을 하는 것입니다. 그래서 클라이언트에서 라우팅 룰을 설정할 때에는 172.16.0.250/24에 포워딩을 시킨 서버의 ip 주소로 설정해야 하는 것입니다.\n정리 테스트를 수행하기 위해 Openstack 서버를 구축하게 되었습니다. 구축 시 요구사항은 인스턴스에 접근할 수 있으면서 리소스 API에 접근 가능해야 한다는 것이었습니다. 서로 다른 네트워크 대역을 가져 리소스 API에 접근하지 못했으나 서버에 ip forward를 수행하여 문제를 해결했습니다.\n문제를 해결해 보니 Neutron에 대해 모르고 있다는 사실을 알게 되었습니다. 다음 포스팅은 Neutron 톱아보기로 돌아오겠습니다.\nReference https://velog.io/@lijahong/0%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-Linux-%EA%B3%B5%EB%B6%80-%EB%B0%A9%ED%99%94%EB%B2%BD-%EC%BB%B4%ED%93%A8%ED%84%B0 https://velog.io/@larshavin/Kolla-Ansible%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-OpenStack-%EC%84%A4%EC%B9%98-1 https://blog.naver.com/love_tolty/220237750951 https://tech.osci.kr/openstack_cinder/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://blog.naver.com/love_tolty/220237750951\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-10-01T00:00:00+09:00","permalink":"https://s0okju.github.io/p/server-setup-4/","title":"서버 구축기 - 4. Kolla-ansible 설치 시 마주한 네트워크 문제"},{"content":"JVM의 메모리 사용 형태 JVM에는 3가지 메모리 사용 형태가 있습니다.\nused: JVM이 사용 중인 메모리 양 committed: JVM이 시스템에게 요청한 메모리 양 max : 실행 중인 환경에서 최대로 할당할 수 있는 메모리 양 메모리 용량은 used \u0026lt;= committed \u0026lt;= max를 따르는데, used 메모리 양이 committed, max 메모리보다 크게 된다면 OOM(Out Of Memory)이 발생1하게 됩니다.\n그라파나를 확인해보면 3가지의 메모리 형태를 알 수 있습니다.\nCommitted Memory의 역할 YGwan님의 블로그: JVM Memory의 3가지 메트릭 지표 - Committed Memory란?를 읽어보는 것을 추천드립니다.\n메모리를 할당하고 해제하는데 필요한 오버헤드를 줄일 수 있다는 특징을 가지고 있습니다.\nJVM이 시스템을 통해 미리 메모리를 할당하여 사용 시 가지고 있는 여유 메모리 공간을 활용합니다. 이를 통해 메모리를 즉시 사용할 수 있으며 메모리를 할당하는데 발생하는 오버헤드를 줄일 수 있게 됩니다.\nAdvanced echo server jvm 메트릭 특징 이전 글에서 스프링부트 컨테이너의 메모리가 지속적으로 증가한다고 언급했습니다. 그래서 저는 jvm에 문제가 있을 수 있겠다는 생각이 들어 관련 지표를 확인해보겠습니다.\n처음에 눈여겨 봤던 것은 JVM의 committed 메모리 양이 점진적으로 증가하고 있다는 점이었습니다.\n😂 사진에서는 잘 보이지 않지만 자세히 보면 1MB씩 상승하고 있습니다.\n자세히 알아보기 전에 heap과 non-heap 메모리에 대해 알아봅시다.\nJava Heap : Instance와 객체가 저장되는 공간으로 모든 Thread에 의해 공유되는 영역2 Native Area(non-heap) : OS레벨에서 관리하는 영역으로 JVM 크기에 강제되지 않기 때문에 프로세스가 이용할 수 있는 메모리 자원을 최대한 활용할 수 있습니다. 다시 돌아와 제가 운영하고 있는 웹 애플리케이션을 확인해 볼까요.\n정말 간단한 EchoController와 프로메테우스 metric 수집을 위한 MetricController가 있습니다.\n1 2 3 4 5 6 7 8 @RestController public class EchoController { @GetMapping(\u0026#34;/echo\u0026#34;) public String echo() { return \u0026#34;Hello, World!\u0026#34;; } } 웹 애플리케이션이 비교적 단순하기 때문에 Heap 영역의 문제는 아니라고 생각했습니다.\n그래도 JVM Heap를 확인해 보겠습니다. 힙 메모리의 사용량은 매우 적게 사용되었고, 안정적으로 유지되었습니다.\nnon-heap 영역을 봐봅시다.\n변화폭이 적어서 차이를 모르겠지만 메모리가 사용량이 조금씩 증가하고 있습니다.\n우선 프로파일링은 무엇을 의미할까요. hudi.blog: 스프링 애플리케이션 배포 직후 발생하는 Latency의 원인과 이를 해결하기 위한 JVM Warm-up를 참고하겠습니다.\nJIT 컴파일러는 애플리케이션에서 자주 실행된다고 판단되는 특정 부분만을 기계어로 컴파일한다. 이 부분을 핫스팟(Hotspot) 이라고 부른다. JIT 컴파일러는 실행중인 애플리케이션의 동작을 분석하고 코드 실행 횟수, 루프 반복 횟수, 메소드 호출 등의 정보를 측정하고 기록한다. 이를 프로파일링이라고 한다.\nJIT 컴파일러는 프로파일링 결과를 토대로 핫스팟을 식별한다. 핫스팟이 식별되었다면, JIT 컴파일러는 메소드 단위로 바이트 코드를 기계어로 번역한다. JIT 컴파일러는 이렇게 번역된 기계어를 코드 캐시(Code Cache) 라는 캐시공간에 저장한다.\n출처 - https://hudi.blog/jvm-warm-up/\nJIT는 전체 코드를 컴파일 하는 것이 아닌 자주 실행되는 부분에만 컴파일하며 이에 대한 데이터를 Code Cache에 저장하게 됩니다.\nNon Heap과 관련된 지표를 확인해봅시다.\n아래의 세 가지 지표가 미세하게 증가하고 있습니다.\nMetaspace : JVM이 로드한 메타데이터를 저장 CodeHeap : JIT 컴파일러 관련 지표 profiled nmethods non-profiled nmethods 하지만 장기적인 관점에서 바라봤을때 메모리 낭비일 수 있겠다는 생각이 들었습니다.\n특히 Metaspace는 JVM 크기에 강제되지 않기 때문에 프로세스가 사용할 수 있는 메모리 자원을 최대한 활용할 수 있게 됩니다. 그러므로 Metaspace 메모리를 제한하여 운영할 필요가 있습니다.\n이러한 성능 제한은 Metaspace에만 적용되는 것이 아니라 다른 영역에서의 메모리 사이즈 제한이 필요합니다.\n안전하게 애플리케이션을 운영하기 위해서는 각 메모리 영역에 맞는 요구사항을 고려하여 결정하는 것이 바람직합니다.\nJava 어플리케이션은 크게 위의 Heap과 Off-Heap 두 공간을 활용하여 동작하는데, 따라서 어플리케이션을 배포할 때 메모리 몇 GB를 할당해야 하는지 결정하기 위해서는 단순히 Xmx(Heap 메모리 최대치를 결정하는 Java 옵션) 값만 생각하면 OOME에 빠지기 쉽다. 실제로는 Xmx에 MaxMetaspace값을 더하고, 추가로 프로그램에서 NIO를 사용해 Native Memory를 직접 할당받는 로직을 고려해서 Heap + Native Memory 사용총량으로 할당을 해야 비교적 정확하다. 특히 컨테이너의 경우 계산을 좀 더 정확하게 해야 시스템에서 OOM killed되는 상황을 면할 수 있다.\n출처 - https://www.samsungsds.com/kr/insights/1232761_4627.html\n정리 실제로 배포해보면서 컨테이너의 지속적인 메모리 사용량이 증가하는 것에 의문점을 가지게 되었습니다. 자바 컨테이너의 메모리 사용은 JVM이 컨트롤할 수 없는 부분이 많아 추적하기 어렵습니다3.\n그래서 자세히 확인할 수 있는 JVM 위주로 확인하게 되었습니다. non-heap 영역이 미세하게 상승하고 있었고, 이는 정상적인 지표라고 생각했습니다. 그러나 장기적으로는 메모리 낭비가 될 수 있으므로 메모리 사이즈를 제한하여 운영해야 합니다.\nReference https://swmobenz.tistory.com/37 https://hudi.blog/jvm-warm-up/ https://velog.io/@dongvelop/Spring-Boot-%EC%84%9C%EB%B2%84-%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81-%EB%8C%80%EC%8B%9C%EB%B3%B4%EB%93%9C%EB%B3%84-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EC%9C%A0%EC%9D%98%EC%82%AC%ED%95%AD#codeheap-non-profiled-nmethods-non-heap https://www.samsungsds.com/kr/insights/1232761_4627.html https://obv-cloud.com/41 https://swmobenz.tistory.com/37\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://12bme.tistory.com/382\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://stackoverflow.com/questions/53451103/java-using-much-more-memory-than-heap-size-or-size-correctly-docker-memory-limi\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-09-29T00:00:00+09:00","permalink":"https://s0okju.github.io/p/opsathlan-2-2/","title":"OpsAthlan 2 - 2. Advanced echo server 메트릭 분석"},{"content":"시나리오 이전글 을 보니 스프링 애플리케이션 컨테이너에 문제가 생긴 것 같다. spring boot 애플리케이션에 대한 지표를 수집할 수 있도록 환경을 구축해보자.\n환경 Ubuntu 22.04 Openstack 2024.1 전체 구상도 아래와 같이 구상됩니다. cAdvisor를 설정한 이유는 컨테이너 관련 메트릭을 수집하기 위해 설정했습니다. docker-compose를 통해 컨테이너를 배포했습니다.\n쿠버네티스는 컨테이너들의 리소스 지표를 제공하기 위해 cAdvisor를 기본적으로 제공합니다.1\n절차 Terraform, Ansible를 활용한 환경 구성 프로메테우스를 활용한 지표 수집 그라파나 시각화 환경 구성 클라우드 리소스의 구성 자체는 이전과 변화하지 않았습니다.\ndocker compose 다만 환경 구성은 변했습니다. 개별 컨테이너로 배포하는 것이 아닌 docker-compose를 통해 관련 컨테이너를 배포했습니다. 그 이유는 컨테이너의 개수가 많아짐에 따라 컨테이너를 큰 묶음으로 관리하기 위해서입니다.\n총 4개의 컨테이너를 배포하게 됩니다\nPrometheus Grafana Springboot application cAdvisor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # docker-compose.yml version: \u0026#34;3.7\u0026#34; services: # Prometheus service prometheus: image: prom/prometheus:latest container_name: prometheus volumes: - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml ports: - \u0026#34;9090:9090\u0026#34; # Expose Prometheus web UI networks: - monitoring # Grafana service grafana: image: grafana/grafana:latest container_name: grafana ports: - \u0026#34;3000:3000\u0026#34; # Expose Grafana web UI networks: - monitoring depends_on: - prometheus # Spring Boot app service advanced-echo: image: opsathlan-advanced-echo:latest container_name: opsathlan-advanced-echo ports: - \u0026#34;8080:8080\u0026#34; networks: - monitoring # cadvsior config cadvisor: container_name: cadvisor image: gcr.io/cadvisor/cadvisor:latest ports: - \u0026#34;8081:8080\u0026#34; volumes: - \u0026#34;/:/rootfs:ro\u0026#34; - \u0026#34;/var/run:/var/run:ro\u0026#34; - \u0026#34;/sys:/sys:ro\u0026#34; - \u0026#34;/var/lib/docker/:/var/lib/docker:ro\u0026#34; # docker container log networks: - monitoring networks: monitoring: driver: bridge 프로메테우스 프로메테우스가 수집한 메트릭 지표는 springboot container 관련 지표와 컨테이너 관련 지표입니다. 그래서 scrape_configs target에 관련 컨테이너 URL를 기입하게 됩니다.\n1 2 3 4 5 6 7 8 9 10 # prometheus.yml global: scrape_interval: 10s # How often to scrape targets scrape_configs: - job_name: \u0026#34;opsathlan-advanced-echo\u0026#34; static_configs: - targets: [\u0026#34;opsathlan-advanced-echo:8080\u0026#34;, \u0026#34;cadvisor:8080\u0026#34;] labels: group: \u0026#34;test\u0026#34; docker compose를 실행하는 Ansible playbook nova에서 docker-compose를 실행시키기 위해서는 관련 바이너리를 설치하고, 필요한 환경 파일을 인스턴스에 복사하는 작업이 필요합니다. docker-compose 라는 ansible role를 만들어 스프링 컨테이너 이미지를 빌드하고, docker compose를 실행시키는 테스크를 추가했습니다.\n개인적으로 작업한 도커 설치 ansible-role 를 확인해보면 docker 설치할때 docker-compose plugin를 설치하게 됩니다. 그래서 별도의 설치 없이 docker compose 명령어 만으로도 docker-compose 파일을 실행시킬 수 있게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 - name: Update apt cache apt: update_cache: yes - name: Install Docker Engine, CLI, containerd, and plugins apt: name: - docker-ce - docker-ce-cli - containerd.io - docker-buildx-plugin - docker-compose-plugin state: present 프로메테우스를 실행시키기 위해서는 config 파일을 nova 인스턴스에 직접 저장해야 하는 작업이 필요합니다. docker-compose.yml 를 다시 확인해보면 local volume이 ./prometheus/prometheus.yml 로 되어 있습니다. 파일 경로는 nova 인스턴스의 파일 경로로 해당 파일 경로에 맞게 설정 파일(prometheus.yml)를 저장해야 합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 # docker-compose.yml services: # Prometheus service prometheus: image: prom/prometheus:latest container_name: prometheus volumes: - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml ports: - \u0026#34;9090:9090\u0026#34; # Expose Prometheus web UI networks: - monitoring prometheus의 폴더를 만든 후에 복사하는 작업을 하면 파일이 저장됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 - name: Create Prometheus configuration directory file: path: \u0026#34;/home/ubuntu/prometheus\u0026#34; state: directory owner: \u0026#34;ubuntu\u0026#34; group: \u0026#34;ubuntu\u0026#34; mode: \u0026#34;0755\u0026#34; - name: Copy Prometheus config file copy: src: ./prometheus.yml dest: /home/ubuntu/prometheus/prometheus.yml owner: \u0026#34;ubuntu\u0026#34; group: \u0026#34;ubuntu\u0026#34; mode: \u0026#34;0644\u0026#34; docker compose 명령어를 수행하면 docker-compose 파일에 정의된 컨테이너들을 실행시킬 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 - name: Copy docker-compose.yml to the target machine copy: src: ./docker-compose.yml dest: /home/ubuntu/docker-compose.yml # Destination on the target machine owner: \u0026#34;ubuntu\u0026#34; group: \u0026#34;ubuntu\u0026#34; mode: \u0026#34;0644\u0026#34; - name: Run docker-compose up to start containers command: docker compose up -d args: chdir: /home/ubuntu/ become: true 프로메테우스를 활용한 지표 수집 메트릭 수집, 시각화, 알림, 서비스 디스커버리 기능을 모두 제공하는 오픈 소스 모니터링 시스템입니다. Pull 방식으로 메트릭을 수집하고 시계열 데이터베이스에 저장하는 특징을 가지고 있습니다.\npull 방식은 프로메테우스가 필요한 메트릭 지표를 대상 서버로부터 가져온다는 것과 같은 말입니다. 그래서 대상이 되는 서버는 프로메테우스가 메트릭을 받아갈 수 있도록 endpoint를 열어둬야 합니다.\n보통의 방식이라면 spring boot actuator를 활용하여 /actuator/metrics 엔드포인트를 여는 방법을 사용할 수 있습니다.\n1 2 3 4 5 # application.properties management.endpoints.web.exposure.include=prometheus,metrics management.endpoint.prometheus.enabled=true management.prometheus.metrics.export.enabled=true 위와 같은 방식을 사용하게 되면 EOF 오류가 발생해 프로메테우스가 정상적으로 지표를 가져올 수 없었습니다. 그래서 header를 TEXT_PLAIN으로 지정해주는 controller를 생성했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import io.micrometer.prometheusmetrics.PrometheusMeterRegistry; @RestController @RequiredArgsConstructor public class MetricsController { private final PrometheusMeterRegistry prometheusMeterRegistry; @GetMapping(\u0026#34;/metrics\u0026#34;) public ResponseEntity\u0026lt;String\u0026gt; metrics() { String scrape = prometheusMeterRegistry.scrape(); HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.TEXT_PLAIN); return new ResponseEntity\u0026lt;\u0026gt;(scrape, headers, HttpStatus.OK); } } springboot actuator 스프링부트 애플리케이션 모니터링과 관리에 필요한 기능을 제공합니다.\n9090 포트에 접속해 프로메테우스 web ui를 보면 스프링부트 애플리케이션과 cAdvisor 지표가 정상적으로 통신됨을 알 수 있습니다.\n그라파나를 통한 시각화 cAdvisor 지표 시각화 그라파나에서 container의 메모리 사용량을 확인해봅니다. 메모리 단위로 확인하기 위해 1024를 두번 나눴습니다.\n관련 지표는 cadvisor Github를 참고하길 바랍니다.\n1 container_memory_usage_bytes{name=\u0026#34;instance_name\u0026#34;}/1024/1024 아래의 그래프를 보면 계속 상승하고 있는 그래프를 확인할 수 있습니다.\n스프링 애플리케이션 관련 지표 시각화 저는 GrafanaLabs에서 제공해주는 JVM 대시보드를 사용했습니다. dashboard ip를 설정하면 다양한 그래프를 확인할 수 있습니다.\n아래의 사진은 일부만 나타낸 것이며 이것 이외에도 다양한 지표가 있습니다.\n다음글 지금까지 모니터링할 수 있는 환경을 구축했습니다. 다음편에는 시각화한 표를 분석해보는 시간을 가지도록 하겠습니다.\nhttps://themapisto.tistory.com/44\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-09-26T00:00:00+09:00","permalink":"https://s0okju.github.io/p/opsathlan-2-1/","title":"OpsAthlan 2 - 1. Advanced echo server 구축"},{"content":"시나리오 Hello World를 반환해주는 springboot 도커 서비스가 있다. 배포해보자!\n환경 ubuntu 22.04 openstack 2024.01 순서 Terraform을 활용한 인프라 구성 Ansible를 활용한 환경 구성 및 빌드 cAdvisor를 활용한 모니터링 cAdvisor를 선택한 이유 cAdvsior는 컨테이너의 Metrix를 모니터링할 수 있는 도구입니다. 기본적인 컨테이너 메트릭 정보를 우선 확인하기 위해 선택하게 되었습니다.\n전체 구성도 Terraform을 활용한 리소스 배포 배포해야 할 것은 아래와 같습니다.\nNova Instance Network Subnet Router Floating ip Security Security Group ssh, spring web port(8080), cAdvisor(8081)를 위한 rule 생성 ssh 통신을 위한 keypair Terraform code terraform은 절차 지향적이기 때문에 리소스 작성 순서가 중요합니다.그래서 간단한 nova 인스턴스 배포의 경우 아래와 같은 순서를 따랐습니다.\n네트워크 인스턴스 생성 floating ip 할당 전체 코드는 깃허브를 참고하길 바랍니다.\n네트워크를 생성한 이후 subnet를 생성한다. 외부와 통신하기 위해서는 external_network와 서브넷과의 연결이 필요합니다. 그러므로 router를 생성한 후 router_interface를 통해 external_network와 서브넷이 연결하게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 # Network 생성 resource \u0026#34;openstack_networking_network_v2\u0026#34; \u0026#34;opsathlan_network\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-network\u0026#34; admin_state_up = true port_security_enabled = var.port_security_enabled } # Subnet 생성 resource \u0026#34;openstack_networking_subnet_v2\u0026#34; \u0026#34;opsathlan_subnet\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-subnet\u0026#34; network_id = openstack_networking_network_v2.opsathlan_network.id cidr = var.subnet_cidr ip_version = 4 dns_nameservers = [\u0026#34;8.8.8.8\u0026#34;, \u0026#34;8.8.4.4\u0026#34;] } # Router 생성 resource \u0026#34;openstack_networking_router_v2\u0026#34; \u0026#34;opsathlan_router\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-router\u0026#34; admin_state_up = true external_network_id = data.openstack_networking_network_v2.ext_network.id } # Router Interface 생성 resource \u0026#34;openstack_networking_router_interface_v2\u0026#34; \u0026#34;router_interface\u0026#34; { router_id = openstack_networking_router_v2.opsathlan_router.id subnet_id = openstack_networking_subnet_v2.opsathlan_subnet.id } 인스턴스 생성 인스턴스에 접근하는데 사용되는 keypair를 생성합니다.\n1 2 3 4 5 # Keypair 생성 resource \u0026#34;openstack_compute_keypair_v2\u0026#34; \u0026#34;opsathlan_keypair\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-keypair\u0026#34; public_key = file(var.pubkey_file_path) } Nova 인스턴스의 접근 제어를 관리하는 Security group를 설정합니다. security group rule은 variables.tf 에서 리스트로 받은 후에 적용할 수 있도록 구성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Security Group 생성 resource \u0026#34;openstack_networking_secgroup_v2\u0026#34; \u0026#34;opsathlan_secgroup\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-secgroup\u0026#34; description = \u0026#34;OpsAthaln Security Group\u0026#34; delete_default_rules = false } # Security Group Rule 생성 resource \u0026#34;openstack_networking_secgroup_rule_v2\u0026#34; \u0026#34;secgroup_rules\u0026#34; { count = length(var.allowed_ports) direction = \u0026#34;ingress\u0026#34; ethertype = \u0026#34;IPv4\u0026#34; protocol = lookup(var.allowed_ports[count.index], \u0026#34;protocol\u0026#34;, \u0026#34;tcp\u0026#34;) port_range_min = lookup(var.allowed_ports[count.index], \u0026#34;port_range_min\u0026#34;) port_range_max = lookup(var.allowed_ports[count.index], \u0026#34;port_range_max\u0026#34;) remote_ip_prefix = lookup(var.allowed_ports[count.index], \u0026#34;remote_ip_prefix\u0026#34;, \u0026#34;0.0.0.0/0\u0026#34;) security_group_id = openstack_networking_secgroup_v2.opsathlan_secgroup.id } variables.tf 에서는 외부에서 접근할 수 있는 포트를 설정합니다.\nssh spring boot application(8080) cAdvisor(8081) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // variables.tf variable \u0026#34;allowed_ports\u0026#34; { description = \u0026#34;List of maps defining allowed ports for the security group. Each map should include protocol, port_range_min, port_range_max, and optionally remote_ip_prefix.\u0026#34; type = list(object({ protocol = string port_range_min = number port_range_max = number remote_ip_prefix = optional(string, \u0026#34;0.0.0.0/0\u0026#34;) })) default = [ { protocol = \u0026#34;tcp\u0026#34; port_range_min = 22 port_range_max = 22 }, { protocol = \u0026#34;tcp\u0026#34; port_range_min = 8080 port_range_max = 8080 }, { protocol = \u0026#34;tcp\u0026#34; port_range_min = 8081 port_range_max = 8081 } ] } Floating ip 할당 및 설정 floating ip를 생성한 후에 nova instance에 붙힙니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 5. Floating IP 생성 및 연결 resource \u0026#34;openstack_networking_floatingip_v2\u0026#34; \u0026#34;floating_ip\u0026#34; { pool = data.openstack_networking_network_v2.ext_network.name } # 4. Nova Instance 생성 resource \u0026#34;openstack_compute_instance_v2\u0026#34; \u0026#34;nova_instance\u0026#34; { name = \u0026#34;opsathlan-${var.scenario_id}-instance\u0026#34; image_name = var.image_name flavor_name = var.flavor_name key_pair = openstack_compute_keypair_v2.opsathlan_keypair.name security_groups = [openstack_networking_secgroup_v2.opsathlan_secgroup.name] network { uuid = openstack_networking_network_v2.opsathlan_network.id } depends_on = [openstack_networking_subnet_v2.opsathlan_subnet] } resource \u0026#34;openstack_compute_floatingip_associate_v2\u0026#34; \u0026#34;floating_ip_association\u0026#34; { floating_ip = openstack_networking_floatingip_v2.floating_ip.address instance_id = openstack_compute_instance_v2.nova_instance.id } Ansible를 활용한 환경 구성 및 컨테이너 실행 Ansible를 활용하여 컨테이너 이미지를 빌드하고 실행시킬 수 있는 환경을 구성해야 합니다. 그래서 총 4개의 role를 만들었습니다.\njava 설치 docker 설치 springboot 이미지 빌드 및 수행 cAdvisor 실행 🤔 여담\nJenkins와 같이 자동화 도구를 사용할 수 있지만 초반에는 불필요하다고 생각했습니다. 배포하고자 하는 애플리케이션의 규모는 현저하게 작았고, 난이도가 있어 잘못하면 예상보다 시간을 많이 할애할 수 있다고 판단했습니다.\nspring boot application 배포 application 소개 배포 애플리케이션은 간단하게 hello world만 반환하는 웹 서비스입니다.\n1 2 3 4 5 6 7 8 @RestController public class EchoController { @GetMapping(\u0026#34;/echo\u0026#34;) public String echo() { return \u0026#34;Hello, World!\u0026#34;; } } 빌드 및 실행 Ansible를 활용하여 spring boot application 컨테이너를 실행합니다. 이때 컨테이너는 8080 외부 포트로 열어둡니다.\n순서\nRepository clone Spring boot application build Container Image build Docker 실행 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 --- - name: Clone Spring Boot application repository git: repo: \u0026#34;https://github.com/S0okJu/OpsAthlan.git\u0026#34; dest: /home/ubuntu/app/spring-boot-app version: sc/1 - name: Build Spring Boot application using Gradle command: ./gradlew clean build args: chdir: /home/ubuntu/app/spring-boot-app/scenario_01/spring_echo register: gradle_build failed_when: gradle_build.rc != 0 - name: Check if JAR file exists stat: path: /home/ubuntu/app/spring-boot-app/scenario_01/spring_echo/build/libs/ register: jar_dir - name: Fail if JAR file does not exist fail: msg: \u0026#34;JAR file was not built successfully.\u0026#34; when: jar_dir.stat.exists == False or jar_dir.stat.isdir == False - name: Copy Dockerfile for Spring Boot copy: src: Dockerfile dest: /home/ubuntu/app/spring-boot-app/scenario_01/spring_echo/Dockerfile - name: Build Docker image for Spring Boot application docker_image: name: echo-springboot-app tag: latest source: build build: path: /home/ubuntu/app/spring-boot-app/scenario_01/spring_echo - name: Run Spring Boot container docker_container: name: sc1-springboot-app image: echo-springboot-app:latest state: started ports: - \u0026#34;8080:8080\u0026#34; 자세한 코드는 github 참고 바랍니다.\ncAdvisor cAdvisor는 컨테이너 이미지를 다운받고 컨테이너를 실행할 수 있도록 구성합니다. 컨테이너는 8081로 외부포트를 열어 개발자가 대시보드를 확인할 수 있도록 설정합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 --- - name: Pull cAdvisor image docker_image: name: google/cadvisor:latest source: pull - name: Run cAdvisor container docker_container: name: cadvisor image: google/cadvisor:latest state: started ports: - \u0026#34;8081:8080\u0026#34; volumes: - \u0026#34;/:/rootfs:ro\u0026#34; - \u0026#34;/var/run:/var/run:ro\u0026#34; - \u0026#34;/sys:/sys:ro\u0026#34; - \u0026#34;/var/lib/docker/:/var/lib/docker:ro\u0026#34; 컨테이너 지표 모니터링 cAdvisor 접속 {floating_ip}:8081에 접속합니다. container 창에 들어가 springboot application을 확인합니다.\nspringboot application에 대한 기본적인 지표를 확인할 수 있습니다.\nTest 자체적으로 만든 test 도구인 gonetworker를 활용하겠습니다.\nGoNetWorker\n자체적으로 만든 네트워크 테스트 도구입니다. 서버에 request를 연속에서 요청한다는 특징이 있습니다.\n블로그 작성 기준 GoNetWorker는 동시로 전송하는 기능이 없어 순차적으로 request를 요청합니다. request 요청 이후 무작위로 sleep합니다. 연속적으로 리소스를 사용하지 않을 것이라고 예상할 수 있습니다.\nNetwork 네트워크 처리율을 보면 처리가 되었다가 0byte로 떨어졌다가 다시 상승합니다. 이는 저희가 사용한 도구인 GoNetWorker의 특징 때문에 발생한 그래프입니다.\n그 이외에도 아래와 같은 특징을 알 수 있습니다.\n평균적으로 1200bytes를 넘어가지 않는다. Memory 그에 비해 memory는 점진적으로 상승하는 그래프를 보여주고 있습니다. 그래프를 보니 웹 애플리케이션이 불필요한 메모리 자원을 낭비한다고 생각했습니다.\n원인을 구체적으로 파악하기 위해서는 springboot에 대한 메트릭이 필요합니다. cAdvisor 만으로는 정교한 메트릭 수집에 한계가 있으므로 프로메테우스를 활용해야 할 것 같습니다.\n다음편에는 \u0026hellip; 다음 편에는 프로메테우스를 활용하여 springboot 관련 메트릭스를 수집하고 문제를 해결해 보겠습니다.\n","date":"2024-09-19T00:00:00+09:00","permalink":"https://s0okju.github.io/p/opsathlan-1/","title":"OpsAthlan 1 - Simple echo server"},{"content":"개인적으로 운영 모니터링을 공부하고 싶었다. 문제는 클라이언트가 없다는 것이었다. 대안으로 스트레스 테스트 도구를 사용해서 극한의 상황을 확인할 수 있다. 나는 현실적인 상황을 재현하고 싶었다. 그래서 사용자가 직접 내 서버를 사용한 것처럼 패킷을 보내는 도구를 만들기로 결심했다.\nGoNetWorker GoNetWorker는 Go를 활용한 가상 운영 시뮬레이션 도구이다.1 미리 설정한 endpoint 정보를 기반으로 서버에 랜덤으로 request를 보내는 도구이다.\nendpoint 정의 - works.json 테스트를 하기 위해서는 endpoint에 대한 정보를 정확하게 기입해야 한다.\nsettings request를 보내기 위한 설정 정보 works request를 보낼 대상에 대한 정보 tasks url의 경로 정보 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;settings\u0026#34;: { \u0026#34;sleep_range\u0026#34;: 5 // request 전송 시 대기시간 범위 }, \u0026#34;works\u0026#34;: [ { \u0026#34;uri\u0026#34;: \u0026#34;http://localhost\u0026#34;, \u0026#34;port\u0026#34;: 8080, \u0026#34;tasks\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;/users/1\u0026#34;, \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34; } ] } ] } 구조 GoNetWorker는 사용자가 특별한 행동을 취하기 전까지 지속적으로 request를 보낸다. 초기에는 ctrl + c를 클릭했을 경우 실행이 중단될 수 있도록 구현했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 go func() { for { select { case \u0026lt;-stopChan: fmt.Println(\u0026#34;Received interrupt signal. Shutting down...\u0026#34;) return default: work := config.Works[rand.Intn(len(config.Works))] task := work.Tasks[rand.Intn(len(work.Tasks))] wg.Add(1) client := \u0026amp;http.Client{} go makeRequest(work, task, client, \u0026amp;wg, config.Settings.SleepRange) randomSleep := time.Duration(rand.Intn(config.Settings.SleepRange)) * time.Second time.Sleep(randomSleep) } } }() 세부적인 구조는 아래와 같다.\n예시 1 go run main.go GoNetWorker로 실행되면 아래와 같이 랜덤으로 request를 보내게 된다.\n서버에서 확인하면 아래와 같이 정상적으로 request를 받았음을 알 수 있다.\n보완할 점 초기 버전이라 추가해야할 점이 많다. 다음 글에서는 아래의 기능을 추가하고 찾아오겠다.\nGet으로 전송할 경우 보완 path에 식별자가 있을 경우 parameter가 있을 경우 Post로 전송할 경우 go를 빌드하기 위한 makefile 이렇게 설명하는게 맞는지 모르겠다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-09-15T00:00:00+09:00","permalink":"https://s0okju.github.io/p/gonetworker-1/","title":"GoNetWorker - 1. GoNetWorker 소개와 초기 기능"},{"content":"Kubernetes 설치는 변재한님의 블로그를 참고했습니다.\nAnsible Ping 문제 Ansible Ping 수행 시 모든 노드에 아래와 같은 오류가 발생했다. 에러 메세지를 확인해보니 ssh 키가 제대로 적용되지 않은 것처럼 보였다.\n1 2 3 4 5 Kubernetes-k8s-node-nf-2 | UNREACHABLE! =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Data could not be sent to remote host \\\u0026#34;10.0.2.176\\\u0026#34;. Make sure this host can be reached over ssh: kex_exchange_identification: Connection closed by remote host\\r\\nConnection closed by UNKNOWN port 65535\\r\\n\u0026#34;, \u0026#34;unreachable\u0026#34;: true } Ansible은 ssh를 활용하여 target 노드와 통신한다.\n해결책 1. 공개키 알고리즘 변경 openstack log를 확인해보니 debian image에 공개키가 authorized_keys에 제대로 복사되지 않아 ssh 통신을 제대로 수행하지 못한 것이었다.\nssh 키를 ed25519 알고리즘으로 생성하면 키가 제대로 복사되지 않았지만 rsa로 변경하면 아래와 같이 키가 복사된다.\n보안을 고려하면 rsa보다는 ed25519 알고리즘이 권장된다.1\n다시 ping을 수행하면 제대로 수행된다.\n해결책 2. ssh-agent 실행 만약에 원격으로 openstack 서버에 접속한다고 가정해보자. 사용자를 변경하거나 세션을 한번 끊게 되면 동일한 이유로 문제가 생긴다.\n우선 우리가 초기에 ansible를 세팅할 때 ssh-agent를 실행하고 private key를 추가하는 작업을 한다. ssh-agent는 ssh-add 명령어를 활용하여 키를 기억하고 사용하게 된다.\n1 2 3 4 # 백그라운드에 agent 실행 eval \u0026#34;$(ssh-agent -s)\u0026#34; # 키 추가 ssh-add ~/.ssh/id_rsa ssh-agent는 프로세스로써 작동돼 중간에 ssh가 중단되더라도 프로세스가 남아있다. 그러므로 현재 사용자(stack)가 실행시킨 프로세스를 먼저 없애고 ssh-agent를 실행시키도록 구현하면 된다.\nopenstack은 stack 사용자가 관리한다. stack 사용자의 ~/.bashrc 에 ssh-agent를 추가하여 stack 사용자로 로그인 성공할때마다 실행할 수 있도록 한다.\n1 2 3 4 5 6 7 8 9 # Ansible ssh agent ## Delete used ssh-agent if [ -f ~/scripts/delete_ssh_agent.sh ]; then . ~/scripts/delete_ssh_agent.sh fi ## Create new ssh-agent eval $(ssh-agent -s) ssh-add ~/.ssh/id_rsa.kubespray delete_ssh_agent.sh 은 openstack을 관리하는 사용자(stack)가 가지고 있는 ssh-agent를 삭제하는 스크립트이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash # Get all PIDs of ssh-agent processes owned by the user \u0026#39;stack\u0026#39; pids=$(pgrep -u stack ssh-agent) # Check if any PIDs were found if [ -z \u0026#34;$pids\u0026#34; ]; then echo \u0026#34;No ssh-agent processes found for user \u0026#39;stack\u0026#39;.\u0026#34; exit 1 else # Loop through each PID and attempt to kill the process for pid in $pids; do echo \u0026#34;Killing ssh-agent process with PID $pid...\u0026#34; kill \u0026#34;$pid\u0026#34; sleep 1 # Check if the process was successfully killed if pgrep -u stack -x ssh-agent \u0026gt; /dev/null; then echo \u0026#34;Failed to kill process with PID $pid. You may need to use sudo.\u0026#34; else echo \u0026#34;Successfully killed process with PID $pid.\u0026#34; fi done fi 새롭게 접속하면 아래와 같이 정상적으로 실행되는 것을 알 수 있다.\n이 방식은 단일 사용자만 관리한다는 가정하에서 만든 해결책이다.\nNameserver 문제 playbook를 배포해보자.\n1 ansible-playbook --become -i inventory/test-cluster/hosts cluster.yml nameserver로 인해 문제가 발생했다. 보통 이런 문제는 nameserver의 설정이 잘못된 경우에 발생되며 안전한 google dns(8.8.8.8, 8.8.4.4)로 설정하면 해결된다.\n1 \u0026#34;msg\u0026#34;: \u0026#34;E: Failed to fetch http://security.debian.org/debian-security/pool/updates/main/p/python-apt/python-apt-common_1.8.4.3_all.deb Temporary failure resolving \u0026#39;deb.debian.org\u0026#39;\\nE: Failed to fetch http://security.debian.org/debian-security/pool/updates/main/p/python-apt/python3-apt_1.8.4.3_amd64.deb Temporary failure resolving \u0026#39;deb.debian.org\u0026#39;\\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\u0026#34;, 해결책 1. resolv.conf 수정(잘못된 접근) DNS를 설정하는 resolv.conf 를 확인해보면 127.0.0.53로 설정되어 있다. 안전하게 google dns 서버로 변경해보자\n1 2 3 # Dynamic resolv.conf(5) file for glibc resolver(3) generated by resolvconf(8) # DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN nameserver 127.0.0.53 실제로는 우리가 직접 변경하면 안된다.\nDO NOT EDIT THIS FILE BY HAND \u0026ndash; YOUR CHANGES WILL BE OVERWRITTEN\nresolv.conf은 /run 폴더 내에 심볼링 링크가 되어 있기 때문에 마음대로 수정하는 것은 권장하지 않는다.2\n실제로 virt-customize 를 활용해서 resolv.conf를 수정하는 이미지를 별도로 만들었지만 의미가 없었다.\n해결책 2. Terraform 수정 가장 안전한 접근법이다. 네트워크 토폴로지를 보면 쿠버네티스 노드들이 kubernetes-network subnet를 활용한다.\nOpenstack의 subnet에 DNS nameserver를 명시할 수 있다. 그러므로 Openstack 인스턴스를 배포하는 terraform 파일을 수정한다. ~/kubespray/contrib/terraform/openstack/modules/network/main.tf 내에 dns_nameservers 를 google dns로 변경시킨다.\n1 2 3 4 5 6 7 8 resource \u0026#34;openstack_networking_subnet_v2\u0026#34; \u0026#34;k8s\u0026#34; { name = \u0026#34;${var.cluster_name}-internal-network\u0026#34; count = var.use_neutron network_id = openstack_networking_network_v2.k8s[count.index].id cidr = var.subnet_cidr ip_version = 4 dns_nameservers = [\u0026#34;8.8.8.8\u0026#34;,\u0026#34;8.8.4.4\u0026#34;] } 원래라면 variables.tf를 수정해야 했지만 제대로 적용이 되지 않았고, google dns server는 거의 불변에 가깝기 때문에 main.tf에 직접 작성해도 된다고 판단했다.\n아래와 같이 제대로 적용된다.\n그 이외의 debian 문제들 그 이후부터 debian에는 잡다한 문제가 발생했다.\n해결책 1. 클라우드 이미지를 ubuntu로 변경 클라우드 이미지를 debian에서 ubuntu로 변경했다. ssh 키를 ed25519 알고리즘으로 적용해도 정상적으로 작동된다. Ansible playbook를 작동시키면 정상적으로 kubernetes가 배포된다.\nhttps://dev.to/ccoveille/how-to-generate-a-secure-and-robust-ssh-key-in-2024-3f4f\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://askubuntu.com/questions/351168/diffrence-between-the-dns-setting-in-etc-resolv-conf-and-etc-network-interfaces\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-08-17T00:00:00+09:00","permalink":"https://s0okju.github.io/p/openstack-kubernetes-installation-2/","title":"Openstack - Kubespray를 활용한 Kubernetes 설치 시 Ansible 오류"},{"content":"Openstack 설치 im2sh.log velog을 참고해서 Devstack을 활용해 Openstack을 설치하면 된다. 다만 로컬 환경에서만 수행할 예정이므로 가상 public ip 대역 생성은 제외했다.\n이미지 생성 Nova에 올린 이미지를 생성한다. Ubuntu보다 Debian이 가볍고 커스텀하기 용이해서 선택1하게 되었다. 아래의 명령어는 debian 공식 홈페이지를 참고했다.\n1 2 3 4 5 6 7 wget https://cloud.debian.org/cdimage/cloud/OpenStack/current-10/debian-10-openstack-amd64.qcow2 openstack image create \\ --container-format bare \\ --disk-format qcow2 \\ --file debian-10-openstack-amd64.qcow2 \\ debian-10-openstack-amd64 dashboard를 보면 정상적으로 이미지가 업로드된 것을 알 수 있다.\nKubespray를 활용한 Kubernetes 구축 OS - Ubuntu 24.04 Openstack - 2024.1 Kubespray - v2.21.0 Terraform - v1.9.3 전반적인 설치 과정은 변재한님의 velog를 참고했습니다.\nopenstack v2, v3 API는 다른점이 많습니다. 만약에 v2 버전으로 하고 싶다면 운영체제와 kubespray, terraform 버전을 맞추는 것이 좋습니다.\nssh 통신을 위한 공개키 생성 쿠버네티스 환경 설정을 위해 Ansible를 사용한다. Ansible은 ssh를 활용하여 대상에 접속할 수 있다. 이 키는 추후 Kubernetes에서 노드끼리 통신할 수 있는 openstack의 keypair로써 활용된다.\n1 2 3 4 5 ssh-keygen -t ed25519 -N \u0026#39;\u0026#39; -f ~/.ssh/id_rsa.kubespray eval $(ssh-agent -s) ssh-add ~/.ssh/id_rsa.kubespray 오류 - One of auth_url or cloud must be specified 결론부터 말하자면 클라이언트와 관련된 환경 변수가 저장되지 않아서 생긴 일이다.\n해결 과정 아래의 오류를 읽어보면 openstack의 provider를 정의하는 tf파일에 속성이 정의되어 있지 않다고 한다.\n~/kubespray/contrib/terraform/openstack/version.tf 를 보면 provider가 정의되어 있다.\n공식 홈페이지를 보면 설정값을 추가할 수 있는데 전부다 선택사항이지만 Openstack 설정과 관련 환경변수가 반드시 저장되어 있어야 한다. 그렇지 않으면 terraform에 별도로 설정해야 한다.\n1 2 3 4 5 6 7 8 # Configure the OpenStack Provider provider \u0026#34;openstack\u0026#34; { user_name = \u0026#34;admin\u0026#34; tenant_name = \u0026#34;admin\u0026#34; password = \u0026#34;pwd\u0026#34; auth_url = \u0026#34;http://myauthurl:5000/v3\u0026#34; region = \u0026#34;RegionOne\u0026#34; } 즉 위와 같은 문제가 발생한 것은 클라이언트와 identity 서비스가 상호 작용을 위한 환경 변수가 없어서 생긴 일인 것이다.\n다만 수동으로 terraform 파일에 작성하거나 환경변수를 설정하게 되면 Domain 정보가 부정확하다는 에러 메세지를 마주할 수 있다. openstack 버전에 따라서 지원하는 환경 변수도 다르거나, 같은 조건에 사람마다 실행 가능 여부가 다른 등 복합적인 이유가 존재한 것 같았다.2\n해결책 Openstack에서 제공해주는 클라이언트 환경 스크립트(openrc)를 사용하면 된다.\nhorizon에 사용자를 클릭하면 openstack rc 파일이 있는데, 이를 다운로드해서 실행시키면 된다.\nLinux에서 환경 변수는 bash를 exit하는 순간 사라진다.3 컴퓨터가 한번 꺼지게 되면 openrc 스크립트를 계속 실행시켜줘야 한다. 환경변수를 영속적으로 저장하는 방법은 ~/.bashrc를 설정하는 것이다. 다운로드한 rc 파일을 실행시키도록 파일을 설정한다.\n1 2 3 if [ -f ~/rc/k8s-openrc.sh ]; then . ~/rc/k8s-openrc.sh fi 중간에 비밀번호를 입력 받아야 하므로 openrc 스크립트에 비밀번호를 넣는다. 개인적으로 이런 방법은 보안 측면에서는 좋지 않아 보인다.\n1 2 3 4 5 export OS_USERNAME=\u0026#34;k8s\u0026#34; # With Keystone you pass the keystone password. # echo \u0026#34;Please enter your OpenStack Password for project $OS_PROJECT_NAME as user $OS_USERNAME: \u0026#34; # read -sr OS_PASSWORD_INPUT export OS_PASSWORD=\u0026#34;yourpassword\u0026#34; source ~/.bashrc를 수행하면 아래와 같이 환경 변수가 정상적으로 설정된다.\n다시 terraform으로 kubernetes를 설치하면 정상적으로 실행된다.\nhttps://www.quora.com/Which-Linux-distribution-is-best-suited-for-a-cloud-server-environment-and-why\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/terraform-provider-openstack/terraform-provider-openstack/issues/267\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://askubuntu.com/questions/395212/why-environment-variable-disappears-after-terminal-reopen\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-08-01T00:00:00+09:00","permalink":"https://s0okju.github.io/p/openstack-kubernetes-installation-1/","title":"Openstack - Kubespray를 활용한 Kubernetes 설치 시 Terraform 오류"},{"content":"서버 상태 지금까지 Multipass 를 활용하여 쿠버네티스 환경을 만들고, 실제로 적용하는 과정을 거쳤다. 현재 3개의 노드로 구성되어 있으며, nfs, mysql, mongo와 같이 데이터베이스가 있는 인스턴스는 nfs, 젠킨스 서버는 ops 인스턴스에 저장했다.\n왜 데이터베이스를 쿠버네티스의 statefulset으로 정의하지 않고 별도로 설치했는지 궁금할 것이다. 이유는 최대한 클라우드 스토리지처럼 구현하고 싶어서였다.\n대시보드 확인하기 일단 쿠버네티스 내에서는 외부와 통신할 수 있는 기능은 크게 두 가지이다.\nIngress Nodeport 일단 사설 네트워크 내에서만 사용할 예정이므로 Nodeport 방식을 사용하도록 하겠다.\n예제 - Grafana ui 확인하기 예로 프로메테우스를 사용한다고 가정해보자. grafana 대시보드에 접근할 필요성이 있을 것이다.\ngrafana ui와 관련된 애플리케이션의 service type를 nodeport로 변경해준 후에 Host server(192.168.50.27)에 방화벽을 설정하면 192.168.50.27 주소로 Grafana ui에 접근할 수 있다. 그러나 문제도 존재한다. 서버에 문제가 생기면 재배포하게 되는데, 쿠버네티스 스케줄러에 따라서 다른 노드에 배치될 수 있다. 이럴 경우 배치된 노드를 확인해가며 기존의 방화벽을 닫고, 새로운 방화벽을 열어줘야 한다.\n쿠버네티스에서는 여러 스케줄링 방식이 있는데1, 그 중 간단한 nodeSelector 로 node1에 강제 배치하여 정적으로 사용할 수 있도록 구성했다.\nnodeSelector 설정 node1에 label를 type=ui로 설정한다.\n1 kubectl label nodes node1 type=ui grafana ui service type 변경 Prometheus, Grafana의 원활한 설치를 위해 kube-prometheus helm을 사용했다. helm 차트의 특성상 values.yaml를 설정하여 값을 주입하므로 values.yaml 를 확인해야 한다. kube-prometheus helm 차트는 여러 helm 차트로 구성되어 있어, 설정하고 싶은 서비스의 values.yaml를 설정하면 된다.\nkube-prometheus-stack/charts/grafana/values.yaml 파일에 대략 216번째 줄에 아래와 같은 서비스 타입을 Nodeport로 변경한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service). ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it. ## ref: http://kubernetes.io/docs/user-guide/services/ ## service: enabled: true # type: ClusterIP type: NodePort # Custom loadBalancerIP: \u0026#34;\u0026#34; loadBalancerClass: \u0026#34;\u0026#34; loadBalancerSourceRanges: [] port: 80 targetPort: 3000 nodePort: 30060 # Custom helm nodeSelector 설정 kube-prometheus-stack/charts/grafana/values.yaml 의 314번째 줄에 node1에 배치되도록 설정했다.\n1 2 3 4 5 ## Node labels for pod assignment ## ref: https://kubernetes.io/docs/user-guide/node-selection/ # nodeSelector: type: ui helm으로 배포한 후 pod 리스트를 보면 {helm으로 배포한 이름}-grafana-{문자열} 보이는데, 파드 내부에는 grafana와 관련된 컨테이너가 포함되어 있다. 즉, 우리가 찾던 ui도 grafana pod 내부에 있다는 것이다.\ngrafana 파드의 정보를 보면 nodeSelector가 정상적으로 적용되었음을 알 수 있다.\n방화벽 설정 node1 인스턴스의 nodeport(30060)에 트래픽이 들어올 수 있도록 허용한다.\n1 2 sudo iptables -t nat -I PREROUTING -i enp3s0 -p tcp --dport 30060 -j DNAT --to-destination 10.120.52.23:30060 sudo iptables -I FORWARD 1 -p tcp -d 10.120.52.23 --dport 30060 -j ACCEPT 30060 포트로 enp3s0 인터페이스에 들어오는 트래픽을 10.120.52.23:30060로 리다이렉션한다. 10.120.52.23:30060에 접속하는 트래픽을 허용한다. 서버의 IP로 접속하면 그라파나 대시보드를 확인할 수 있다.\n문제점 Multipass로 인스턴스를 만들고, 외부와의 접속을 위해 트래픽을 제어하는 것은 문제점이 있다.\n외부와의 접속을 할때마다 iptable를 설정해야 한다.\n서버가 shutdown이 될 경우 이전에 설정했던 iptables rule이 사라진다. 이럴 경우 규칙을 다시 설정하거나, 이를 대비해서 별도의 패키지를 설치해서 iptables이 영속성이 있도록 설정2해야 한다. 인스턴스가 많으면 많을수록 관리하기 어렵다.\n쿠버네티스 이외에도 다양한 인스턴스가 있지만 일리리 명령어로 상태를 확인하고, 설정하는 것은 번거롭다. 그러므로 오케스트레이션 도구가 필요했다. 위의 해결책으로 OpenStack 서버를 구축하고자 한다. 인스턴스를 public cloud 처럼 자유자재로 생성 및 삭제할 수 있으며, 네트워크 설정이 용이했다. 또한 클라우드에 사용되는 기술을 대부분 활용할 수 있어 클라우드를 알아가는데 좋은 도구라고 생각했다.\n다음 포스팅은 기존의 서버를 포맷시키고, openstack 서버 구축기로 찾아오겠다.\nhttps://kubernetes.io/ko/docs/tasks/configure-pod-container/assign-pods-nodes/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://unix.stackexchange.com/questions/52376/why-do-iptables-rules-disappear-when-restarting-my-debian-system\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-07-16T00:00:00+09:00","permalink":"https://s0okju.github.io/p/server-access-dashboard-and-management-problems/","title":"서버 구축기 - 3. 쿠버네티스에서 대시보드 접속하기 그리고 운영의 문제점"},{"content":"들어가며 쿠버네티스에서 유상태 애플리케이션을 실행할때 주로 PV, PVC를 활용해서 데이터를 저장한다. 위치에 따라 내부, 외부 저장 유무를 결정하는데, 내부적으로 저장하는 것은 접근성도 떨어질 뿐만 아니라 노드를 운용하는데 비효율적이라고 판단했다. 외부적인 방안은 네트워크를 통한 데이터 저장이다.\n그렇다면 온프로미스 환경에서 네트워크로 통신하여 저장할 수 있는 방법이 있을까? 직접 쿠버네티스 환경에 구축하여 적용하고자 한다.\nNFS(Network File System) 네트워크에 파일을 저장하는 방식이다. 즉, 원격 컴퓨터에 있는 파일 및 디렉토리에 엑세스하고, 해당 파일 디렉토리가 로컬에 있는 것처럼 사용하는 분산 파일 시스템이다.\n파일 시스템 중에 ZFS가 있다. 중간에 두 개의 개념이 혼용돼서 NFS에 ZFS 파일 시스템을 설치했다. 그리고 다시 포맷팅했다.. 자세히 알고 싶으면 토스 블로그, OpenZFS로 성능과 비용, 두 마리 토끼 잡기 참고하길 바란다.\n용량 부족 문제 유상태 애플리케이션은 무엇이 있을까? 대부분 데이터베이스를 꼽을 것이다. 데이터베이스는 생각보다 큰 용량을 차지한다. 자체 서버는 SSD 1TB용량을 가지고 있다. 그러나 4개의 노드 인스턴스와 시스템 데이터, 미래에 추가할 인스턴스까지 더하면 많은 용량이 요구된다. 개인적으로 SSD라는 비싼 저장소를 단순 데이터 보관용으로 사용하기에는 많이 아깝다고 생각했다.\n다행히 DAS와 4TB HDD가 있었다. SSD에는 시스템에 활용될 데이터를 저장하고, 그 외의 데이터는 DAS에 저장하도록 구성하려고 한다.\n구성 가상 환경을 하나 만들어서 NFS 관련 패키지를 설치하고, 각각의 노드가 네트워킹을 통해 접속할 수 있게 구성할 예정이다.\n구성도 구성을 간략하게 설명하자면 아래와 같다.\nNFS 서버 전용 가상 머신을 만든다. Host 서버에 DAS를 마운팅하고, NFS Server 인스턴스에 host에 마운팅된 경로를 다시 마운트한다. 환경 설정하기 Disk Format DAS에 있는 HDD는 4TB로 1TB, 1TB, 1.8TB로 파티셔닝했다.\n디스크 포맷 및 파티셔닝은 무중력 인간님의 블로그를 참고하자.\n부팅될때마다 자동 마운팅이 될 수 있도록 fstab 파일을 수정했다.\n문제1. UUID가 없는 경우 ext4로 포맷이 안된경우 PARTITION UUID만 있다. 그러므로 mkfs.ext4 를 활용하여 ext4를 포맷시키자.\n문제 2. Emergency Mode fsatab를 잘못 설정하면 Linux는 최소한의 권한만 제공하는 emergency mode로 도입하게 된다. fstab 파일을 적절하게 기입하자.\nNFS 설치하기 NFS Multipass mounting 1 multipass mount /mnt/das/vol1 nfs:/nfs/vol1 NFS Server 설치 NFS multipass Instance 내에 NFS 서버 라이브러리를 설치하자. 설정은 Jung-taek Lim님의 블로그를 참고했다.\n1 2 3 sudo apt-get install nfs-common nfs-kernel-server rpcbind portmap systemctl enable nfs-kernel-server systemctl start nfs-kernel-server 네트워크 범위를 10.120.52.0/24로 지정했는데, 이는 multipass instance가 10.120.52.0/24 범위 내에 있기 때문이다.\nmulitpass에서 사용되는 네트워크 드라이브를 보면 qemu를 사용한다.\n네트워크 인터페이스를 보면 사용가능한 IP 범위는 10.120.52.0/24 임을 알 수 있다.\n1 2 3 4 5 6 7 sudo mkdir -p /nfs/vol1 sudo chown nobody:nogroup /nfs/vol1 # nfs가 공유할 수 있는 네트워크 설정 sudo vim /etc/exports/nfs/vol1 10.120.52.0/24(rw,sync,no_subtree_check,no_root_squash) # 위의 설정 반영 sudo exportfs -r 마스터 노드에 showmount하여 NFS가 제대로 작동되었는지 확인했을때 아래와 같이 export list가 잘 보여지면 된다.\nDynamic Provisioner 쿠버네티스에서 스토리지를 사용하는 방식을 생각해보자. 전역적인 데이터 스토리지에 접근하기 위해서는 [[D K8S- Pod|Pod]]를 생성할때 PVC를 생성하여, 관리자가 생성한 PV와 연결하게 된다. 그러나 파드를 생성할때마다 PV, PVC를 만들어서 스토리지에 접근해야 한다. 이와 같이 번거로움을 해결하고자 자동으로 볼륨을 생성해주는 Dynamic Provisioner를 사용하게 된다.\nProvisioner는 kubernetes-sigs Github에 있는 nfs-subdir-external-provisioner를 사용했다.\n위의 파일을 적절하게 설정하게 test를 실행시키면, 아래와 같이 pvc가 만들어지게 된다. 또한 파드를 삭제하게 되면 자동으로 pvc가 삭제되는 것을 볼 수 있다.\n참고 https://gruuuuu.github.io/cloud/k8s-volume/ https://do-hansung.tistory.com/57 https://1week.tistory.com/114 ","date":"2024-07-14T00:00:00+09:00","permalink":"https://s0okju.github.io/p/server-setup-nfs-in-multipass-instance/","title":"서버 구축기 - 2. Multipass를 활용한 NFS 구성"},{"content":"사이드 프로젝트를 직접 운영하기 위해 서버를 구축하기로 결심했다. 그래서 기존에 있는 Windows 데스크톱을 ubuntu 20.04 서버 환경으로 세팅했다.\nRTL8125 오류 여기서 문제가 발생했다. LAN 카드 정보 확인 결과, 네트워크 인터페이스가 제대로 인식되지 않았던 것이었다. 랜카드가 RTL8125의 경우에는 별도의 드라이버 설치가 필요했다. 하나의 모순이 생겼다. 우리가 패키지를 설치할때는 네트워크 연결이 필수지만, 네트워크에 문제가 생겨 외부 패키지를 다운받을 수 없던 것이었다. 물론 해결책이 없던 것이 아니였다.\n안드로이드 폰을 이용해 tethering해서 네트워크 연결을 시키고 해당 라이브러리를 설치1 20.04 은 커널 버전을 5.4를 쓰는데 이 버전에서 LAN이 인식이 안돼 5.8 이상 버전을 설치 2 슬프게도 나는 두 방법에 실패하게 되었고 22.04LTS를 설치하게 되었다. 그리고 깔끔하게 성공했다.\n구축 쿠버네티스의 구축 자체는 생각보다 간단하지만 상업 환경에서 안전하고 가용성 있게 지속적으로 관리하는 것은 어렵다. 그래서 자체 호스팅하는 것은 권장하지 않는다고 한다. 3\n그러나 교육의 관점에서는 자체 호스팅해서 사용하는 것은 좋은 선택이라고 생각한다. 퍼블릭 클라우드를 사용하다가 요금을 폭탄맞을 수 있기 때문이다.\nMultipass 선택한 이유 리눅스 기반 가상 머신은 선택지가 생각보다 다양하게 있다. 나는 vagrant, multipass 중에서 고민했다. Multipass를 선택한 이유는 사용하기 쉬워서이다.\nvagrant의 장점은 다양한 이미지를 사용할 수 있다는 점과 Vagrantfile를 통해 자동화 스크립트를 만들 수 있다는 점이었다. 하지만 단점이 존재했는데, 스크립트를 짜는 법을 알아야한다는 점과 내가 쿠버네티스 구축하는 전반적인 방식을 이해하지 못해 디버깅 하지 못한다는 것이었다. 실제로 깃허브의 오픈소스를 clone해서 구성해봤는데, 네트워크 할당이 제대로 되지 않아 많은 시간을 쓰기도 했다.\nmultipass는 Canonical에서 만들었으며 Ubuntu의 가상화환경(VM)을 쉽게 구성할 수 있도록 해주는 도구4이다. 즉 우분투 보급사가 만든 가상환경이라고 보면 된다. 우분투 외의 리눅스를 써야한다면 multipass는 좋은 선택지는 아니지만 centOS Linux가 더이상 지원되지 않는다는 점5에서 쓸 수 있는 건 우분투 밖에 없는 것 같다.\n쿠베네티스 구성 자세한 설치는 enumclass tistory를 참고했다.\nCalico 선택한 이유 CNI 플러그인에서 가장 대중적으로 쓰이는 것은 Calico이다. CNI 플러그인 벤치마크를 보면 리소스 사용률이나, 속도 측면에서 Calico가 우수한 편이다.\nCNI의 특징은 Pod에 IP를 할당한다. 이러한 특징이 대규모 환경에서는 IP 부족으로 이어질 수 있다. 6 그러나 소규모 프로젝트는 많은 pod를 생성하지 않으므로 IP는 부족하지 않을 것이다.\n결과 10.120.52.x IP는 multipass instance의 IP이고, 192.168.x.x는 calico의 IP이다.\n실제로 파드를 생성해 실행시켜 보면 아래와 같은 결과를 얻을 수 있다.\nhttps://physical-world.tistory.com/56\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://romillion.tistory.com/96\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n쿠버네티스를 활용한 클라우드 네이티브 데브옵스, 존 어렌들, 저스틴 도밍거스 지음, 한빛 미디어\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://kim-dragon.tistory.com/176\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://arstechnica.com/gadgets/2020/12/centos-shifts-from-red-hat-unbranded-to-red-hat-beta/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://dobby-isfree.tistory.com/201\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-07-12T00:00:00+09:00","permalink":"https://s0okju.github.io/p/server-setup-multinode-kubernetes/","title":"서버 구축기 - 1. 쿠버네티스 멀티 노드 환경 구성하기"},{"content":"Multipass를 구동하려고 하는 중 디스크 용량이 꽉찼다는 메세지를 얻게 되었다.\n1 cannot create temporary directory for the root file system: No space left on device 서버에서 사용중인 SSD는 1TB인데 5개의 Multipass 인스턴스로 다 찬다는 것은 말이 되지 않았다.\n실제 하드웨어 상에서 부착된 용량과 ubuntu lvm에서 사용되는 최대 용량이 달랐다. 그러므로 이 용량을 적절하게 조절해야 한다.\n해결 LV의 용량을 VG의 크기만큼 조절한다. Stack Exchange에서 요구한대로 명령어를 사용하면 해결된다.\n1 2 3 4 5 lvm lvm\u0026gt; lvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv lvm\u0026gt; exit resize2fs /dev/ubuntu-vg/ubuntu-lv 자세하게 봐보자. ubuntu-vg는 SSD의 용량만큼 적절하게 할당 받았다. 그러나 Alloc PE를 보면 100 GB 밖에 남지 않았다는 것을 알 수 있다.\nlvdisplay로 lv를 확인해보면 ubuntu-lv가 100GB로 할당되었음을 확인할 수 있다.\n여기서 LVM(Logical Volume Manager)의 개념을 알아야 하는데, 파일 시스템에 추상화 계층을 추가하여 논리적 스토리지를 생성할 수 있게 해준다.1\n저자의 환경에서는 Root LV가 100GB로만 할당되어 있어, 하드 디스크의 용량 만큼 사용할 수 없었던 것이었다.\n현재 환경에서는 하나의 물리 디스크만 사용하기 때문에 설정하는데 복잡한 것은 없다. 그러므로 추가적으로 ubuntu-vg에 lv를 추가하지 않는 한 Root lv를 vg 크기만큼 사이즈를 키우면 된다.\nhttps://tech.cloud.nongshim.co.kr/2018/11/23/lvmlogical-volume-manager-1-%EA%B0%9C%EB%85%90/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-05-28T00:00:00+09:00","permalink":"https://s0okju.github.io/p/server-management-disk-1/","title":"그 많은 서버 디스크 용량을 누가 잡아 먹었는가?"},{"content":"ORM 선택 Go의 경우에는 JAVA와 달리 대표적인 ORM 프레임워크가 없어, 개발자가 직접 선택해서 사용해야 한다.\n다양한 Go ORM 프레임워크 순위를 알고 싶다면 OSS Insight 를 참고하길 바란다.\n기본적으로 mysql driver 사용을 생각했다. 그러나 정교한 저수준의 쿼리를 다루는 것이 아니기 때문에 ORM를 적극적으로 활용하고 싶었다.\n조건 깔끔한 도메인 정의 여러 종류의 DB 지원 컴파일 레벨에서 디버깅 가능 Ent Ent란 Facebook에서 개발한 Go ORM이다. 공식 설명에 의하면 그래프 구조의 데이터베이스 스키마를 가지고 있으며, 코드 생성을 기반으로 하는 정적 타이핑을 지원한다.1 이는 위에서 말한 조건에 어느 정도 충족이 된다.\n다만 ent는 관계형 데이터베이스에 적합하며 NoSQL 기반 데이터베이스에는 적합하지 않았다.2 ORM 선택 조건에는 부합하지 않았으나 대부분의 ORM이 RDB 위주로 지원한다는 것3을 감안했을때 ent은 RDB 사용 시 괜찮은 선택이라고 생각한다.\n초반에 Gorm 사용도 고려했다. 그러나 모델을 정의하는데 사용되는 struct tag 는 개인적으로 가독성이 좋지 않다는 인상이 들었다.\n1 2 3 4 5 6 type Model struct { ID uint `gorm:\u0026#34;primaryKey\u0026#34;` CreatedAt time.Time UpdatedAt time.Time DeletedAt gorm.DeletedAt `gorm:\u0026#34;index\u0026#34;` } 첫인상 ORM 라이브러리의 schema를 본 순간 그래프 데이터베이스인 줄 알았다. 과거에 Neo4j4라는 그래프 데이터베이스를 사용해본 적이 있는데, 이 노드, 그래프를 별도로 정의해서 구현한 점이 상당히 유사했기 때문이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 type Task struct { ent.Schema } // Fields of the Task. func (Task) Fields() []ent.Field { return []ent.Field{ field.Int(\u0026#34;id\u0026#34;), field.String(\u0026#34;title\u0026#34;), field.Int(\u0026#34;total_status\u0026#34;), field.Time(\u0026#34;created_at\u0026#34;).Default(time.Now()), field.Time(\u0026#34;modified_at\u0026#34;).Default(time.Now()), } } // Edges of the Task. func (Task) Edges() []ent.Edge { return []ent.Edge{ edge.To(\u0026#34;subtask\u0026#34;, SubTask.Type), } } 공식 홈페이지를 참고해보면 Ent ORM에 대해 이렇게 설명했다.\nEasily model database schema as a graph structure.\nschema 구조를 그래프 구조로 구현되어 있다는 것이다. 아마 ent가 그래프 탐색에 대한 자신감을 표현한 것도 구조적인 이유때문이지 않을까 싶다.\n적용하기 Schema ent의 참조 방식은 독특하다. 기존의 참조 방식과 반대이기 때문이다.5 ent 공식 문서에 의하면 edge.To를 사용하고 있으면 설정한 Edge를 소유한다고 정의한다.6\nA schema that defines an edge using the edge.To builder owns the relation, unlike using the edge.From builder that gives only a back-reference for the relation (with a different name).\n🤔 필자의 경우 위의 정의를 고려하고 구현하니 더 헷갈리기 시작했다. 그래서 관계 소유자인 schema만 정의하고, 그 외에는 서로의 연관관계를 설정해준다는 마음으로 구현했다.\nMember : Task Entity가 1:N 연관 관계를 가진다고 가정해보자.\nMember는 tasks라는 관계의 소유자이다. 그러므로 edge.To로 관계를 설정한다. 하지만 Task는 many에 해당되기 때문에 아무것도 설정하지 않는다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Fields of the Member. func (Member) Fields() []ent.Field { return []ent.Field{ field.Int(\u0026#34;id\u0026#34;), field.String(\u0026#34;email\u0026#34;), field.String(\u0026#34;username\u0026#34;), field.String(\u0026#34;password\u0026#34;), field.Time(\u0026#34;created_at\u0026#34;).Default(time.Now()), } } // Edges of the Member. func (Member) Edges() []ent.Edge { return []ent.Edge{ edge.To(\u0026#34;tasks\u0026#34;, Task.Type), } } Task에서는 Member에서 소유한 관계(user)를 역참조해서 관계를 정의하게 된다. 이때 Member는 관계에서 One에 해당되니 Unique() 함수를 붙이게 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Fields of the Task. func (Task) Fields() []ent.Field { return []ent.Field{ field.Int(\u0026#34;id\u0026#34;), field.String(\u0026#34;title\u0026#34;), field.Int(\u0026#34;total_status\u0026#34;), field.Time(\u0026#34;created_at\u0026#34;).Default(time.Now()), field.Time(\u0026#34;modified_at\u0026#34;).Default(time.Now()), } } // Edges of the Task. func (Task) Edges() []ent.Edge { return []ent.Edge{ edge.From(\u0026#34;member\u0026#34;, Member.Type).Ref(\u0026#34;tasks\u0026#34;).Unique(), } } 참조하는 이유는 어떤 schema와 참조하는지를 명시하기 위해서라고 보면 된다.\n\u0026hellip; because there can be multiple references from one schema to other.\n예제 - 데이터 생성 코드 그럼 데이터를 생성할때 어떻게 해야할까? 참조하는 Schema(Task)에서 Member 정보를 추가해주면 된다.\n공식 문서에서는 직접 Query해서 데이터를 가져왔지만7 , 그 외에도 Schema 데이터( 예제에서는 ent.Member) 혹은 아이디만으로도 추가가 가능하니 공식 문서를 참고하길 바란다.\n1 2 3 4 5 6 7 8 9 func (s *Store) Create(ctx *gin.Context, b request.CreateTask) error { // create Task _, err := s.client.Task.Create().SetTitle(b.Title).SetTotalStatus(0) .SetMemberID(b.UserId).Save(ctx) if err != nil { return err } return nil } 실제로 데이터베이스를 보면 {참조하는 관계명} _ {참조하는 관계명}으로 이뤄져 있다.\n위의 예제에서는 1:N(One-to-Many)인 경우에면 설명했지만 (M:N)의 경우에는 {참조하는 관계명} _ {참조하는 관계명}의 이름을 가진 테이블이 생성된다.\nhttps://entgo.io/docs/getting-started/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n필자는 MongoDB에 적용시키고자 검색을 여러번 했지만 끝내 찾지 못했다.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://blog.billo.io/devposts/go_orm_recommandation/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://neo4j.com/docs/getting-started/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://umi0410.github.io/blog/golang/how-to-backend-in-go-db/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://entgo.io/docs/schema-edges#quick-summary\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://entgo.io/docs/schema-edges#o2o-two-types\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-05-22T00:00:00+09:00","permalink":"https://s0okju.github.io/p/entgo-1/","title":"Entgo - Ent ORM"},{"content":"Go를 활용하여 직접 2~3개의 서비스를 만들고 나니, 구현할때 지나쳤던 에러들이 큰 눈덩이로 변해서 덮쳐왔다. 문제가 생긴 지점은 명확했지만, 그것보다는 문제 발생의 근본적인 원인을 찾고자 했었다. 내가 생각한 가장 큰 원인은 테스트 코드의 부재였다. 테스트의 장점은 누구나 알고 있을 것이다. 코드의 안전성을 확보할 수 있으며 문제 발생 시 빠르게 수정할 수 있다는 것이다. 현재 코드에는 숨겨진 구멍들이 존재하는데, 이를 해결하기 위해서는 단위 테스트, 통합 테스트가 필요했다. 기존에 있던 코드를 활용하여 테스트 코드로 작성하려고 시도했으나, 지금까지 적은 Go 코드는 테스트를 용이하도록 작성되지 않았다. 특히 인터페이스를 적절하게 사용하지 않아서 Mock를 사용하는데 어려움이 생기기도 했다. 그래서 처음부터 다시 제작하고자 한다.\n나는 동일한 코드에 대해 이번까지 포함해서 2번 변경했다. 어떻게 변경했는지 알아보자\n초기 - Go 한달차의 적응기 에러 타입과 상태 타입 1차에서는 ErrorType으로 특정 에러 타입을 정의하는데 사용할 타입을 지정해줬다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type ErrorType int type Error struct { // Code is a custom error code ErrorType ErrorType // Err is a error string Err error // Description is a human-friendly message. Description string } type ErrorRes struct { // Code is http status code Code int `json:\u0026#34;code\u0026#34;` } 모든 서비스들이 사용할 경우, 각 서비스가 사용할 에러 코드를 일리리 지정했다. map를 활용하여 에러 타입에 해당되는 상태 코드를 매핑했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 const ( // Common SUCCESS ErrorType = 0 INVALID_JSON_FORMAT ErrorType = 1001 INVALID_URI_FORMAT ErrorType = 1002 // Member INVALID_MEMBER ErrorType = 2001 ERROR_MEMBER_DB ErrorType = 2101 // Task ERROR_TASK_DB ErrorType = 3101 ) var codeMap = map[ErrorType]int{ // Common INVALID_JSON_FORMAT: http.StatusBadRequest, INVALID_URI_FORMAT: http.StatusBadRequest, // Member INVALID_MEMBER: http.StatusUnauthorized, ERROR_MEMBER_DB: http.StatusInternalServerError, // Task ERROR_TASK_DB: http.StatusInternalServerError, } 에러 반환 직접 커스텀한 에러값을 입력값으로 해서 context를 활용해 Json 형태로 반환해줬다.\n1 2 3 4 5 6 7 8 9 10 11 12 // getCode is get Status code from codeMap. func getCode(flag ErrorType) *ErrorRes { return \u0026amp;ErrorRes{Code: codeMap[flag]} } func ErrorFunc(ctx *gin.Context, err *Error) { res := getCode(err.ErrorType) log.Println(err) ctx.AbortWithStatusJSON(res.Code, res) return } \u0026ldquo;저렇게 하면 오류가 제대로 반환되지 않을텐데?\u0026ldquo;라고 생각하는 사람이 있을 것이다. 저때 당시 gin이 에러를 응답하는 매커니즘을 잘 몰라 단순하게 AbortWithStatusJson으로 응답하도록 했다. 비록 결과는 에러 json + 성공 json 둘다 나왔지만 말이다.\n1차 - Middleware가 있었다고요? gin의 오류 처리 매커니즘을 제대로 이해하지 못해 단순하게 응답을 처리했다. 이제는 Middleware를 제작함으로써 에러 응답 반환이 수월하도록 구현했다.\n에러 상태 타입 ErrorType -\u0026gt; WebCode로 이름을 변경했으며, 에러뿐만 아니라 상태 전체를 표시했다. 여기서 눈치 빠르신 분은 알겠지만 에러 타입에는 상태값을 직접 포함하도록 구현했다. 왜냐하면 상태 코드를 추가할 일이 많은데 작업을 두번해야 하기 때문이다.\n1 2 TaskCreationSuccess WebCode = 220101 SubtaskCreationSuccess WebCode = 220151 예로 220101 이라면 2(Task Service)/201(Status code)/01(Unique number)로 구성하도록 했다.\n에러 타입과 반환 이전에는 에러 타입을 Error라고 정했지만 gin 내부의 에러를 쓰면서 헷갈리기 시작했다. 그래서 NetError로 이름을 변경했으며, Description field는 잘 사용하지 않는 것 같아 삭제했다.\n1 2 3 4 5 6 7 8 9 10 11 12 type NetError struct { Code codes.WebCode Err error } func NewNetError(code codes.WebCode, err error) *NetError { logrus.Errorf(\u0026#34;Code : %d, Error : %v\u0026#34;, code, err) if err != nil { return \u0026amp;NetError{Code: code, Err: err} } return \u0026amp;NetError{Code: code, Err: nil} } 에러는 동일하게 code만 반환하도록 구현했다.\n1 2 3 4 5 6 7 8 // Response type ErrorResponse struct { Code codes.WebCode `json:\u0026#34;codes\u0026#34;` } func NewErrorResponse(code codes.WebCode) *ErrorResponse { return \u0026amp;ErrorResponse{Code: code} } 성공 응답값을 생각해보면 크게 두 가지가 존재한다.\n데이터가 없는 경우 데이터가 있는 경우 이 두가지를 나눠서 별도의 함수를 만들었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 func SuccessWith(ctx *gin.Context, code codes.WebCode, data any) { status := codes.GetStatus(code) res := NewSuccessResponseWith(code, data) ctx.Header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) ctx.AbortWithStatusJSON(status, res) } func Success(ctx *gin.Context, code codes.WebCode) { status := codes.GetStatus(code) ctx.Header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) res := NewSuccessResponse(code) ctx.JSON(status, res) } 에러 반환 로직은 gin Middleware를 사용했다. 자세한 사항은 [[Gin - 예외처리 2. 커스텀 예외처리 구현하기]] 를 참고하길 바란다.\n2차 - 프론트엔드를 하고나니\u0026hellip; 왜 백엔드와 프론트엔드가 짝꿍이 맞아야 하는지 알 것 같았다. 플러터의 경우에는 응답값과 똑같은 필드를 가진 모델로 반환해야 한다. 만약에 정확하게 받지 못하면, 아무런 값도 받을 수 없다.(심지어 상태값도 말이다.) 플러터로써는 해결 방안을 찾지 못해서 서버측에서 성공이든 오류든 모두 동일한 형태로 반환하도록 구현해야 했었다.\nResponse Format 위에서 1차, 2차를 보면 구체적인 메세지가 없었다. 메세지를 넣지 않은 이유는 클라이언트에게 저런것까지 보여줄 필요가 없다고 생각해서였다. 그러나 앱개발하면서 정보가 없어서 정말 힘들었다. 😥\nError Response의 Best Practice를 찾아보니 대부분 자체적인 code와 이를 설명하는 메세지가 포함되어 있었다.1 이번에 수정한 응답에서는 메세지를 추가하기로 했다.\n에러 상태 타입 일단 지금까지 사용했던 에러 코드보다 길이를 더 축소시켰다. 그래서 자주 사용하는 상태 코드를 백의 자리로 두고, 1씩 카운트를 추가하면서 에러 코드를 정의했다. 또한 에러코드에 매핑되는 에러 메세지를 추가함으로써, 동일한 에러 메세지를 반환하도록 했다. 메세지는 정확하게 적지 않고, 문제를 전반적으로 보여줄 수 있도록 작성했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 const ( // 1xx is Bad Request error code InvalidHeader ErrorCode = 101 InvalidBody ErrorCode = 102 InvalidQuery ErrorCode = 103 // 2xx is Unauthorized error code BadAuthenticationData ErrorCode = 201 TokenExpired ErrorCode = 202 ... ) var errorMessage = map[ErrorCode]string{ InvalidHeader: \u0026#34;The provided header values are invalid.\u0026#34;, InvalidBody: \u0026#34;The body of the request is invalid.\u0026#34;, InvalidQuery: \u0026#34;The query parameters are invalid.\u0026#34;, ... } 에러 타입 netErrorOptions 라는 옵션을 추가시켰는데, 여기에는 서비스의 메타 정보가 포함되어 있다.\n1 2 3 4 5 6 7 8 9 10 11 // NetError have options and ErrorCode that contains status code. type NetError struct { // options is metadata about service options netErrorOptions // Type is a unique data that contains http status code. Code ErrorCode // Description is an error details. Description string // Err is an error message. Err error } 메타 정보로써 태그가 있는데, 서비스 이름을 문자열로 표시하는 것이다. 태그는 서버에서 디버깅을 조금더 수월하게 하기 위해서 도입했다. 오류코드가 모두 동일해서 어떤 서비스의 오류인지 명시하는 용도로 사용된다.\n1 2 3 4 5 // netErrorOptions is meta data about service type netErrorOptions struct { // tag is the service name tag string } 이 태그는 애플리케이션을 시작할때 한번만 설정하도록 함으로써, 모든 NetError가 해당 tag를 가지도록 구현했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 var ( opts netErrorOptions ) // netErrorOptions is meta data about service type netErrorOptions struct { // tag is the service name tag string } // SetTag sets the service name func SetTag(tag string) { mu := sync.Mutex{} mu.Lock() defer mu.Unlock() opts.tag = tag } 에러 반환 에러는 크게 두 가지로 나눠져 있다.\nStatus: 응답의 타입 (\u0026ldquo;Success\u0026rdquo; or \u0026ldquo;Error\u0026rdquo;) Data : 에러 데이터 혹은 성공 시 반환할 데이터 1 2 3 4 5 // BaseResponse is common response that use in success and failed type BaseResponse struct { Status string `json:\u0026#34;status\u0026#34;` Data interface{} `json:\u0026#34;data\u0026#34;` } 다만 생성자를 만들때 성공, 실패 여부를 따로 만들었다. 그 이유는 입력하는 값이 다 달랐기 때문이다.\n성공한 경우 반환하고자 하는 데이터를 Data 필드에 넣기만 하면 된다.\n1 2 3 4 5 6 func NewSuccessBaseResponse(data interface{}) *BaseResponse { return \u0026amp;BaseResponse{ Status: \u0026#34;Success\u0026#34;, Data: data, } } 그러나 오류인 경우는 다르다. 반환할 오류 타입은 형식화되어 있으므로, ErrorResponse 구조체를 따로 만들어서 Data 필드에 저장했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 // ErrorResponse is the error response format type ErrorResponse struct { Code ErrorCode `json:\u0026#34;code\u0026#34;` Message string `json:\u0026#34;message\u0026#34;` } func NewErrorBaseResponse(data NetError) *BaseResponse { res := ErrorResponse{Code: data.Code, Message: GetErrorMsg(data.Code)} return \u0026amp;BaseResponse{ Status: \u0026#34;Error\u0026#34;, Data: res, } } BaseResponse가 반환되면, 메소드로 응답값을 반환하도록 구현했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Failed response failed formatted response. // It converts NetError to ErrorResponse to extract necessary things. func (b BaseResponse) Failed(ctx *gin.Context) { res := b.GetErrorData() status := parseStatusCode(res.Code) ctx.AbortWithStatusJSON(status, b) return } // OKSuccess uses when status code is 200 func (b BaseResponse) OKSuccess(ctx *gin.Context) { ctx.Header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) ctx.AbortWithStatusJSON(200, b) } 테스트 gin 테스트 사례2를 참고해서 성공했을 경우의 테스트 코드를 작성해보았다. 테스트 코드는 테이블 중심 테스트(Table-Driven Test) 방식3을 채택했다. 단위 테스트가 비슷한 구조로 이뤄져 있는 경우, 테이블 중심 테스트로 수행하면 코드 중복을 피할 수 있어 로직을 변경하거나 새로운 케이스를 추가하기 쉽다는 장점이 있다. 작성한 테스트 코드에는 테스트 구조가 비슷하고, 라우팅 설정 등 공통된 부분이 많아서 테이블 중심 테스트를 선택하게 되었다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 func setupRouter() *gin.Engine { r := gin.Default() r.Use(errorutils.ErrorMiddleware()) return r } func TestSuccessResponse(t *testing.T) { r := setupRouter() res := errorutils.NewSuccessBaseResponse(nil) // Set table tests := []struct { description string fn func(*gin.Context) expectedCode int path string }{ { // Ok Success description: \u0026#34;OKSuccess\u0026#34;, fn: func(c *gin.Context) { res.OKSuccess(c) }, expectedCode: 200, path: \u0026#34;/test/ok\u0026#34;, }, { // Created Success description: \u0026#34;CreatedSuccess\u0026#34;, fn: func(c *gin.Context) { res.CreatedSuccess(c) }, expectedCode: 201, path: \u0026#34;/test/created\u0026#34;, }, } for _, tc := range tests { t.Run(tc.description, func(t *testing.T) { w := httptest.NewRecorder() req, _ := http.NewRequest(\u0026#34;GET\u0026#34;, tc.path, nil) // Specific handler function and serve r.GET(tc.path, tc.fn) r.ServeHTTP(w, req) if w.Code != tc.expectedCode { t.Errorf(\u0026#34;Expected %d, got %d\u0026#34;, tc.expectedCode, w.Code) } }) } } 테스트 코드를 실행하면 아래와 같은 결과를 얻을 수 있다.\n마치며 하나의 간단한 CRUD가 있는 서비스 2개를 가지고 코드를 여러번 수정했다. 그 과정에서 다양한 오류를 만났고, 테스트 코드의 중요성도 확실히 알았다. 특히 이번에 테스트 코드를 작성하면서 오류를 많이 마주 했는데, 이를 재빠르게 수정하면서 테스트의 중요성을 느끼게 되었다.\nhttps://pjh3749.tistory.com/273\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://gin-gonic.com/docs/testing/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://bugoverdose.github.io/development/go-table-driven-tests/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-05-11T00:00:00+09:00","permalink":"https://s0okju.github.io/p/go-code-refactor-error-1/","title":"Todopoint 서버코드 개선기 -  1. 공동 에러 처리하기"},{"content":"EKS Amazon Web Services(AWS)에 Kubernetes 컨트롤 플레인을 설치, 운영 및 유지 관리할 필요가 없는 관리형 서비스이다1.\nECS vs EKS 둘 다 컨테이너 오케스트레이션이라는 점에서 공통점을 가짐 ECS는 AWS가 만든 자체적인 오케스트레이션 구조로 K8S 구조와 완전 다름 K8S는 오픈소스가 크게 활성화되어 있고, 다양한 플러그를 사용할 수 있음 ECS는 오픈소스가 크게 활성화되어 있지 않음. Auto-scaling 측면에서 EKS는 수동 및 자동 배포가 가능하지만 ECS는 수동으로만 가능하다. 오류 수정하는데 ECS는 전문가가 필요함 📌 자세한 사항은 물통꿀꿀이님의 블로그, [AWS] ECS vs Kubernetes를 참고하길 바람.\n🤔 EKS가 무조건 정답은 아니다. 프로젝트에 따라서는 ECS가 더 적합한 선택지일 수 있다.\nECS 적용 사례 - 밍글 왜 EKS 설치는 어려운가? 우리가 알고 있는 쿠버네티스 구조와 달리 쿠버네티스는 일부 도구만 제공하고, 나머지는 별도의 설치가 필요하다. 이런 이유로 버전 관리가 어려워지기도 한다.\n쿠버네티스 제공 : kube-proxy, kubelet CNCF Graduated Project를 보면 etcd, coreDNS 등을 볼 수 있다.\nEKS 구조 앞서 말했듯이 직접 구축하게 되면, 관리해야 하는 요소들이 많다. 이러한 어려움을 덜어내고자 EKS를 사용할 수 있다. EKS는 관리형 \b서비스로서 Control Plane를 직접 구성하지 않고 K8S를 사용할 수 있다. EKS 특징 EKS CNI를 활용하여 VPC 네트워크 상에서 파드간 통신 가능 IAM을 활용하여 권한 설정 가능 AWS가 가용 영역별 API Server ,etcd 배포하여 고가용성 보장 eksctl를 활용하여 워커노드를 custom할 수 있음. 📌 자세한 사항은 Jaden Park님의 블로그, Amazon EKS 란?를 참고하길 바란다.\nAWS Side Workflow 쿠버네티스의 api 서버를 각 가용영역에 배포 api 데이터, 쿠버네티스의 상태 데이터를 확인하기 위해 etcd를 같이 배포 쿠버네티스에서 오는 call 에 대한 IAM 구성 쿠버네티스 마스터 노드의 오토스케일링 설정 클러스터가 안정적으로 구현하도록 여기에 연결할 수 있는 로드밸런서를 구성 Reference 초보자를 위한 EKS 맛 보기 ECS vs Kubernetes https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/what-is-eks.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-05-01T00:00:00+09:00","permalink":"https://s0okju.github.io/p/what-is-eks/","title":"AWS - EKS란 무엇인가"},{"content":"Introduction Terraform 코드를 분석하고 있었다. 어디에서는 IAM을 쓰고, 어디에서는 Profile을 쓴다. 과연 그 둘의 차이점은 무엇일까? 알아보도록 하자!\nProfile Amazon Ec2는 IAM role을 가진 profile을 사용하게 된다.\n질문 1. awscli를 통해 profile을 지정할 수 있는데, 이것도 Instance Profile인가? If you use the AWS CLI, API, or an AWS SDK to create a role, you create the role and instance profile as separate actions, with potentially different names. If you then use the AWS CLI, API, or an AWS SDK to launch an instance with an IAM role or to attach an IAM role to an instance, specify the instance profile name. 출처 - 공식 문서\n질문이 잘못되었다. EC2는 해당 IAM Role을 가진 Profile을 사용한다. 그래서 awscli의 경우 \u0026ndash;profile을 통해 직접 profile 이름을 지정할 수 있다. 왜 필요할까? aws cli를 사용할 때 profile 기능을 이용하면 여러개의 자격 증명을 등록하고 스위칭해서 효율적으로 사용할 수 있습니다.\n출처 - https://cloudest.oopy.io/posting/101\n질문 2. IAM Role과의 차이점은? Profile이 필요한 이유는 여러개의 자격 증명을 등록하고 스위칭해서 효율적으로 사용하기 위함이라고 했다. 특징을 봤을때 IAM의 Role과 비슷해 보였다.\nRoles are designed to be “assumed” by other principals which do define “who am I?”, such as users, Amazon services, and EC2 instances An instance profile, on the other hand, defines “who am I?” Just like an IAM user represents a person, an instance profile represents EC2 instances. The only permissions an EC2 instance profile has is the power to assume a role.\n출처 - https://www.quora.com/In-AWS-what-is-the-difference-between-a-role-and-an-instance-profile\nIAM Role은 무엇을 할 수 있는지에 대한 행위를 정의하는 것이라면 Profile Instance는 \u0026ldquo;누가 만들었는지를 정의하기\u0026rdquo; 위함이라면 보면 된다.\nConclusion 내가 이러한 궁금증을 가지게 된 것은 IAM을 한사람에게만 부여된다고 생각했기 때문이었다. 그런 맥락에서는 profile이 필요 없기 때문이다. Reference https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html ","date":"2024-04-16T00:00:00+09:00","permalink":"https://s0okju.github.io/p/iam-and-profile/","title":"AWS - IAM 그리고 Profile"},{"content":"이전에는 Gin이 어떻게 예외 처리를 하는지 언급했다. 이제부터 직접 구현하고자 한다.\nError Wrapping 직접 제작한 에러 코드인 WebCode와 에러를 감싼 NetError를 만들었다.\n1 2 3 4 type NetError struct { Code codes.WebCode Err error } 프로젝트 구현할때 주로 gin 혹은 ent 라이브러리를 쓴다. 별도의 Error 구조체(NetError)를 정의함으로써 Err에 gin 혹은 ent 에러를 담겠다는 의미인 것이다.\nGin에서의 예외 처리 Gin Error() 함수에 의하면 Gin의 Context에 Error를 담은 후에 Middleware에서 처리하는 것을 권장하고 있다.\nError attaches an error to the current context. The error is pushed to a list of errors. It\u0026rsquo;s a good idea to call Error for each error that occurred during the resolution of a request. A middleware can be used to collect all the errors and push them to a database together, print a log, or append it in the HTTP response. Error will panic if err is nil.\n라이브러리 코드를 보면 Errors 필드가 정의되어 있는데, errorMsg는 []*Error 에러 리스트로 타입을 가지고 있다. 위의 설명대로 error의 리스트가 Context 내부에 구현되어 있는 것이다.\n1 2 3 4 5 6 7 8 9 // gin/context.go type Context struct { // ... Errors errorMsgs // .. } // gin/errors.go type errorMsgs []*Error 즉, ctx.Error를 활용하여 입력받은 에러를 Gin의 에러로 감싼 후에 Context의 Errors 리스트에 넣게 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // gin/context.go // Error attaches an error to the current context. The error is pushed to a list of errors.// It\u0026#39;s a good idea to call Error for each error that occurred during the resolution of a request.// A middleware can be used to collect all the errors and push them to a database together, // print a log, or append it in the HTTP response. // Error will panic if err is nil. func (c *Context) Error(err error) *Error { if err == nil { panic(\u0026#34;err is nil\u0026#34;) } var parsedError *Error ok := errors.As(err, \u0026amp;parsedError) if !ok { parsedError = \u0026amp;Error{ Err: err, Type: ErrorTypePrivate, } } c.Errors = append(c.Errors, parsedError) return parsedError } Context 내부에 있는 Error 리스트는 Middleware에서 처리하게 된다.\nGin에서는 HandlerFunc를 slice로 구현된 HandlerChain이 있는데, 이는 Gin이 각가지의 Handler를 Chain내에 넣고 처리하는 것이다.\n1 type HandlersChain []HandlerFunc 그럼 오류를 어떻게 발생시키면 될까? HandlerChain 내에 있는 대기 중인(Pending) Handler를 호출하지 않도록 하면된다. 즉, Context를 Abort하면 되는 것이다. 그 이후에 처리할 Handler가 없어지면서 종료가 된다.\nAbort prevents pending handlers from being called. Note that this will not stop the current handler. Let\u0026rsquo;s say you have an authorization middleware that validates that the current request is authorized. If the authorization fails (ex: the password does not match), call Abort to ensure the remaining handlers for this request are not called.\n프로젝트에 적용하기 절차 위의 내용을 가지고 실제로 적용해보자. 프로젝트 구성은 3계층으로 커스텀 에러 감싸기는 서비스 로직에 수행하도록 했다.\nService 계층에 Error Wrapping을 한다. Controller에 context 내에 있는 에러 리스트에 예외를 넣는다. Middleware에 Error Wrapping한 것을 Unwrapping 하면서 예외 타입을 학인한다. 커스텀 에러라면 WebCode에 따른 응답값을 반환한다. 그림으로 표현하자면 아래와 같다. 코드 코드는 아래의 두 사이트를 참고했습니다.\nNaver D2, Golang, 그대들은 어떻게 할 것인가 - 3. error 래핑 Naver D2, Golang, 그대들은 어떻게 할 것인가 - 4. error 핸들링 Service Layer Persistence Layer에서 얻은 에러 값을 직접 받은 후에 Service 계층에서 적절하게 NetError로 감싼다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // service func (s *MemberService) CreateMember(ctx *gin.Context, req data.RegisterReq) (*ent.Member, *errorutils.NetError) { // Check member Exist existedMem, err := s.Store.GetMemberByEmail(ctx, req.Email) if err != nil { return nil, \u0026amp;errorutils.NetError{Code: codes.MemberInternalServerError, Err: err} } if ent.IsNotFound(err) { mem, err2 := s.Store.Create(ctx, req) if err2 != nil { return nil, \u0026amp;errorutils.NetError{Code: codes.MemberCreationError, Err: err2} } return mem, nil } return existedMem, nil } Controller Layer Controller는 Service 계층에서 감싼 커스텀 에러를 받은 후에 Context의 에러 리스트에 넣는다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func (controller *MemberController) RegisterMember(ctx *gin.Context) { req := data.RegisterReq{} err := ctx.ShouldBindJSON(\u0026amp;req) if err != nil { _ = ctx.Error(errorutils.NewNetError(codes.MemberInvalidJson, err)) return } // Create member mem, err2 := controller.service.CreateMember(ctx, req) if err2 != nil { // Service 계층에서 받은 에러를 Context 내 에러 리스트에 넣는다. _ = ctx.Error(err2) return } mid := data.MemberId{MemberId: mem.ID} response.SuccessWith(ctx, codes.MemberCreationSuccess, mid) } Middleware 에러 응답값을 반환할 HandlerFunc를 구현한다.\nNext를 활용하여 대기 중인 핸들러를 실행시킨다. errors.As를 활용하여 Context 에러 리스트에 있는 에러가 커스텀 에러인지 확인한다. 정확히 말하자면 에러를 unwrapping하면서 커스텀 에러인지 확인한 후에 있다면 netError에 넣게 된다. WebCode를 활용하여 응답값을 얻은 후 AbortWithStatusJson 를 활용하여 Response json을 전송한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func ErrorHandler() gin.HandlerFunc { return func(ctx *gin.Context) { // Pending 중인 핸들러 실행 ctx.Next() // JSON이 두번 쓰이는 것을 대비해서 Body 확인 isBodyWritten := ctx.Writer.Written() err := ctx.Errors.Last() if err != nil { // 커스텀 에러인지 확인 var netErr *errorutils.NetError if errors.As(err, \u0026amp;netErr) { code := netErr.GetCode() statusCode := codes.GetStatus(code) res := response.NewErrorResponse(code) if !isBodyWritten { ctx.AbortWithStatusJSON(int(statusCode), res) } } else { res := response.NewErrorResponse(codes.GlobalInternalServerError) if !isBodyWritten { ctx.AbortWithStatusJSON(http.StatusInternalServerError, res) } } } } } 마무리 Go의 예외 매커니즘은 다른 언어와 달라서 틀을 잡는데 많은 시간을 사용했다. 공식 문서나 다른 사람들의 예제 코드를 분석하면서, Go 스러움이 무엇인지 자츰 배워간다는 느낌이다. 그러나 코드양이 많아지면서 오히려 코드가 복잡해지는 것 같기도 하다.\n","date":"2024-04-10T00:00:00+09:00","permalink":"https://s0okju.github.io/p/gin-error-handling-2/","title":"Gin 예외처리 - 2. 커스텀 예외처리"},{"content":"Go와 예외처리 Go에서의 예외처리 Go에서는 함수에서 반환된 에러 객체(error)로 처리한다. 다행히도 multi-return이 가능하기에 에러 반환을 더욱 수월하게 해줄 수 있다.\n1 2 3 4 f, err := Sqrt(-1) if err != nil { fmt.Println(err) } try ~ catch가 없는 이유 공식 문서에 의하면 try ~ catch는 난해한 코드를 생성하며, 개발자에게 너무많은 일반적인 예외를 처리하도록 장려한다고 한다.\nWe believe that coupling exceptions to a control structure, as in the try-catch-finally idiom, results in convoluted code. It also tends to encourage programmers to label too many ordinary errors, such as failing to open a file, as exceptional.\n다른 언어처럼 try ~ catch 를 어렴풋이 구현할 수 있다. 중간에 실행의 흐름을 끊는 panic 함수를 사용하는 것이다. 하지만 반대로 생각하자면 모든 에러들을 panic으로 처리해야 할까? Go에서는 그것이 아니라는 것이다. 이런 이유로 Go는 시의적절하기 예외처리할 수 있도록 error를 반환하는 방식으로 처리하게 된다.\nError - Wrapping Go에서는 에러처리할 때 Error 객체를 넘겨준다고 한다. 물론 일반 에러 객체를 넘겨줄 수 있지만 개발자가 직접 만든 에러를 만들어서 넘겨줄 수 있다. Error Wrapping이란 쉽게 말하자면 error 객체를 감싸는 또다른 구조체를 만드는 것이라고 보면 된다.\ngin에서의 Error를 봐보자. gin의 Error 내에 필드로 error가 존재한다. 이러한 과정을 Error Wrapping이라고 보면 된다.\n1 2 3 4 5 6 // Error represents a error\u0026#39;s specification. type Error struct { Err error Type ErrorType Meta any } 그럼 예를 들어보자. ctx.Error를 실행했는데 의도치 않게 errors.As가 적절하게 실행되지 않는다고 가정해보자. error.As는 Error Type을 확인하는 함수인데, 만약에 타입이 적절하지 않는다면, 입력한 error을 감싼 Error를 반환하게 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func (c *Context) Error(err error) *Error { if err == nil { panic(\u0026#34;err is nil\u0026#34;) } var parsedError *Error ok := errors.As(err, \u0026amp;parsedError) if !ok { parsedError = \u0026amp;Error{ Err: err, Type: ErrorTypePrivate, } } c.Errors = append(c.Errors, parsedError) return parsedError } 그럼 원본 에러(error)에 접근할 수 있을까? 바로 Unwrap를 통해 얻을 수 있은 것이다.\n1 2 3 4 // Unwrap returns the wrapped error, to allow interoperability with errors.Is(), errors.As() and errors.Unwrap() func (msg *Error) Unwrap() error { return msg.Err } 그림으로 표현하면 아래와 같다.\n기존의 문제점 Gin Context의 잘못된 활용법 공식 문서에서 말하는 Context는 데드라인, 취소 시그널, API에 대한 경계값을 가지는 값으로 정의된다. 그래서 조건에 따라 실행이 중단될 수 있다는 것으로 이해했다. gin은 자체적인 Context를 가지고 있으며, context를 중단시킬 수 있는 여러 함수들이 존재한다.\nService Layer에서 커스텀 에러 타입으로 반환하도록 구현했다.\n1 2 3 4 5 6 7 8 9 10 11 12 func (controller *MemberController) RegisterMember(ctx *gin.Context, req request.RegisterReq) { req = request.RegisterReq{} err := ctx.ShouldBindJSON(req) // ... // Create member err2 := controller.service.CreateMember(ctx, req) if err2 != nil { errorutils.ErrorFunc(ctx, err2) return } webutils.Success(ctx) } 커스텀 에러 타입을 자세히 보면, 자체적으로 제작한 에러 코드와 error을 담을 Err 필드가 존재한다.\n1 2 3 4 5 6 7 8 type Error struct { // Code is a custom error codes ErrorType ErrorType // Err is a error string Err error // Description is a human-friendly message. Description string } 애플리케이션에 오류 발생시 현재 실행을 멈추고, 응답값을 보내는 ErrorFunc도 만들었다.\n1 2 3 4 5 6 func ErrorFunc(ctx *gin.Context, err *Error) { res := getCode(err.ErrorType) ctx.AbortWithStatusJSON(res.Code, res) return } 공식 문서 에 의하면AbortWithStatusJSON에는 내부적으로 Context를 중단시킬 수 있는Abort 함수를 사용한다. 구체적으로 Abort 함수는 현재의 handler는 그대로 남지만, 그 이후의 handler를 처리하지 않겠다는 것이다.\nAbort prevents pending handlers from being called. Note that this will not stop the current handler. Let\u0026rsquo;s say you have an authorization middleware that validates that the current request is authorized. If the authorization fails (ex: the password does not match), call Abort to ensure the remaining handlers for this request are not called.\n문장을 보면서 내가 Gin 프레임워크의 에러처리를 완전히 잘못했음을 깨닫게 되었다.\nGin Error 미사용 공식 문서에 의하면 Gin은 자신들의 Error를 사용하는 것을 권장하며, middleware가 이를 처리하여 오류 response를 처리하라고 명시되어 있다.\nError attaches an error to the current context. The error is pushed to a list of errors. It\u0026rsquo;s a good idea to call Error for each error that occurred during the resolution of a request. A middleware can be used to collect all the errors and push them to a database together, print a log, or append it in the HTTP response. Error will panic if err is nil.\n즉, 오류가 발생할때마다 gin의 Context에서 제공해주는 Error로 감싸며, Middleware에 있는 Handler가 이를 순차적으로 처리해야 한다는 것이다.\n2부에서는 지금까지는 내가 만들었던 예외처리에는 어떠한 문제점이 있는지 확인해봤다. 2부에서는 위에서 설명한 잘못된 에러처리를 공식문서에서 제시한 올바른 에러처리를 구현하고자 한다.\nMiddleware에 Handler 구현 gin.Error를 활용하여 Error를 wrapping하고, Middleware에서 처리하기 ","date":"2024-04-03T00:00:00+09:00","permalink":"https://s0okju.github.io/p/gin-error-handling-1/","title":"Gin 예외처리 - 1. Go Error"},{"content":"도커와 데이터베이스 한참 컨테이너를 공부를 했을 당시 도커의 주 목적이 빠른 배포/개발 환경을 구성하는 것이기 때문에 영속성을 가진 데이터베이스에는 적합하지 않았다고 알고 있다. 이에 대한 문제점으로 DB 데이터 손실을 언급한다. 그런데 도커를 생각해보면 volume 기능이 존재한다. 즉, 데이터를 영속적으로 보관할 수 있다는 것이다. 현구막 기술 블로그에서 보면 이론적으로는 도커를 사용해도 되었으나, 서비스 운영 측면에서는 도커라는 기술이 불안정해 쓰이지 않는다고 언급되어 있다.\n쿠버네티스에서는요? 그런데 저건 도커에 한정된 이야기인 것 같다. 쿠버네티스의 특징을 잘 살펴보면 상태를 사전에 정의하고, 정의된 상태로 운영된다. 운영 측면에서 편리해 보이고, 안전성이 문제라고 한다면 다중화를 하면 된다. 이정훈님의 medium - 실제로 본 DB on Kubernetes 효과에서 언급했다싶이 일부 성능 향상도 보이기도 했다고 한다.\nTodoPoint에서는 어떻게 구성하지? StatefulSet을 사용하자가 결론이다. StatefulSet은 상태 정보를 구성할때 주로 사용된다. 아래의 블로그처럼 CronJob을 활용해서 백업을 구성하도록 설계해야 겠다는 생각이 들었다. Reference https://nangman14.tistory.com/79 ","date":"2024-03-13T00:00:00+09:00","permalink":"https://s0okju.github.io/p/todopoint-db-design/","title":"TodoPoint - 쿠버네티스 환경에서 DB는 어떻게 구성하지?"},{"content":"Java 진영에서 자주 사용되는 ORM은 JPA이다. 하지만 Go에서는 대표적인 ORM은 없는듯하다. 그런탓에 이런저런 라이브러리가 많은데, 상황에 맞게 라이브러리를 쓰세요 식의 Go스러움을 많이 볼 수 있다.\n어떤 라이브러리를 사용할 것인가? gorm 과거에는 gorm이 많이 쓰인다고 하던데, 여러 문제점이 존재한다고 한다.1개인적으로 최악의 기능은 설정들을 struct tag로 관리하는 것이라고 생각한다.\nent 요즘 뜨고 있는 라이브러리는 FaceBook에서 자체적으로 개발한 ent이다. GO 코드로 스키마를 작성하면 DB에 모델링을 해주며, 특히 Graph 탐색에 특화되어 있다.\n나의 선택은 나의 선택은 ent이다. 일단 대기업이 개발했다는 점에서 1차적으로 신뢰가 간다. sqlboiler가 화려한 벤치마크 결과를 보여줘서 궁금하긴 하지만 문서화가 잘된 ent를 우선적으로 해보고자 한다.\nORM 꼭 필요한 것인가? 웹개발 자체를 얕게 해오면서 ORM에 익숙해졌다. 그러면서 ORM은 선택이 아닌 필수라고 여기면서 사용했던 것 같다. 하지만 ORM 추천글을 읽으면서 꼭 필요한가에 대해 생각하게 된 계기가 되었던 것 같다.2 개발자 입장에서는 CRUD를 추상화된 방법으로 사용할 수 있으며 구현이 용이하고, 가독성이 있는 코드를 사용할 수 있다. 하지만 ORM에 대한 부정적인 입장의 대부분의 과다하게 사용했을 때를 가정한다. 개인적으로는 어떠한 라이브러리든 과다하게 사용하는 것은 오버헤드가 따른다고 생각한다. 또한 DB 설계 자체가 잘못되어서 일 수 있다. 나와 같은 소규모 프로젝트를 사용하는 경우라면 기존 라이브러리에 있는 모든 기능을 쓸 일이 없다. 그러므로 ORM을 쓰려고 한다. 그리고 개발자가 쓰기 편해야 유지보수하기 편할 것 같다는 생각이 더 많이 든다.\nhttps://umi0410.github.io/blog/golang/how-to-backend-in-go-db/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.reddit.com/r/golang/comments/t3bp79/a_good_orm_for_golang/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-03-09T00:00:00+09:00","permalink":"https://s0okju.github.io/p/go-db-migration/","title":"Go-DB에 마그이션하기"},{"content":"스프링 부트 스프링 부트의 특징은 아래의 문장으로 정리 가능하다.\n스프링 프레임워크는 IoC/DI를 통해 객체 간의 의존 관계를 설정하고, AOP를 통해 핵심 관점과 부가 관점을 분리해 개발하며, PSA를 통해 추상화된 다양한 서비스들을 일관된 방식으로 사용하도록 한다.\nIoC : 객체의 생성과 관리를 개발자가 하는 것이 아니라 프레임워크가 대신하는 것 DI : 외부에서 객체를 주입받아 사용하는 것 AOP : 프로그래밍을 할 때 핵심 관점과 부가 관점을 나누어서 개발하는 것 PSA : 어느 기술을 사용하던 일관된 방식으로 처리하는 것 IoC/DI를 통해 객체 간의 의존 관계를 설정하고, AOP를 통해 핵심 관점(service)과 부가 관점(Repository)를 분리해 개발하며, PSA(JpaRepository)를 통해 추상화된 다양한 서비스들을 일관된 방식으로 사용하도록 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // Service public interface UserService { void createUser(UserRegisterCommand command); User getUserById(Long userId); UserLoginResponse loginUserByEmail(String email); Boolean logoutById(String userId); } @Service public class UserServiceImp implements UserService{ @Autowired // IoC/DI private UserRepository userRepository; @Autowired private UserMapper userMapper; @Override public void createUser(UserRegisterCommand command) { try { userRepository.save(new UserEntity(command.getEmail(), command.getPassword(), command.getUsername())); }catch (DataAccessException e){ throw new DefaultException(ResponseMessage.DATA_ACCESS_ERROR,\u0026#34;user\u0026#34;); }catch (Exception e){ throw new DefaultException(ResponseMessage.DB_UNEXPECTED_ERROR,\u0026#34;user\u0026#34;); } } } // UserRepository // Crud에서 인터페이스 구현체가 자동으로 만들어진다. // PSA public interface UserRepository extends JpaRepository\u0026lt;UserEntity,Long\u0026gt;,UserCustomRepository{ } 의존성 주입 의존성 주입은 스프링 컨테이너가 생성한 객체(빈)을 주입받는 것을 의미한다. 이때 빈은 스프링 컨테이너가 생성하고 관리하는 객체를 의미한다.\n주입을 받을때는 @Autowired를 사용하고, 빈을 등록하고 싶으면 @Component를 선언하면 된다. 프레임워크에 사용되는 대부분의 핵심 어노테이션은(@Repository, @Service 등) @Component가 포함되어 있다.\n의존성 주입에 대해서는 inung_92님의 velog, [Spring]스프링 기초 - DI을 참고하길 바란다.\n스프링 웹 계층 presentation 계층 HTTP 요청 및 응답 처리 비즈니스 계층 비즈니스 로직 구현 퍼시스턴트 계층 스토리지 관련 로직 추가 위와 같은 구조는 Layered Architecture라고 한다. 이때 레이어 간 응집성을 높이고 의존도를 낮추기 위해서는 계층 간 호출 시 인터페이스를 통해 호출해야 한다. 즉, 상위 계층은 직접적으로 하위 계층을 호출하지 않고 추상적인 인터페이스에 의존한다.\nQ. 응집도란?\n응집도란 한 모듈 내부의 처리 요소들이 서로 관련되어 있는 정도를 말한다. 즉, 모듈이 독립적인 기능을 수행하지 또는 하나의 기능을 중심으로 책임이 잘 뭉쳐있는지를 나타내며 모듈이 높은 응집도를 가질수록 좋다.\n출처 - https://madplay.github.io/post/coupling-and-cohesion-in-software-engineering\nQ. 의존성을 왜 낮춰야 하는 걸까?\nA. 의존성이란 다른 객체의 영향을 받고, 다른 객체에 따라 결과가 좌우되는 것이다. 의존성을 낮춰 객체들 간 전파되는 변경에 대한 영향을 최소화 시켜, 유지보수 시 원활하게 하기 위해서이다.\n출처 - https://velog.io/@ung6860/Spring스프링-기초-DIDependency-Injection\nQ. 의존성을 낮추는 방법?\nA. 기본적으로 의존성을 클래스 내부에서 new 연산자를 사용하는 것이 아닌 외부로부터 객체의 의존성을 주입받아야한다. 외부로부터 의존성을 주입받는 것의 의미는 \u0026lsquo;의존성 주입\u0026rsquo;에 초점을 맞추는게 아니라 \u0026lsquo;의존성 탈피를 위하여 외부로부터 주입\u0026rsquo;이라는 부가적인 설명에 초점을 맞추어야한다.\nPresentation 웹 클라이언트의 요청 및 응답 처리한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @RestController @RequestMapping(\u0026#34;/membership/\u0026#34;); public class RegisterMembershipController { @Autowired private MembershipService membershipService; @PostMapping(path = \u0026#34;register\u0026#34;) public Membership createmembership(@RequestBody MembershipRequest request){ MembershipCommand command = MembershipCommand.builder() .name(\u0026#34;hello\u0026#34;).builder(); return membershipService.createMembership(command); } 보통은 @Controller를 통해 controller를 지정해줄 수 있다. rest api를 구현했는데, 이에 적합한 @RestController가 있다. @RestController는 @Controller + @ResponseBody라고 보면 된다. 자세한 사항은 망나니 개발자님의 블로그를 참고하길 바란다.\nService 비즈니스 로직을 처리하는 계층이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public interface MembershipService{ Membership createMembership(MembershipCommand command); } @Service public class MembershipServiceImp implements MembershipService{ @Autowired MembershipRepository membershipRepository; @Override Membership createMembership(MembershipCommand command){ membershipRepository.save(command); } } 핵심 비즈니스 로직은 연산으로 구성된 인터페이스를 정의하고 그에 대한 구현을 따로 작성하는 것이 좋다. 그래서 컨트롤러가 인터페이스에 의존하게 되는데, 나중에서 서비스 구현이 변경되더라도 인터페이스에 의존돼 컨트롤러를 변경할 필요가 없다는 것이다.\nRepository 저장소에 접근하는 영역으로 DAO(Data Access Object) 영역이라고 부른다. @Repository를 사용하여 Spring repository를 지정해준다. 주목할 점은 인터페이스 내용이 비어 있더라도 스프링 데이터가 JPA 런타임에 인터페이스 구현체를 자동으로 만들어준다.\n1 2 3 4 @Repository public interface MembershipRepository extends CrudRepository\u0026lt;Membership,Long\u0026gt; { } DTO 계층 간에 데이터 교환을 위한 객체를 의미한다.\n처음 공부 했을 때 “Dto 다 똑같은 객체겠군… 오류 발생하면 난리나겠네”라고 생각했다. 각 계층마다 Dto가 다를 수 있다. 아래의 예시를 보면 Controller → Service에서는 Dto가 Membership 객체를 사용했다. 하지만 Membership 도메인과 실제 엔티티와 다를 경우라면 어떨까? Service → Repository에서는 Dto가 MembershipEntity가 되는 것이다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 // Controller membershipService.createMembership(member); // MembershipService public interface MembershipService{ Membership createMembership(Membership member); } @Component public class MembershipMapper { MembershipEntity MapToEntity(Membership member){ return new MembershipEntity( // ... 생략 ); } } @Service public class MembershipServiceImp implements MembershipService{ @Autowired MembershipRepository MembershipRepository; @Override Membership createMembership(Membership membership){ MembershipRepository.save(MembershipMapper.MapToEntity(membership)); } } 테스트 소프트웨어 개발 시 테스트 단계는 필수다. 이 단계에서는 작성한 코드를 기반으로 검증한다. 테스트는 크게 단위 테스트와 통합 테스트가 존재한데, 단위 테스트를 집중적으로 설명하고자 한다.\n단위 테스트란 하나의 모듈을 기준으로 독립적으로 진행되는 가장 작은 단위의 테스트이다. 특정 부분만 독립적으로 테스트하기 때문에 빠르게 문제 여부를 확인할 수 있게 된다.\n자세한 사항은 망나니 개발자님의 블로그를 참고하길 바란다.\n데이터베이스 JPA(Java Persistent API) 관계형 데이터베이스를 사용하는 방식을 정의한 인터페이스이다. 이를 구현한 ORM 프레임워크로는 hibernate가 있다.\nJPA는 다양한 쿼리 방법을 지원한다.\nJPQL QueryDSL Native SQL JDBC API 직접 사용 요즘은 QueryDSL를 사용하는 추세라고 한다.\n앤티티 DB의 테이블과 매핑되는 객체로 DB 테이블과 직접 연결된다.\n영속성 컨텍스트 엔티티를 관리하는 가상 공간이다.\n1차 캐시 캐시키는 엔티티의 키로 둬서, 조회시 1차적으로 영속성 컨텍스트 내에 있는 값을 반환한다. 쓰기 지연 빈번한 context swtiching에 대한 overhead를 줄여주기 위해 한꺼번에 모았다가 처리한다. 변경 감지 영속성 컨텍스트 내에 있는 캐시와 변경된 entity값을 비교해서 엔티티 변경 감지 후 DB에 자동 반영 비즈니스 로직, DB 상에 있는 로직의 꼬이는 것을 방지 Q. 왜 영속성 테스트를 사용하는 것인가?\nA. 트랜잭션의 횟수나 방식을 효율적으로 관리해 서버의 속도를 늘리고, 중복되는 쿼리를 줄일 수 있다.\nWAS가 시작되면 EntityMangerFactory가 생성되고, 트랜잭션마다 생성되는 EntityManager를 관리한다. 이때 EntityManager는 Entity의 생명 주기를 관리한다.(관리 상태, 비영속 상태, 삭제된 상태)\n스프링 부트는 EntityManagerFactory를 하나만 생성해서 관리하고, 의존성 주입해서 사용함. 빈을 하나만 생성해서 동시성 문제를 해소했다.\nSpring data api 인터페이스 및 주석 세트를 제공해서 데이터 액세스 계층을 쉽게 구축할 수 있도록 한다. JPA를 한 단계 추상화시킨 Repository라는 인터페이스를 제공함으로써 이뤄진다.\n스프링 데이터의 공통적인 기능에서 JPA의 유용한 기술이 추가 (스프링 데이터 인터페이스를 상속받아서 간단한 트랜잭션 처리 가능)\nReference Spring - ORM, JPA, Hibernate, JDBC 총정리 [JPA] 객체지향 쿼리, JPQL [TDD] 단위 테스트(Unit Test) 작성의 필요성 (1/3) [Spring]스프링 기초 - DI(Dependency Injection) ","date":"2024-02-03T00:00:00+09:00","permalink":"https://s0okju.github.io/p/basic-spring-1/","title":"Spring - Spring Boot 간략하게 알아보기"},{"content":"backend Terraform Backend는 Terraform의 state(.tfstate)파일의 저장 공간을 지정하는 설정이다. 기본적으로는 로컬에 저장되지만, 설정에 따라서는 원격 저장소(S3, Terraform Cloud 등)에 저장할 수 있다.\nTerraform에서 상태 파일은 테러폼 코드와 인프라 객체를 매핑하기 위해 존재한다.\nThe primary purpose of Terraform state is to store bindings between objects in a remote system and resource instances declared in your configuration.\n출처 - Terraform 공식 홈페이지\n테라폼이 작동되기 전에 refresh라는 작업을 통해 테라폼 상태 파일을 업데이트한다. 테라폼 코드의 일부 리소스 블록을 삭제하게 된다면, 실제 인프라 객체에 적용(apply)하기 전에 refresh라는 작업을 통해 테라폼 상태 파일이 업데이트 되는 것이다.\nThis(refresh) won\u0026rsquo;t modify your real remote objects, but it will modify the Terraform state.\n출처 - Terraform 공식 홈페이지\n상태 파일 내 객체와 인프라 객체와 1:1로 매핑하는 개념이기 때문에, 상태 파일이 유실되거나 일부만 잘못 설정되어도 큰 문제를 야기할 수 있다. 이러한 상황을 대비해 Backup과 Locking은 필수적으로 적용하게 된다.\nLocking 동시에 상태 파일을 접근하게 되면 충돌이 발생하거나 인프라 설정이 꼬이게 되어 전반적으로 큰 문제가 될 수 있다. Locking을 통해 동시에 접근 상태를 막아 의도치 않는 변경을 예방해야 한다.\nBackup 상태 파일이 유실되는 경우를 대비해서 Backup을 해야 한다. S3을 사용할때는 versioning을 사용하여 히스토리를 기억한다.\nstack overflow에 Terraform fails because tfstate (S3 backend) is lost을 읽어보는 것을 추천합니다.\n실습 1. S3 Backend 만들기 해당 실습해서는 Locking하는 것을 제외했다.\nS3 Bucket 생성 Terraform backend 정보는 apply되면 코드가 순차적으로 실행되면서 S3에 상태 파일 업로드를 시도하게 된다. 만약에 설정하지 않으면 아래와 같은 오류를 받게 된다.\n1 2 3 Error: creating S3 Bucket (tf-backend-d7mekz) ACL: operation error S3: PutBucketAcl, https response error StatusCode: 400, ~ api error AccessControlListNotSupported: The bucket does not allow ACLs aws_s3_bucket, aws_s3_bucket_acl을 활용해 private S3 bucket을 생성한다. 또한 히스토리를 저장하기 위해 S3의 Versioning도 추가했다.\n공식 홈페이지에서도 Versioning 설정을 적극 권장하고 있다.\nWarning! It is highly recommended that you enable Bucket Versioning on the S3 bucket to allow for state recovery in the case of accidental deletions and human error.\n출처 -Backend Type: s3 | Terraform | HashiCorp Developer\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;tf_backend\u0026#34; { bucket = \u0026#34;d7_tf_backend\u0026#34; tags = { Name = \u0026#34;tf_backend\u0026#34; } } resource \u0026#34;aws_s3_bucket_acl\u0026#34; \u0026#34;example\u0026#34; { bucket = aws_s3_bucket.tf_backend.id acl = \u0026#34;private\u0026#34; } # Versioning resource \u0026#34;aws_s3_bucket_versioning\u0026#34; \u0026#34;versioning_example\u0026#34; { bucket = aws_s3_bucket.tf_backend.id versioning_configuration { status = \u0026#34;Enabled\u0026#34; } } Backend 설정 S3 생성 이후 Hashicorp의 공식 홈페이지를 참고하여 backend을 설정하면 된다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;tf-backend-d7mekz\u0026#34; key = \u0026#34;terraform.tfstate\u0026#34; region = \u0026#34;ap-northeast-2\u0026#34; } } terraform 블록에 수정이 가하면 init를 해줘야 한다. init 후 apply을 실행시키면 아래와 같이 S3에 상태 파일이 저장되었음을 확인할 수 있다.\nRemote Backend을 설정하게 되면 로컬에 있는 상태 파일 내용은 사라지게 된다.\nReference Backend Type: s3 | Terraform | HashiCorp Developer Backend 활용하기Terraform \u0026amp; AWS 101 ","date":"2024-01-29T00:00:00+09:00","permalink":"https://s0okju.github.io/p/terraform-backend/","title":"Terraform - Backend"},{"content":"Terraform 이란 HashiCorp에서 오픈소스로 개발중인, 클라우드 및 온프로미스 인프라를 코드로 관리할 수 있는 코드이다. 인프라 환경 구성 시 선언적 코드형식을 사용하여 리소스를 생성, 수정, 삭제하여 관리가 가능한 laC(Infrastructure as Code) 프로비저닝 도구이다.\nTerraform 작동 방식 주로 Write, Plan, Apply 절차로 이뤄진다.\nWrite : Hashicorp에서 자체 개발한 HCL 언어로 스크립트 작성 Plan : 상태를 비교하며 변경점을 사용자에게 보여줌 Apply : 실행하는 단계, 순차적으로 실행 의존 관계에 따라 순서를 명시할 수 있다. 상태 비교를 통해 휴먼 에러를 줄일 수 있다. 그러나 코드의 무게가 무거워지면, 실행하는데 오래걸리는 원인이 될 수 있다. 그 외에도 다양한 단점이 존재할 것이다. 그럼에도 불구하고 생산성 측면, 종속성 그래프 등 장점들이 더 크기 때문에 사용하는 것이다.\n클라우드 컴퓨팅 환경은 참조된 리소스가 처음의 리소스를 참조하는 순환루프 문제를 지니고 있다. 무한한 반복으로 인해 시스템이 빠르게 고갈되고, 성능이 저하된다. Terraform에서는 리소스의 참조 내용을 그래프로 보여주는 Resource Graph를 제공해주는데, 이를 활용해 순환루프 문제를 예방할 수 있다.\n연습 자세한 문법은 Terraform Language Documentation을 참고하길 바란다.\n시나리오 AWS를 활용한다. 개발, 운영 VPC를 구축한다. 각 VPC에서 private, public subnet이 존재한다. 외부와 통신을 위해 internet gateway와 내부 리소스끼리만 통신할 수 있는 NAT gateway를 설치한다. 구현 AWS를 활용한다. Terraform AWS Registry에서 제공해주는 코드를 그대로 사용한다. 나의 경우 region을 ap-northeast-2 로 설정했다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } } # Configure the AWS Provider provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } 개발 운영 vpc를 구축한다. vpc 모듈을 별도로 만들어 코드를 재사용했고, env를 변수로 삼아 리소스의 이름에 개발 환경을 명시하도록 했다.\n1 2 3 4 5 6 7 8 9 10 11 // For Dev module \u0026#34;dev_vpc\u0026#34; { source = \u0026#34;./vpc.d\u0026#34; env = \u0026#34;dev\u0026#34; } // For Production module \u0026#34;prd_vpc\u0026#34; { source = \u0026#34;./vpc.d\u0026#34; env = \u0026#34;prd\u0026#34; } 모듈의 main.tf에는 public, private Subnet 그리고 NAT gateway, Internet gateway를 생성하는 코드를 작성했다.\nPrivate Subnet을 구성하기 위해서는 라우팅 테이블이 필요하지만, 임시이므로 Private NAT Gateway만 작성했다. 여기서 주목해야 할 점은 NAT, Internet Gateway의 경우 Subnet이 우선적으로 만들어져야 실행된다.(만약에 그러지 않는다면 오류가 뜬다.) 그러므로 코드를 작성할때 각 리소스의 의존성을 고려해서 작성해야 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } } // Configure the AWS Provider provider \u0026#34;aws\u0026#34; { region = \u0026#34;ap-northeast-2\u0026#34; } // Create a VPC resource \u0026#34;aws_vpc\u0026#34; \u0026#34;default\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; tags = { Name = \u0026#34;terraform_default_vpc_${var.env}\u0026#34; } } // public subnet resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public_subnet_1\u0026#34; { vpc_id = aws_vpc.default.id cidr_block = \u0026#34;10.0.0.0/24\u0026#34; tags = { Name = \u0026#34;terraform_public_subnet_1_${var.env}\u0026#34; } } // private subnet resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private_subnet_1\u0026#34; { vpc_id = aws_vpc.default.id cidr_block = \u0026#34;10.0.10.0/24\u0026#34; tags = { Name = \u0026#34;terraform_private_subnet_1_${var.env}\u0026#34; } } // private NAT resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;private_nat\u0026#34; { connectivity_type = \u0026#34;private\u0026#34; subnet_id = aws_subnet.private_subnet_1.id tags = { Name = \u0026#34;terraform_nat_${var.env}\u0026#34; } } // Internet gateway resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;igw\u0026#34; { vpc_id = aws_vpc.default.id tags = { Name = \u0026#34;terraform_igw_${var.env}\u0026#34; } } Terraform 도입과 관련된 기술 블로그 좌충우돌 Terraform 입문기, 우아한형제들 기술블로그 DevOps팀의 Terraform 모험 Terraform IaC 도구를 활용한 AWS 웹콘솔 클릭 노가다 해방기 → 다들 웹콘솔에서의 해방을 외치고 있었다..\n","date":"2024-01-25T00:00:00+09:00","permalink":"https://s0okju.github.io/p/basic-terraform-1/","title":"Terraform - 기본 문법 익히기"}]